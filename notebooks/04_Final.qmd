---
title: "Experimental Results"
author: "David Kubek"
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: true
    code-summary: "Show code"
    self-contained: true
  pdf:
    toc: true
    number-sections: true
    colorlinks: true
jupyter: python3
execute:
  warning: false
  message: false
---

# Introduction {#sec-intro}

This chapter presents a comprehensive experimental evaluation of five
optimization methods for solving the approximate symmetry problem in networks.
We compare three iterative methods operating on the Birkhoff polytope---the
Quadratic Symmetry Approximator (QSA), Manifold Optimization (MF), and Interior
Point Method (IP)---against two temperature-based approaches---Dimensionality
Reduction (DR) and Orthogonal Relaxation (OR). Through systematic evaluation on
synthetic network models, we will be interested (among others) in evaluating
mainly the following questions:

- Second-order optimization methods and menifold optimization will outperform
first-order methods by exploiting curvature information in the non-convex
landscape.

- For temperature-based methods, indefinite relaxation will yield superior
solutions compared to convex relaxation as per prevailing assumption suggested
by Lyzinski~\cite{lyzinski2015graph}. This hypothesis will be rigorously tested
using statistical methods (t-tests and Cohen's D effect sizes) in addition to
visual comparisons.

- Dimensionality reduction from $O(n^2)$ to $O(n)$ parameters will not result in
degraded solution quality.

- Method performance rankings will remain consistent across different network
topologies and sizes.

Our experimental design employs seven synthetic network models with controlled
structural properties: 
- Erdős-Rényi (ER) random graphs
- Barabási-Albert (BA) scale-free networks representing real-world structures
- Lateral Random Model (LRM) networks providing controlled approximate symmetries
- Random Geometric Graphs (GEO2D) in 2D Euclidean space modeling spatial networks
- Random Geometric Graphs (GEO3D) in 3D Euclidean space for volumetric networks
- Duplication-Divergence (DD) networks simulating biological network evolution
- Holme-Kim (HK) scale-free networks with tunable clustering coefficients

This systematic evaluation enables us to assess both the absolute performance of
each method and their relative strengths across varying problem instances and topological structures.


## Experimental Setup

```{python}
#| label: setup
#| code-summary: "Import libraries and set up visualization defaults"

import numpy as np
import pandas as pd
import seaborn as sns
from pathlib import Path
import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec
import matplotlib.colors as mcolors
import warnings
import scienceplots

# Suppress warnings for cleaner output
warnings.filterwarnings('ignore', category=FutureWarning)

# Set visualization style for publication-ready figures
plt.style.use(['science'])
plt.rcParams.update({
    'font.size': 13,
    'axes.labelsize': 18,  # Increase axis description font size
    'axes.titlesize': 22,  # Increase plot title font size
    'xtick.labelsize': 12,
    'ytick.labelsize': 12,
    'legend.fontsize': 18,  # Increase legend font size
    'figure.titlesize': 24, # Increase suptitle font size
    #'font.family': 'serif',
    #'font.serif': ['Times'],
    'text.usetex': True,
    'text.latex.preamble': '\\usepackage{libertinus}',
    'pgf.rcfonts': False,
    'pdf.fonttype': 42,  # Ensures font embedding
    'pgf.texsystem': 'pdflatex',
    'pgf.preamble': '\n'.join([
        '\\usepackage[a-1b]{pdfx}',
        '\\usepackage[utf8]{inputenc}',
        '\\usepackage[T1]{fontenc}',
        '\\usepackage{libertine}',  # This is Linux Libertine, very similar
        '\\usepackage{amsmath,amssymb}',
    ]),
    'figure.dpi': 300,
    'savefig.dpi': 300,
    'savefig.bbox': 'tight',
    #'savefig.bbox': 'standard',
    'savefig.pad_inches': 0.1,
    'image.interpolation': 'none' 
})

# Define consistent color palette for methods
#METHOD_COLORS = {
#    'QSA': '#2E86AB',      # Blue
#    'Manifold': '#A23B72',  # Purple
#    'InteriorPoint': '#F18F01',  # Orange
#    'SoftSort': '#C73E1D',  # Red
#    'OT4P4AS': '#4A7C59'    # Green
#}
METHOD_COLORS = dict(zip([
    'QSA',
    'Manifold',
    'InteriorPoint',
    'SoftSort',
    'OT4P4AS',
], sns.color_palette("deep")))
METHOD_COLORS.update(
    dict(zip([
    'DimensionalityReduction',
    'OrthogonalRelaxation',
    ], sns.color_palette("deep")[3:5]
    ))
)


# Method name mapping
METHOD_NAMES = {
    'QSA': 'QSA',
    'Manifold': 'MF',
    'InteriorPoint': 'IP',
    'DimensionalityReduction': 'DR',
    'OrthogonalRelaxation': 'OR',
}

# Set the color palette
sns.set_palette(list(METHOD_COLORS.values()))

# Define graph type constants
STANDARD_GRAPH_TYPES = ["ER", "BA", "GEO2D", "GEO3D", "DD", "HK", "BRAIN"]
ALL_GRAPH_TYPES = ["ER", "BA", "LRM", "GEO2D", "GEO3D", "DD", "HK", "BRAIN"]
STANDARD_WITH_LRM = ["ER", "BA", "GEO2D", "GEO3D", "DD", "HK", "LRM", "BRAIN"]

# Create output directory for figures
figure_dir = Path("figures")
table_dir = Path("tables")
figure_dir.mkdir(exist_ok=True)
table_dir.mkdir(exist_ok=True)

# Helper function to save figures
def save_figure(fig, name):
    """Save figure in PDF format with consistent settings"""

    fig.savefig(figure_dir / f"{name}.pdf", format='pdf', backend='pgf')


def save_table(df, name, caption="", show_index=None, longtable=False, **kwargs):
    """Export table to publication-ready LaTeX with full MultiIndex support.
    
    Parameters
    ----------
    df : pd.DataFrame
        DataFrame to export
    name : str
        Output filename (without .tex extension)
    caption : str
        Table caption
    show_index : bool, optional
        Whether to show the index. Auto-detected if None.
    longtable : bool
        Whether to use longtable environment (for multi-page tables).
        Default False uses regular tabular wrapped in table environment.
    **kwargs
        Additional arguments passed to df.to_latex()
    """
    import re
    
    # Auto-detect if we should show index
    if show_index is None:
        show_index = isinstance(df.index, pd.MultiIndex) or not df.index.equals(pd.RangeIndex(len(df)))
    
    has_multiindex_rows = isinstance(df.index, pd.MultiIndex) and show_index
    has_multiindex_cols = isinstance(df.columns, pd.MultiIndex)
    
    # For MultiIndex, reset index to avoid malformed headers
    if has_multiindex_rows:
        df_export = df.reset_index()
        show_index_for_latex = False
    else:
        df_export = df.copy()
        show_index_for_latex = show_index
    
    # Drop columns that are entirely empty/NaN
    df_export = df_export.dropna(axis=1, how='all')
    
    # Dynamic column formatting with reduced padding
    col_formats = ['@{}']  # Remove left padding
    
    if show_index_for_latex:
        col_formats.append('l')
    
    # Format data columns
    for col in df_export.columns:
        if pd.api.types.is_numeric_dtype(df_export[col]):
            col_formats.append('r')
        else:
            col_formats.append('l')
    
    col_formats.append('@{}')  # Remove right padding
    column_format = ''.join(col_formats)
    
    def safe_float_format(x):
        try:
            if pd.isna(x):
                return ''
            if isinstance(x, (int, float)):
                return f"{x:.3f}" if abs(x) < 100 else f"{x:.1f}"
            return str(x)
        except:
            return str(x)
    
    # Extract label from kwargs if provided
    label = kwargs.pop('label', None)
    
    if longtable:
        # Use longtable for multi-page tables
        # Note: longtable uses [c], [l], [r] for alignment, not [htbp]
        latex_str = df_export.to_latex(
            index=show_index_for_latex,
            escape=False,
            column_format=column_format,
            multirow=has_multiindex_rows,
            multicolumn=has_multiindex_cols,
            longtable=True,
            float_format=safe_float_format,
            na_rep='',
            caption=caption if caption else None,
            position='c',  # Center alignment for longtable
            label=label,
            **kwargs
        )
    else:
        # Use regular tabular wrapped in table environment
        latex_str = df_export.to_latex(
            index=show_index_for_latex,
            escape=False,
            column_format=column_format,
            multirow=has_multiindex_rows,
            multicolumn=has_multiindex_cols,
            longtable=False,
            float_format=safe_float_format,
            na_rep='',
            **kwargs
        )
        
        # Wrap in table environment with caption and label
        table_wrapper = "\\begin{table}[htbp]\n\\centering\n"
        if caption:
            table_wrapper += f"\\caption{{{caption}}}\n"
        if label:
            table_wrapper += f"\\label{{{label}}}\n"
        table_wrapper += latex_str
        table_wrapper += "\\end{table}\n"
        latex_str = table_wrapper
    
    # Post-process to fix any remaining issues
    # Remove any duplicate toprule/midrule at the start
    latex_str = re.sub(r'(\\toprule\n)+', r'\\toprule\n', latex_str)
    
    with open(table_dir / f"{name}.tex", 'w', encoding='utf-8') as f:
        f.write(latex_str)
```

```{python}
#| label: load-data
#| code-summary: "Load experimental results data"

# Load the experimental results
output_dir = Path("processed_data")
df = pd.read_csv(output_dir / "graph_symmetry_results_brain.csv")

# Preprocess: Rename GEO to GEO2D for consistency
df['graph_type'] = df['graph_type'].replace({'GEO': 'GEO2D'})

# Apply method name mapping
df['method_display'] = df['method'].map(METHOD_NAMES)

# Display basic information about the dataset
print(f"Total number of experimental runs: {len(df):,}")
print(f"Methods evaluated: {', '.join(df['method'].unique())}")
print(f"Graph types: {', '.join(df['graph_type'].unique())}")
print(f"\nData shape: {df.shape}")
print(f"\nColumns: {', '.join(df.columns)}")
```

```{python}
# Filter dirty data
df = df[~((df["method"] == "SoftSort") & (df["method_max_iter"] == 3000))]
```

```{python}
# Decode rewiring values (0 -> 0, 1 -> 13, 2 -> 50, 3 -> 130)
df['rew'] = df['rew'].replace({0.0: 0, 1.0: 13, 2.0: 50, 3.0: 130}).astype('Int64')
```

```{python}
df['graph_type'] = df['graph_type'].replace({"LRM_ER": "LRM"})
```

```{python}
df['density'] = (df['density'] / 100).apply(lambda x: f"{x:.2f}")
```

```{python}
hlp = df.filter(regex='^method').drop_duplicates()
df_iter_params = hlp[hlp["method"].isin(["QSA", "Manifold", "InteriorPoint"])]
df_temp_params = hlp[hlp["method"].isin(["SoftSort", "OT4P4AS"])]
df_iter_params.set_index(list(df_iter_params.columns))
df_temp_params.set_index(list(df_iter_params.columns))
```

### Computational Environment

Experiments were executed on a heterogeneous cluster environment (MetaCentrum). This section does not analyze timing variance; hardware specifics are omitted deliberately. Runtime plots are generated for relative method comparison only.


### Network Models and Experimental Design {#sec-network-models}

Our experiments employ three synthetic network models, each serving a specific
purpose in evaluating method performance:

1. **Erdős-Rényi (ER)**: Random graphs with no inherent structure, providing a
baseline for symmetry detection in unstructured networks.

2. **Barabási-Albert (BA)**: Scale-free networks generated through preferential
attachment, representing the heterogeneous degree distributions common in
real-world networks.

3. **Lateral Random Model (LRM)**: Networks with controlled bilateral symmetry,
subsequently perturbed through edge rewiring to create known approximate
symmetries.

The experimental design systematically varies network size (nodes: 20, 50, 100),
edge density (sparse: 15, dense: 40), and for LRM networks, the degree of
asymmetry through rewiring (0, 13, 50, 130 edge modifications). This yields 28
distinct graph configurations spanning three topological families.


### Performance Metrics {#sec-metrics}

We evaluate methods using three primary metrics:

1. **Success Rate**: Proportion of runs finding non-identity solutions,
indicating the method's ability to avoid trivial optima.

2. **Convergence Behavior**: Number of iterations until method convergence or
local optimum is found, revealing computational efficiency and optimization
landscape navigation.

3. **Symmetry Coefficient**: The normalized asymmetry measure $S(A) \in [0,1]$
where lower values indicate better approximate symmetries, as defined
in~\cref{eq:symmetry-coefficient}.


# Analysis of Iterative Methods {#sec-iterative}

We first examine the three iterative optimization methods that operate directly
on the Birkhoff polytope: QSA (the current state-of-the-art), Manifold
Optimization (MF), and the Interior Point method (IP). These methods represent
different algorithmic philosophies: first-order gradient methods (QSA), manifold
optimization (MF), and second-order methods (IP).

```{python}
#| label: filter-iterative
#| code-summary: "Filter data for iterative methods"

# Filter for iterative methods
iterative_methods = ["QSA", "Manifold", "InteriorPoint"]
df_iterative = df[df["method"].isin(iterative_methods)].copy()

# Ensure consistent ordering
graph_types = ALL_GRAPH_TYPES
method_order = ['QSA', 'MF', 'IP']
```

## Identity Avoidance and Parameter Selection {#sec-identity}

Plots show success rate (share of simulations with any non‑identity solution) vs penalty $c$, split by method, graph type, size, density, rewiring. Use them to find the smallest $c$ giving a high stable plateau. 

```{python}
#| label: fig-identity-success
#| fig-cap: "Success rate (proportion of non-identity solutions) as a function of penalty parameter $c$ for iterative methods across different graph types, network sizes, and edge densities. Higher values indicate better performance in avoiding trivial solutions. Note that some elements might overlap."
#| fig-width: 12
#| fig-height: 10

# Calculate success rates for non-identity solutions
identity_data = df_iterative.copy()
identity_data['is_non_identity'] = identity_data["n_nodes"] != identity_data["fixed_points"]

# Aggregate by simulation to check if ANY run was non-identity
success_by_sim = (
    identity_data
    .groupby(["density", "method_display", "graph_type", "n_nodes", "rew", "c", "sim_id"], 
             dropna=False)["is_non_identity"]
    .any()
    .reset_index()
    .rename(columns={'is_non_identity': 'success'})
)

# Calculate proportion of successful simulations
success_rates = (
    success_by_sim
    .groupby(["density", "method_display", "graph_type", "n_nodes", "rew", "c"], 
             dropna=False)["success"]
    .mean()
    .reset_index()
    .rename(columns={'success': 'success_rate'})
)

# Create separate plots for each graph type
# Standard graphs: ER, BA, GEO2D, GEO3D, DD, HK (no rewiring parameter)
standard_graph_types = STANDARD_GRAPH_TYPES
for graph_type in standard_graph_types:
    data_subset = success_rates[success_rates["graph_type"] == graph_type]
    
    # Skip if no data for this graph type
    if data_subset.empty:
        continue
    
    g = sns.relplot(
        data=data_subset,
        x='c', 
        y='success_rate', 
        hue='method_display',
        palette={METHOD_NAMES[m]: METHOD_COLORS[m] for m in ["QSA", "Manifold", "InteriorPoint"]},
        style="density",
        col="n_nodes", 
        row="graph_type",
        kind="line",
        markers=True,
        height=3,
        facet_kws={'sharex': True, 'sharey': True}
    )

    for text in g._legend.get_texts():
        if text.get_text() == 'method_display':
            text.set_text('Method')
        elif text.get_text() == 'density':
            text.set_text('Density')
    
    g.set_axis_labels("Penalty parameter $c$", "Success rate")
    g.set_titles("{row_name} ($n={col_name}$)")
    g.fig.suptitle(f"Identity Avoidance: {graph_type} Network", y=1.04, weight='bold')
    
    save_figure(g.fig, f"identity-success-{graph_type.lower()}")
    plt.show()

# Handle LRM separately due to different structure (has rewiring parameter)
lrm_data = success_rates[success_rates["graph_type"] == "LRM"]
if not lrm_data.empty:
    g = sns.relplot(
        data=lrm_data,
        x='c', 
        y='success_rate', 
        hue='method_display',
        palette={METHOD_NAMES[m]: METHOD_COLORS[m] for m in ["QSA", "Manifold", "InteriorPoint"]},
        style="density",
        col="rew",
        row="n_nodes",
        kind="line",
        markers=True,
        height=3,
        facet_kws={'sharex': True, 'sharey': True}
    )

    for text in g._legend.get_texts():
        if text.get_text() == 'method_display':
            text.set_text('Method')
        elif text.get_text() == 'density':
            text.set_text('Density')
    
    g.set_axis_labels("Penalty parameter $c$", "Success rate")
    g.set_titles("LRM ($n={row_name}$, $k={col_name}$)")
    g.fig.suptitle("Identity Avoidance: LRM Network", y=1.04, weight='bold')
    
    save_figure(g.fig, "identity-success-lrm")
    plt.show()
```

```{python}
#| label: fig-identity-success-unified
#| fig-cap: "Unified view of success rates (proportion of non-identity solutions) as a function of penalty parameter $c$ for iterative methods. Each row corresponds to a graph type, columns show different network sizes, and line styles indicate edge densities. Methods are distinguished by color with a shared legend."
#| fig-width: 14
#| fig-height: 18

# Prepare data for unified plot (standard graph types only)
unified_identity_data = success_rates[success_rates["graph_type"].isin(standard_graph_types)].copy()

# Ensure proper ordering of graph types
unified_identity_data['graph_type'] = pd.Categorical(
    unified_identity_data['graph_type'], 
    categories=standard_graph_types, 
    ordered=True
)

# Create the unified FacetGrid
g = sns.relplot(
    data=unified_identity_data,
    x='c', 
    y='success_rate', 
    hue='method_display',
    palette={METHOD_NAMES[m]: METHOD_COLORS[m] for m in ["QSA", "Manifold", "InteriorPoint"]},
    style="density",
    col="n_nodes", 
    row="graph_type",
    kind="line",
    markers=True,
    height=2.5,
    aspect=1.2,
    facet_kws={'sharex': True, 'sharey': True}
)

# Customize legend labels
for text in g._legend.get_texts():
    if text.get_text() == 'method_display':
        text.set_text('Method')
    elif text.get_text() == 'density':
        text.set_text('Density')

# Move legend to the right side, vertically centered
sns.move_legend(g, "center left", bbox_to_anchor=(1.02, 0.5), frameon=True)

# Set axis labels and titles
g.set_axis_labels("", "")  # Remove individual axis labels
g.set_titles("{row_name} ($n={col_name}$)")

# Add overall title
g.fig.suptitle("Identity Avoidance Across Graph Types", y=1.00, weight='bold', fontsize=24)

# Adjust layout for better spacing with room for common labels
plt.tight_layout(rect=[0.05, 0.05, 0.98, 0.98])

# Add common axis labels after tight_layout
g.fig.text(0.5, 0.03, "Penalty parameter $c$", ha='center', fontsize=18)
g.fig.text(0.03, 0.5, "Success rate", va='center', rotation='vertical', fontsize=18)

# Save and display
save_figure(g.fig, "identity-success-unified")
plt.show()
```

 

## Convergence Analysis {#sec-convergence}

Histograms show iteration counts at stop. Heatmaps show fraction hitting max iterations. Use them to spot heavy tails or high cap rates; then adjust tolerance, init, or max_iter.

```{python}
#| label: fig-iteration-distribution
#| code-summary: "Analyze convergence behavior across methods and graph types"
#| fig-cap: "Distribution of iterations until convergence for iterative methods across different graph types. The spike at 500 iterations indicates instances that reached the maximum iteration limit without converging."
#| fig-width: 10
#| fig-height: 8

# Create iteration distribution plot
g = sns.displot(
    data=df_iterative,
    x="metric_iterations",
    hue="method_display",
    palette={METHOD_NAMES[m]: METHOD_COLORS[m] for m in ["QSA", "Manifold", "InteriorPoint"]},
    col="method_display",
    row="graph_type",
    kind="hist",
    stat="proportion",
    bins=30,
    height=3,
    facet_kws={'sharex': True, 'sharey': True}
)

g._legend.set_title("Method")


g.set_axis_labels("Number of iterations", "Proportion of runs")
g.set_titles("{row_name} - {col_name}")
g.fig.suptitle("Iteration Distribution by Method and Graph Type", y=1.02, weight='bold')

save_figure(g.fig, "iteration-distribution")
plt.show()
```


```{python}
#| label: fig-convergence-heatmap
#| fig-cap: "Non-convergence rates across methods and problem configurations. Values indicate the proportion of runs reaching the maximum iteration limit (500 iterations) without satisfying convergence criteria. Lower values indicate more reliable convergence."
#| fig-width: 11
#| fig-height: 10
#| code-summary: "Create heatmap of non-convergence rates"

# Calculate convergence rates
convergence_data = df_iterative.copy()
convergence_data['hit_max_iter'] = (
    convergence_data["method_max_iter"] == convergence_data["metric_iterations"]
).astype(int)

# Aggregate convergence rates
conv_rates = (
    convergence_data
    .groupby(['method_display', 'graph_type', 'n_nodes', 'density'])['hit_max_iter']
    .agg(['mean', 'count'])
    .reset_index()
    .rename(columns={'mean': 'non_convergence_rate', 'count': 'total_runs'})
)


# --- Create the Discrete Colormap ---
# 1. Choose a base continuous colormap
base_cmap = 'rocket' 

# 2. Define the number of discrete levels you want
n_colors = 20

# 3. Create a new discrete colormap from the base map
discrete_cmap = mcolors.ListedColormap(
    sns.color_palette(base_cmap, n_colors).as_hex()
)

# Determine which graph types have data for iterative methods
available_graph_types = sorted(conv_rates['graph_type'].unique())
n_methods = len(method_order)
n_graph_types = len(available_graph_types)

# Create heatmap grid with dynamic dimensions
fig, axes = plt.subplots(n_methods, n_graph_types, 
                         figsize=(3 * n_graph_types, 3 * n_methods),
                         squeeze=False)

# Generate heatmaps
for i, method in enumerate(method_order):
    for j, graph_type in enumerate(available_graph_types):
        subset = conv_rates[
            (conv_rates['method_display'] == method) & 
            (conv_rates['graph_type'] == graph_type)
        ]
        
        if not subset.empty:
            heatmap_data = subset.pivot(
                index='density', 
                columns='n_nodes', 
                values='non_convergence_rate'
            )
            
            sns.heatmap(
                heatmap_data, 
                ax=axes[i, j], 
                annot=True,
                fmt='.3f',
                vmin=0, 
                vmax=1.0,
                cbar=(j == n_graph_types - 1),
                cmap=discrete_cmap,
            )
            
            axes[i, j].set_title(f'{method} - {graph_type}')
            axes[i, j].set_xlabel('Number of nodes' if i == n_methods - 1 else '')
            axes[i, j].set_ylabel('Edge density' if j == 0 else '')
        else:
            # No data available - show empty plot
            axes[i, j].text(0.5, 0.5, 'No data', 
                          ha='center', va='center',
                          transform=axes[i, j].transAxes)
            axes[i, j].set_title(f'{method} - {graph_type}')
            axes[i, j].axis('off')

plt.suptitle('Non-Convergence Rates: Proportion of Runs Reaching Iteration Limit', y=1.00, weight='bold')
plt.tight_layout()
save_figure(fig, "convergence-heatmap")
plt.show()
```


 

## Symmetry Performance {#sec-symmetry-iterative}

Plots show best symmetry (min per simulation) vs penalty $c$ for non‑identity runs. Look for elbows (diminishing returns), density gaps, and rewiring effects (LRM). Choose $c$ where symmetry is low and success rate is already high.


```{python}
#| label: fig-symmetry-vs-c
#| fig-cap: "Symmetry coefficient as a function of penalty parameter $c$ for non-identity solutions. Lower values indicate better approximate symmetries. Error bars show 95% confidence intervals."
#| fig-width: 12
#| fig-height: 10

# Filter for non-identity solutions only
symmetry_data = df_iterative[
    df_iterative["n_nodes"] != df_iterative["fixed_points"]
].copy()

# Get best symmetry per simulation
best_symmetry = (
    symmetry_data
    .groupby(["density", "method_display", "graph_type", "n_nodes", "rew", "c", "sim_id"], 
             dropna=False)["symmetry"]
    .min()
    .reset_index()
)

# Plot for standard graph types (ER, BA, GEO2D, GEO3D, DD, HK)
standard_graph_types = STANDARD_GRAPH_TYPES
for graph_type in standard_graph_types:
    data_subset = best_symmetry[best_symmetry["graph_type"] == graph_type]
    
    # Skip if no data for this graph type
    if data_subset.empty:
        print(f"Data for {graph_type} is empty! Skipping.")
        continue
    
    g = sns.relplot(
        data=data_subset,
        x="c",
        y="symmetry",
        hue="method_display",
        palette={METHOD_NAMES[m]: METHOD_COLORS[m] for m in ["QSA", "Manifold", "InteriorPoint"]},
        style="density",
        col="n_nodes",
        row="graph_type",
        kind="line",
        markers=True,
        errorbar='ci',
        height=3,
    )
    for text in g._legend.get_texts():
        if text.get_text() == 'method_display':
            text.set_text('Method')
        elif text.get_text() == 'density':
            text.set_text('Density')
    
    g.set_axis_labels("Penalty parameter $c$", "Symmetry coefficient")
    g.set_titles("{row_name} ($n={col_name}$)")
    g.fig.suptitle(f"Symmetry Quality vs Penalty: {graph_type} Networks", y=1.04, weight='bold')
    
    save_figure(g.fig, f"symmetry-vs-c-{graph_type.lower()}")
    plt.show()

# Plot for LRM networks with rewiring faceting
lrm_data = best_symmetry[best_symmetry["graph_type"] == "LRM"]
if not lrm_data.empty:
    # LRM networks have different parameter structure - facet by rewiring
    g = sns.relplot(
        data=lrm_data,
        x="c",
        y="symmetry",
        hue="method_display",
        palette={METHOD_NAMES[m]: METHOD_COLORS[m] for m in ["QSA", "Manifold", "InteriorPoint"]},
        style="density",
        row="n_nodes",
        col="rew",
        kind="line",
        markers=True,
        errorbar='ci',
        height=3,
        facet_kws={'sharex': True, 'sharey': True}
    )
    for text in g._legend.get_texts():
        if text.get_text() == 'method_display':
            text.set_text('Method')
        elif text.get_text() == 'density':
            text.set_text('Density')
    
    g.set_axis_labels("Penalty parameter $c$", "Symmetry coefficient")
    g.set_titles("LRM ($n={row_name}$, $k={col_name}$)")
    g.fig.suptitle("Symmetry Quality vs Penalty: LRM Networks", y=1.04, weight='bold')
    
    # Adjust spacing for the larger grid
    #g.fig.subplots_adjust(hspace=0.15, wspace=0.15)
    
    save_figure(g.fig, "symmetry-vs-c-lrm")
    plt.show()
```

```{python}
#| label: fig-symmetry-vs-c-unified
#| fig-cap: "Unified view of symmetry coefficient as a function of penalty parameter $c$ for non-identity solutions. Each row corresponds to a graph type, columns show different network sizes, and line styles indicate edge densities. Lower values indicate better approximate symmetries. Error bars show 95% confidence intervals."
#| fig-width: 14
#| fig-height: 18

# Prepare data for unified plot (standard graph types only)
unified_symmetry_data = best_symmetry[best_symmetry["graph_type"].isin(standard_graph_types)].copy()

# Ensure proper ordering of graph types
unified_symmetry_data['graph_type'] = pd.Categorical(
    unified_symmetry_data['graph_type'], 
    categories=standard_graph_types, 
    ordered=True
)

# Create the unified FacetGrid
g = sns.relplot(
    data=unified_symmetry_data,
    x='c', 
    y='symmetry', 
    hue='method_display',
    palette={METHOD_NAMES[m]: METHOD_COLORS[m] for m in ["QSA", "Manifold", "InteriorPoint"]},
    style="density",
    col="n_nodes", 
    row="graph_type",
    kind="line",
    markers=True,
    errorbar='ci',
    height=2.5,
    aspect=1.2,
    facet_kws={'sharex': True, 'sharey': True}
)

# Customize legend labels
for text in g._legend.get_texts():
    if text.get_text() == 'method_display':
        text.set_text('Method')
    elif text.get_text() == 'density':
        text.set_text('Density')

# Move legend to the right side, vertically centered
sns.move_legend(g, "center left", bbox_to_anchor=(1.02, 0.5), frameon=True)

# Set axis labels and titles
g.set_axis_labels("", "")  # Remove individual axis labels
g.set_titles("{row_name} ($n={col_name}$)")

# Add overall title
g.fig.suptitle("Symmetry Quality Across Graph Types", y=1.00, weight='bold', fontsize=24)

# Adjust layout for better spacing with room for common labels
plt.tight_layout(rect=[0.05, 0.05, 0.98, 0.98])

# Add common axis labels after tight_layout
g.fig.text(0.5, 0.03, "Penalty parameter $c$", ha='center', fontsize=18)
g.fig.text(0.03, 0.5, "Symmetry coefficient", va='center', rotation='vertical', fontsize=18)

# Save and display
save_figure(g.fig, "symmetry-vs-c-unified")
plt.show()
```

```{python}
#| label: voting-functions
#| code-summary: "Define Condorcet and Borda voting functions for parameter selection"

def find_condorcet_winner(rankings):
    """
    Find Condorcet winner from ranked preferences.
    
    A Condorcet winner is a candidate that would win in a head-to-head 
    comparison against every other candidate.
    
    Parameters
    ----------
    rankings : dict
        Dictionary mapping stratum to list of (config, score) tuples,
        ordered by preference (best first)
        
    Returns
    -------
    tuple or None
        Configuration that is Condorcet winner, or None if no winner exists
    """
    # Collect all unique configurations
    all_configs = set()
    for ranking in rankings.values():
        for config, _ in ranking:
            all_configs.add(config)
    
    all_configs = list(all_configs)
    
    if len(all_configs) == 0:
        return None
    
    # Compute pairwise victories
    for candidate in all_configs:
        is_condorcet = True
        
        for opponent in all_configs:
            if candidate == opponent:
                continue
            
            # Count how many "voters" (strata) prefer candidate over opponent
            candidate_wins = 0
            opponent_wins = 0
            
            for stratum, ranking in rankings.items():
                candidate_rank = None
                opponent_rank = None
                
                for rank, (config, _) in enumerate(ranking):
                    if config == candidate:
                        candidate_rank = rank
                    if config == opponent:
                        opponent_rank = rank
                
                # If both found, compare ranks (lower is better)
                if candidate_rank is not None and opponent_rank is not None:
                    if candidate_rank < opponent_rank:
                        candidate_wins += 1
                    elif opponent_rank < candidate_rank:
                        opponent_wins += 1
            
            # Candidate must beat opponent in pairwise comparison
            if opponent_wins >= candidate_wins:
                is_condorcet = False
                break
        
        if is_condorcet:
            return candidate
    
    return None


def borda_count(rankings):
    """
    Select winner using Borda count voting.
    
    Each stratum assigns points: highest rank gets n-1 points,
    second gets n-2, etc., where n is number of candidates.
    
    Parameters
    ----------
    rankings : dict
        Dictionary mapping stratum to list of (config, score) tuples
        
    Returns
    -------
    tuple
        Configuration with highest Borda score
    """
    scores = {}
    
    for stratum, ranking in rankings.items():
        n = len(ranking)
        for rank, (config, _) in enumerate(ranking):
            points = n - rank - 1  # Top gets n-1, second gets n-2, etc.
            scores[config] = scores.get(config, 0) + points
    
    if not scores:
        return None
    
    return max(scores.items(), key=lambda x: x[1])[0]
```

```{python}
#| label: tbl-optimal-c
#| tbl-cap: "Optimal penalty parameter values for each method selected using ranked voting across graph topologies, network sizes, and densities. Only configurations achieving perfect success rate (100% identity avoidance) are considered."

def rank_c_values_stratified(method_data, symmetry_data, method_name):
    """
    Rank c values separately for each (graph_type, n_nodes, density) stratum.
    Only considers c values that achieve perfect success rate globally across ALL strata.
    
    Parameters
    ----------
    method_data : pd.DataFrame
        Success data filtered for one method
    symmetry_data : pd.DataFrame  
        Symmetry data filtered for one method
    method_name : str
        Display name of the method
        
    Returns
    -------
    dict
        Rankings by stratum: {(graph_type, n_nodes, density): [(c, score), ...]}
    """
    # First pass: identify c values with perfect global success rate
    global_c_metrics = (
        method_data
        .groupby('c')
        .agg(
            successful_sims=('success', 'sum'),
            total_sims=('success', 'count'),
            success_rate=('success', 'mean')
        )
        .reset_index()
    )
    
    # Filter to c values with perfect success rate globally
    perfect_c_values = set(global_c_metrics[global_c_metrics['success_rate'] > 0.99]['c'].values)
    
    if not perfect_c_values:
        return {}
    
    rankings = {}
    
    # Second pass: rank only the perfect c values within each stratum
    for (graph_type, n_nodes, density), group in method_data.groupby(['graph_type', 'n_nodes', 'density']):
        # Get symmetry scores for each globally-perfect c value in this stratum
        stratum_symmetry = symmetry_data[
            (symmetry_data['graph_type'] == graph_type) &
            (symmetry_data['n_nodes'] == n_nodes) &
            (symmetry_data['density'] == density)
        ]
        
        c_scores = []
        for c_val in perfect_c_values:
            c_sym = stratum_symmetry[stratum_symmetry['c'] == c_val]['symmetry']
            
            if len(c_sym) > 0:
                # Score: negative mean symmetry (lower symmetry = higher score)
                score = -c_sym.mean()
                c_scores.append({
                    'c': c_val,
                    'score': score,
                    'mean_symmetry': c_sym.mean()
                })
        
        if c_scores:
            # Sort by score (highest first)
            c_scores = sorted(c_scores, key=lambda x: x['score'], reverse=True)
            rankings[(graph_type, n_nodes, density)] = [(x['c'], x['score']) for x in c_scores]
    
    return rankings


# Find optimal c values using ranked voting across strata
optimal_c = []
voting_details_c = []

for method in ["QSA", "Manifold", "InteriorPoint"]:
    method_display = METHOD_NAMES[method]
    method_success = success_by_sim[success_by_sim["method_display"] == method_display]
    method_symmetry = symmetry_data[symmetry_data["method_display"] == method_display]
    
    # Get rankings for each stratum
    rankings = rank_c_values_stratified(method_success, method_symmetry, method_display)
    
    if not rankings:
        continue
    
    # Find Condorcet winner among c values
    condorcet_c = find_condorcet_winner(rankings)
    
    # If no Condorcet winner, use Borda count
    if condorcet_c is None:
        winner_c = borda_count(rankings)
        winner_type = "Borda Count"
    else:
        winner_c = condorcet_c
        winner_type = "Condorcet"
    
    if winner_c is None:
        continue
    
    # Calculate overall statistics for winner
    winner_data = method_success[method_success['c'] == winner_c]
    winner_symmetry = method_symmetry[method_symmetry['c'] == winner_c]
    
    optimal_c.append({
        'Method': method_display,
        'Selection': winner_type,
        'Optimal c': winner_c,
        'Success Rate': winner_data['success'].mean() if len(winner_data) > 0 else np.nan,
        'Mean Symmetry': winner_symmetry['symmetry'].mean() if len(winner_symmetry) > 0 else np.nan,
        'Median Symmetry': winner_symmetry['symmetry'].median() if len(winner_symmetry) > 0 else np.nan
    })
    
    # Store top 3 c values for each stratum (for detailed display)
    for stratum, ranking in rankings.items():
        graph_type, n_nodes, density = stratum
        for rank, (c_val, score) in enumerate(ranking[:3]):
            # Get metrics for this c value in this stratum
            stratum_success = method_success[
                (method_success['graph_type'] == graph_type) &
                (method_success['n_nodes'] == n_nodes) &
                (method_success['density'] == density) &
                (method_success['c'] == c_val)
            ]
            
            stratum_symmetry = method_symmetry[
                (method_symmetry['graph_type'] == graph_type) &
                (method_symmetry['n_nodes'] == n_nodes) &
                (method_symmetry['density'] == density) &
                (method_symmetry['c'] == c_val)
            ]
            
            voting_details_c.append({
                'Method': method_display,
                'Graph Type': graph_type,
                'n': n_nodes,
                'rho': density,
                'Rank': rank + 1,
                'c': c_val,
                'Success Rate': stratum_success['success'].mean() if len(stratum_success) > 0 else np.nan,
                'Mean Symmetry': stratum_symmetry['symmetry'].mean() if len(stratum_symmetry) > 0 else np.nan
            })

optimal_c_df = pd.DataFrame(optimal_c)
print(optimal_c_df.to_markdown(index=False, floatfmt=".4f"))
save_table(optimal_c_df, "optimal_parameters_iterative", 
           "Optimal penalty parameters for iterative methods (ranked voting)",
           label="tab:optimal-parameters-iterative"
           )

# Display voting details for transparency
voting_details_c_df = pd.DataFrame(voting_details_c)
print("\n" + "="*80)
print("Top 3 c Values per Stratum (Graph Type × Size × Density):")
print("="*80)
for method in [METHOD_NAMES[m] for m in ["QSA", "Manifold", "InteriorPoint"]]:
    print(f"\n{method} Method:")
    method_votes = voting_details_c_df[voting_details_c_df['Method'] == method]
    if not method_votes.empty:
        # Show sample of voting details (first 15 rows to avoid overwhelming output)
        print(method_votes[['Graph Type', 'n', 'rho', 'Rank', 'c', 'Mean Symmetry']].head(15).to_markdown(index=False, floatfmt=".4f"))
        if len(method_votes) > 15:
            print(f"... ({len(method_votes) - 15} more strata)")
```

Optimal $c$ table: smallest $c$ giving the max number of successful simulations per method. Later plots use these values.


```{python}
#| label: fig-symmetry-comparison
#| fig-cap: "Comparison of symmetry coefficients achieved by each method using optimal penalty parameters. Box plots show the distribution across all simulations, with lower values indicating better performance."
#| fig-width: 12
#| fig-height: 8

# Use optimal c values from earlier analysis
optimal_c_dict = {
    d["Method"]: d["Optimal c"] for d in optimal_c
}

# Filter data for optimal c values
optimal_data = symmetry_data[
    symmetry_data.apply(
        lambda x: x['c'] == optimal_c_dict.get(x['method_display']), 
        axis=1
    )
]

# Get best result per simulation
optimal_best = (
    optimal_data
    .groupby(["density", "method_display", "graph_type", "n_nodes", "rew", "sim_id"], 
             dropna=False)["symmetry"]
    .min()
    .reset_index()
)

# Create comparison plots for standard graph types
standard_graph_types = STANDARD_GRAPH_TYPES
for graph_type in standard_graph_types:
    data_subset = optimal_best[optimal_best["graph_type"] == graph_type]
    
    # Skip if no data for this graph type
    if data_subset.empty:
        continue
    
    g = sns.catplot(
        data=data_subset,
        x="n_nodes",
        y="symmetry", 
        hue="method_display",
        palette={METHOD_NAMES[m]: METHOD_COLORS[m] for m in ["QSA", "Manifold", "InteriorPoint"]},
        row="graph_type",
        col="density",
        kind="box",
        showfliers=False,
        height=3,
        aspect=1.5,
        dodge=True
    )

    g._legend.set_title("Method")
    g.set_axis_labels("Number of nodes", "Symmetry coefficient")
    g.set_titles("{row_name} ($\\rho={col_name}$)")
    g.fig.suptitle(f"Method Comparison: {graph_type} Networks", y=1.04, weight='bold')
    
    save_figure(g.fig, f"symmetry-comparison-{graph_type.lower()}")
    plt.show()

# LRM comparison (special case with rewiring parameter)
lrm_best = optimal_best[optimal_best["graph_type"] == "LRM"]
if not lrm_best.empty:
    g = sns.catplot(
        data=lrm_best,
        x="rew",
        y="symmetry",
        hue="method_display",
        palette={METHOD_NAMES[m]: METHOD_COLORS[m] for m in ["QSA", "Manifold", "InteriorPoint"]},
        col="n_nodes",
        row="density",
        kind="box",
        showfliers=False,
        height=3,
        aspect=1.5,
        dodge=True
    )
    
    g._legend.set_title("Method")
    g.set_axis_labels("Number of rewirings", "Symmetry coefficient")
    g.set_titles("LRM ($n={col_name}$, $\\rho={row_name}$)")
    g.fig.suptitle("Method Comparison: LRM Networks", y=1.04, weight='bold')
    
    save_figure(g.fig, "symmetry-comparison-lrm")
    plt.show()
```

```{python}
#| label: fig-symmetry-comparison-unified
#| fig-cap: "Unified comparison of symmetry coefficients achieved by iterative methods using optimal penalty parameters across graph types. Each row corresponds to a graph type, columns show different edge densities. Box plots display distribution across simulations with lower values indicating better approximate symmetries. Methods are distinguished by color with a shared legend."
#| fig-width: 14
#| fig-height: 18

# Prepare data for unified plot (standard graph types only)
unified_symmetry_data = optimal_best[optimal_best["graph_type"].isin(standard_graph_types)].copy()

# Ensure proper ordering of graph types
unified_symmetry_data['graph_type'] = pd.Categorical(
    unified_symmetry_data['graph_type'], 
    categories=standard_graph_types, 
    ordered=True
)

# Create the unified FacetGrid
g = sns.catplot(
    data=unified_symmetry_data,
    x="n_nodes",
    y="symmetry", 
    hue="method_display",
    palette={METHOD_NAMES[m]: METHOD_COLORS[m] for m in ["QSA", "Manifold", "InteriorPoint"]},
    row="graph_type",
    col="density",
    kind="box",
    showfliers=False,
    height=2.5,
    aspect=1.2,
    dodge=True
)

# Customize legend
g._legend.set_title("Method")

# Move legend to the right side, vertically centered
sns.move_legend(g, "center left", bbox_to_anchor=(1.02, 0.5), frameon=True)

# Set titles
g.set_titles("{row_name} ($\\rho={col_name}$)")


# Add overall title
g.fig.suptitle("Symmetry Quality Comparison", y=1.00, weight='bold', fontsize=24)

# Adjust layout with room for common labels
plt.tight_layout()

# Add common axis labels after tight_layout
g.set_axis_labels("", "")  # Remove individual axis labels
g.fig.text(0.5, 0.01, "Number of nodes", ha='center', fontsize=18)
g.fig.text(0.01, 0.5, "Symmetry coefficient", va='center', rotation='vertical', fontsize=18)

# Save and display
save_figure(g.fig, "symmetry-comparison-unified")
plt.show()
```


### Scalability Analysis and Method Comparison

Boxplots: symmetry at chosen $c$ across graph type, size, density, rewiring. Use spread for variability, median for typical value, outliers for hard cases.


```{python}
#| label: tbl-symmetry-summary
#| tbl-cap: "Summary statistics for symmetry coefficients achieved by each method across different graph types using optimal parameters"

# Calculate summary statistics
summary_stats = []
for method in ["QSA", "Manifold", "InteriorPoint"]:
    optimal_best_method = optimal_best[optimal_best["method_display"] == METHOD_NAMES[method]]
    
    groups = (
        optimal_best_method
        .groupby(["graph_type", "n_nodes", "density", "rew"], dropna=False)
    )
    for group_id, group_data in groups:
        graph_type, n_nodes, density, rew = group_id

        method_graph_data = group_data['symmetry']
        if not method_graph_data.empty:
            summary_stats.append({
                'Method': METHOD_NAMES[method],
                'Network': graph_type,
                'Nodes': n_nodes,
                'Density': density,
                'Rewirings': '' if pd.isna(rew) else str(rew),
                'Median': method_graph_data.median(),
                'Mean': method_graph_data.mean(),
                'Std Dev': method_graph_data.std(), 
            })

summary_df = pd.DataFrame(summary_stats)
print(summary_df.to_markdown(index=False, floatfmt=".4f"))
save_table(
    summary_df.set_index(["Method", "Network", "Nodes", "Density", "Rewirings"]),
    "symmetry_summary_iterative",
    "Summary statistics for symmetry coefficients achieved by iterative methods",
    label="tab:symmetry-summary-iterative",
    #header=["Method", "Network", "$n$", r"\rho", "$k$", "Median", "Mean", "SD", "Min", "Max"]
)
```

```{python}
# # Create composite grouping that preserves rewiring information
# def create_graph_rewiring_key(row):
#     if pd.isna(row['rew']):
#         return (row['graph_type'], '')
#     else:
#         return (row['graph_type'], row['rew'])
# 
# # Calculate summary statistics with proper multi-index grouping
# summary_stats = []
# for method in ["QSA", "Manifold", "InteriorPoint"]:
#     method_data = optimal_best[optimal_best["method_display"] == METHOD_NAMES[method]]
#     
#     for (graph_type, rew_label), group_data in method_data.groupby(['graph_type', 'rew'], dropna=False):
#         symmetry_data = group_data["symmetry"]
#         
#         summary_stats.append({
#             'Method': METHOD_NAMES[method],
#             'Graph Type': graph_type,
#             'Rewiring': rew_label,
#             'Mean': symmetry_data.mean(),
#             'Median': symmetry_data.median(),
#             'Std Dev': symmetry_data.std(), 
#             'Min': symmetry_data.min(),
#             'Max': symmetry_data.max(),
#         })
# 
# # Create DataFrame with multi-index
# summary_df = pd.DataFrame(summary_stats)
# 
# print(summary_df.to_markdown(floatfmt=".4f", index=False))
# save_table(
#     summary_df.set_index(["Method", "Graph Type",]),
#     "symmetry_summary_with_rewiring",
#     "Summary statistics for symmetry coefficients with rewiring parameters",
#     label="tab:symmetry-summary-iterative",
# )
```

Quick scan rule: low median + tight box = stable; wide box = sensitive.

# Analysis of Temperature-Based Methods {#sec-temperature}

We now examine the two temperature-based methods: Dimensionality Reduction (DR) 
and Orthogonal Relaxation (OR). These methods employ continuous relaxation 
controlled by a temperature parameter τ, balancing between continuous optimization 
(high τ) and discrete constraints (low τ). This section first analyzes the 
fundamental behavior using convex loss functions without annealing, then extends 
to advanced comparisons of loss formulations and annealing strategies.

```{python}
#| label: filter-temperature
#| code-summary: "Filter data for temperature-based methods"

# Select temperature-based methods (using current method names)
temperature_methods = ["DimensionalityReduction", "OrthogonalRelaxation"]

df_temp = (
    df
    .query("method in @temperature_methods")
    .assign(
        # Add display name mapping
        method_display=lambda x: x['method'].map(METHOD_NAMES),
        # Identify annealing runs
        is_anneal=lambda x: ~x['method_final_tau'].isna() if 'method_final_tau' in x.columns else False,
        # Create combined method/loss identifier
        method_loss_combined=lambda x: x['method_display'] + '/' + x['method_loss']
    )
    .copy()
)

# Display dataset summary
print(f"Temperature-based methods data shape: {df_temp.shape}")
print(f"\nMethods: {', '.join(df_temp['method_display'].unique())}")
print(f"Loss functions: {', '.join(df_temp['method_loss'].unique())}")
print(f"\nTemperature ranges by method:")
for method in df_temp['method_display'].unique():
    tau_range = df_temp.query("method_display == @method")['method_initial_tau'].unique()
    print(f"  {method}: {sorted(tau_range)}")
```

```{python}
#| label: filter-temperature-basic
#| code-summary: "Filter data for basic temperature analysis (convex loss, no annealing)"

# Filter for convex loss without annealing
df_temp_basic = df_temp[
    (df_temp['method_loss'] == 'convex') & 
    (~df_temp['is_anneal'])
].copy()

print(f"\nBasic temperature analysis data shape: {df_temp_basic.shape}")
print(f"Methods: {', '.join(df_temp_basic['method_display'].unique())}")
print(f"\nParameter ranges:")
print(f"  Temperature (τ): {sorted(df_temp_basic['method_initial_tau'].unique())}")
print(f"  Penalty (c): {sorted(df_temp_basic['c'].unique())}")
```

## Basic Parameter Analysis {#sec-temperature-basic}

We begin by analyzing the fundamental effects of temperature (τ) and penalty (c) 
parameters using convex loss functions without annealing. This establishes baseline 
understanding before examining more complex loss formulations.

### Identity Avoidance {#sec-temperature-identity}

Similar to iterative methods, temperature-based methods must avoid converging to 
trivial identity solutions. We examine how the penalty parameter c affects success 
rates across different temperature values.

```{python}
#| label: temperature-identity-data
#| code-summary: "Prepare identity avoidance data for temperature methods"

# Calculate success rates for non-identity solutions
identity_data_temp = df_temp_basic.copy()
identity_data_temp['is_non_identity'] = (
    identity_data_temp["n_nodes"] != identity_data_temp["fixed_points"]
)

# Aggregate by simulation to check if ANY run was non-identity
success_by_sim_temp = (
    identity_data_temp
    .groupby([
        "method_display", "graph_type", "n_nodes", "density", "rew", 
        "method_initial_tau", "c", "sim_id"
    ], dropna=False)["is_non_identity"]
    .any()
    .reset_index()
    .rename(columns={'is_non_identity': 'success'})
)

# Calculate proportion of successful simulations
success_rates_temp = (
    success_by_sim_temp
    .groupby([
        "method_display", "graph_type", "n_nodes", "density", "rew",
        "method_initial_tau", "c"
    ], dropna=False)["success"]
    .mean()
    .reset_index()
    .rename(columns={'success': 'success_rate'})
)
```

```{python}
#| label: fig-temperature-identity-separate
#| fig-cap: "Success rate (proportion of non-identity solutions) as a function of penalty parameter c for temperature-based methods across different graph types, network sizes, edge densities, and temperature values. Each subplot shows one temperature value."
#| fig-width: 14
#| fig-height: 10

# Create separate plots for each graph type and method
standard_graph_types = STANDARD_GRAPH_TYPES

for method in ["DR", "OR"]:
    for graph_type in standard_graph_types:
        data_subset = success_rates_temp[
            (success_rates_temp["graph_type"] == graph_type) &
            (success_rates_temp["method_display"] == method)
        ]
        
        if data_subset.empty:
            continue
        
        # Use deep palette starting from the 6th color for densities
        density_levels = sorted(data_subset['density'].unique())
        density_colors = sns.color_palette("deep")[5:]
        density_palette = {d: density_colors[i % len(density_colors)] for i, d in enumerate(density_levels)}

        g = sns.relplot(
            data=data_subset,
            x='c',
            y='success_rate',
            hue='density',
            hue_order=density_levels,
            palette=density_palette,
            style='method_initial_tau',
            col='n_nodes',
            kind='line',
            markers=True,
            size=5,
            height=3,
            aspect=1.2,
            facet_kws={'sharex': True, 'sharey': True}
        )
        
        # Customize legend
        for text in g._legend.get_texts():
            if text.get_text() == 'density':
                text.set_text('Density')
            elif text.get_text() == 'method_initial_tau':
                text.set_text(r'Temperature $\tau$')
        
        g.set_axis_labels("Penalty parameter $c$", "Success rate")
        g.set_titles("$n={col_name}$")
        g.fig.suptitle(
            f"Identity Avoidance: {method} Method on {graph_type} Networks",
            y=1.02, weight='bold'
        )
        
        # Set x-axis to log scale
        for ax in g.axes.flat:
            ax.set_xscale('log')
        
        save_figure(g.fig, f"temperature-identity-{method.lower()}-{graph_type.lower()}")
        plt.show()

# Handle LRM separately
for method in ["DR", "OR"]:
    lrm_data = success_rates_temp[
        (success_rates_temp["graph_type"] == "LRM") &
        (success_rates_temp["method_display"] == method)
    ]
    
    if not lrm_data.empty:
        # Use deep palette starting from the 6th color for densities
        density_levels_lrm = sorted(lrm_data['density'].unique())
        density_colors_lrm = sns.color_palette("deep")[5:]
        density_palette_lrm = {d: density_colors_lrm[i % len(density_colors_lrm)] for i, d in enumerate(density_levels_lrm)}

        g = sns.relplot(
            data=lrm_data,
            x='c',
            y='success_rate',
            hue='density',
            hue_order=density_levels_lrm,
            palette=density_palette_lrm,
            style='method_initial_tau',
            col='rew',
            row='n_nodes',
            kind='line',
            markers=True,
            height=3,
            aspect=1.0,
            facet_kws={'sharex': True, 'sharey': True},
        )
        
        for text in g._legend.get_texts():
            if text.get_text() == 'density':
                text.set_text('Density')
            elif text.get_text() == 'method_initial_tau':
                text.set_text(r'Temperature $\tau$')

        # Set x-axis to log scale
        for ax in g.axes.flat:
            ax.set_xscale('log')
        
        g.set_axis_labels("Penalty parameter $c$", "Success rate")
        g.set_titles("$n={row_name}$, $k={col_name}$")
        g.fig.suptitle(
            f"Identity Avoidance: {method} Method on LRM Networks",
            y=1.02, weight='bold'
        )
        
        save_figure(g.fig, f"temperature-identity-{method.lower()}-lrm")
        plt.show()
```

```{python}
#| label: fig-temperature-identity-unified
#| fig-cap: "Unified view of success rates for temperature-based methods across graph types. Each row represents a graph type, columns show network sizes, and line styles distinguish temperature values. Colors indicate edge densities."
#| fig-width: 16
#| fig-height: 20

# Create unified plots for each method
for method in ["DR", "OR"]:
    unified_data = success_rates_temp[
        (success_rates_temp["graph_type"].isin(standard_graph_types)) &
        (success_rates_temp["method_display"] == method)
    ].copy()
    
    if unified_data.empty:
        continue
    
    # Ensure proper ordering
    unified_data['graph_type'] = pd.Categorical(
        unified_data['graph_type'],
        categories=standard_graph_types,
        ordered=True
    )
    
    g = sns.relplot(
        data=unified_data,
        x='c',
        y='success_rate',
        hue='density',
        style='method_initial_tau',
        col='n_nodes',
        row='graph_type',
        kind='line',
        markers=True,
        height=2.5,
        aspect=1.2,
        facet_kws={'sharex': True, 'sharey': True}
    )
    
    # Customize legend
    for text in g._legend.get_texts():
        if text.get_text() == 'density':
            text.set_text('Density')
        elif text.get_text() == 'method_initial_tau':
            text.set_text(r'Temperature $\tau$')
    
    # Move legend to the right
    sns.move_legend(g, "center left", bbox_to_anchor=(1.02, 0.5), frameon=True)
    
    g.set_titles("{row_name} ($n={col_name}$)")
    g.fig.suptitle(
        f"Identity Avoidance: {method} Method Across Graph Types",
        y=1.00, weight='bold', fontsize=24
    )
    
    # Set x-axis to log scale
    for ax in g.axes.flat:
        ax.set_xscale('log')
    
    # Adjust layout
    plt.tight_layout(rect=[0.05, 0.05, 0.98, 0.98])
    
    # Add common axis labels
    g.set_axis_labels("", "")  # Remove individual axis labels
    g.fig.text(0.5, 0.01, "Penalty parameter $c$", ha='center', fontsize=18)
    g.fig.text(0.01, 0.5, "Success rate", va='center', rotation='vertical', fontsize=18)
    
    save_figure(g.fig, f"temperature-identity-unified-{method.lower()}")
    plt.show()
```

### Temperature Effects on Symmetry Quality {#sec-temperature-tau-effects}

We examine how the temperature parameter τ affects solution quality across 
different penalty values and graph configurations.

```{python}
#| label: temperature-tau-data
#| code-summary: "Prepare data for temperature effects analysis"

# Filter for non-identity solutions
temp_symmetry_basic = df_temp_basic[
    df_temp_basic["n_nodes"] != df_temp_basic["fixed_points"]
].copy()

# Get best symmetry per simulation
temp_best_basic = (
    temp_symmetry_basic
    .groupby([
        "method_display", "graph_type", "n_nodes", "density", "rew",
        "method_initial_tau", "c", "sim_id"
    ], dropna=False)['symmetry']
    .min()
    .reset_index()
)
```

```{python}
#| label: fig-temperature-tau-separate
#| fig-cap: "Effect of temperature parameter τ on symmetry quality across penalty values c shown as heatmaps. Lower symmetry coefficients (darker colors) indicate better solutions. Rows represent edge densities, columns show network sizes. Temperature τ on y-axis, penalty c on x-axis."
#| fig-width: 14
#| fig-height: 10

# Compute global min/max for consistent colorbar across all figures
global_vmin = temp_best_basic['symmetry'].min()
global_vmax = temp_best_basic['symmetry'].max()

# Create heatmap plots for each graph type and method
for method in ["DR", "OR"]:
    for graph_type in standard_graph_types:
        data_subset = temp_best_basic[
            (temp_best_basic["graph_type"] == graph_type) &
            (temp_best_basic["method_display"] == method)
        ]
        
        if data_subset.empty:
            continue
        
        # Compute mean symmetry for each (tau, c) combination
        heatmap_data = (
            data_subset
            .groupby(['density', 'n_nodes', 'method_initial_tau', 'c'])['symmetry']
            .mean()
            .reset_index()
        )
        
        # Get unique values for layout
        densities = sorted(heatmap_data['density'].unique())
        n_nodes_vals = sorted(heatmap_data['n_nodes'].unique())
        
        # Create subplot grid
        n_rows = len(densities)
        n_cols = len(n_nodes_vals)
        fig, axes = plt.subplots(n_rows, n_cols, figsize=(2*n_cols, 1.75*n_rows), 
                                 squeeze=False, constrained_layout=True)
        
        # Use global min/max for consistent colorbar across all figures
        vmin = global_vmin
        vmax = global_vmax
        
        # Use Tufte-inspired colormap: simple sequential, minimal decoration
        cmap = 'viridis_r'  # Reversed so dark = low (better)
        
        # Create heatmaps
        for i, density in enumerate(densities):
            for j, n_nodes in enumerate(n_nodes_vals):
                ax = axes[i, j]
                
                # Filter data for this subplot
                subset = heatmap_data[
                    (heatmap_data['density'] == density) &
                    (heatmap_data['n_nodes'] == n_nodes)
                ]
                
                if not subset.empty:
                    # Pivot for heatmap
                    pivot = subset.pivot(
                        index='method_initial_tau',
                        columns='c',
                        values='symmetry'
                    )
                    
                    # Create heatmap
                    sns.heatmap(
                        pivot,
                        ax=ax,
                        cmap=cmap,
                        vmin=vmin,
                        vmax=vmax,
                        cbar=False,  # We'll add a single colorbar later
                        linewidths=0.5,
                        linecolor='white',
                        square=False
                    )
                    
                    # Tufte-inspired: minimal labels
                    if j == 0:
                        ax.set_ylabel(r'$\tau$', fontsize=12)
                    else:
                        ax.set_ylabel('')
                        ax.set_yticklabels([])
                    
                    if i == n_rows - 1:
                        ax.set_xlabel('$c$', fontsize=12)
                    else:
                        ax.set_xlabel('')
                        ax.set_xticklabels([])
                    
                    # Title for each subplot
                    ax.set_title(f'$\\rho={density}$, $n={n_nodes}$', fontsize=11)
                else:
                    ax.axis('off')
        
        # Add single colorbar
        fig.colorbar(
            plt.cm.ScalarMappable(
                norm=mcolors.Normalize(vmin=vmin, vmax=vmax),
                cmap=cmap
            ),
            ax=axes,
            label='Mean Symmetry Coefficient',
            aspect=30,
            pad=0.02
        )
        
        fig.suptitle(
            f"Temperature vs Penalty Effects: {method} Method on {graph_type} Networks",
            fontsize=14, weight='bold',
        )
        
        save_figure(fig, f"temperature-tau-{method.lower()}-{graph_type.lower()}")
        plt.show()
        plt.close(fig)

# Handle LRM separately
for method in ["DR", "OR"]:
    lrm_data = temp_best_basic[
        (temp_best_basic["graph_type"] == "LRM") &
        (temp_best_basic["method_display"] == method)
    ]
    
    if not lrm_data.empty:
        # Compute mean symmetry for each (tau, c) combination
        heatmap_data = (
            lrm_data
            .groupby(['density', 'rew', 'method_initial_tau', 'c'])['symmetry']
            .mean()
            .reset_index()
        )
        
        # Get unique values for layout
        densities = sorted(heatmap_data['density'].unique())
        rew_vals = sorted(heatmap_data['rew'].dropna().unique())
        
        # Create subplot grid
        n_rows = len(densities)
        n_cols = len(rew_vals)
        fig, axes = plt.subplots(n_rows, n_cols, figsize=(2.5*n_cols, 2*n_rows),
                                 squeeze=False, constrained_layout=True)
        
        # Use global min/max for consistent colorbar across all figures
        vmin = global_vmin
        vmax = global_vmax
        cmap = 'viridis_r'
        
        # Create heatmaps
        for i, density in enumerate(densities):
            for j, rew in enumerate(rew_vals):
                ax = axes[i, j]
                
                subset = heatmap_data[
                    (heatmap_data['density'] == density) &
                    (heatmap_data['rew'] == rew)
                ]
                
                if not subset.empty:
                    pivot = subset.pivot(
                        index='method_initial_tau',
                        columns='c',
                        values='symmetry'
                    )
                    
                    sns.heatmap(
                        pivot,
                        ax=ax,
                        cmap=cmap,
                        vmin=vmin,
                        vmax=vmax,
                        cbar=False,
                        linewidths=0.5,
                        linecolor='white',
                        square=False
                    )
                    
                    if j == 0:
                        ax.set_ylabel(r'$\tau$', fontsize=12)
                    else:
                        ax.set_ylabel('')
                        ax.set_yticklabels([])
                    
                    if i == n_rows - 1:
                        ax.set_xlabel('$c$', fontsize=12)
                    else:
                        ax.set_xlabel('')
                        ax.set_xticklabels([])
                    
                    ax.set_title(f'$\\rho={density}$, $k={int(rew)}$', fontsize=11)
                else:
                    ax.axis('off')
        
        # Add single colorbar
        fig.colorbar(
            plt.cm.ScalarMappable(
                norm=mcolors.Normalize(vmin=vmin, vmax=vmax),
                cmap=cmap
            ),
            ax=axes,
            label='Mean Symmetry Coefficient',
            aspect=30,
            pad=0.02
        )
        
        fig.suptitle(
            f"Temperature vs Penalty Effects: {method} Method on LRM Networks ($n=100$)",
            fontsize=14, weight='bold',
        )
        
        save_figure(fig, f"temperature-tau-{method.lower()}-lrm")
        plt.show()
        plt.close(fig)
```

```{python}
#| label: fig-temperature-tau-unified
#| fig-cap: "Unified heatmap view of temperature vs penalty effects across graph types. Each row represents a graph type, columns show different network sizes. Within each heatmap, temperature τ is on y-axis and penalty c on x-axis. Darker colors indicate better (lower) symmetry coefficients."
#| fig-width: 16
#| fig-height: 22

# Create unified heatmap plots for each method
for method in ["DR", "OR"]:
    unified_data = temp_best_basic[
        (temp_best_basic["graph_type"].isin(standard_graph_types)) &
        (temp_best_basic["method_display"] == method)
    ].copy()
    
    if unified_data.empty:
        continue
    
    # Ensure proper ordering
    unified_data['graph_type'] = pd.Categorical(
        unified_data['graph_type'],
        categories=standard_graph_types,
        ordered=True
    )
    
    # Compute mean symmetry for each combination
    heatmap_data = (
        unified_data
        .groupby(['graph_type', 'n_nodes', 'density', 'method_initial_tau', 'c'])['symmetry']
        .mean()
        .reset_index()
    )
    
    # Get unique values for layout
    graph_types_present = sorted(heatmap_data['graph_type'].unique())
    n_nodes_vals = sorted(heatmap_data['n_nodes'].unique())
    
    # Create subplot grid: rows = graph types, cols = n_nodes
    n_rows = len(graph_types_present)
    n_cols = len(n_nodes_vals)
    
    # Two density levels means we'll show 2 heatmaps per cell (side by side)
    # So we need n_cols * 2 actual columns
    fig = plt.figure(figsize=(5*n_cols*2, 3.5*n_rows), constrained_layout=True)
    gs = fig.add_gridspec(n_rows, n_cols*2, hspace=0.15, wspace=0.15)
    
    # Use global min/max for consistent colorbar across all figures
    vmin = global_vmin
    vmax = global_vmax
    cmap = 'viridis_r'  # Tufte-inspired: dark = better (lower symmetry)
    
    axes = []
    for i, graph_type in enumerate(graph_types_present):
        for j, n_nodes in enumerate(n_nodes_vals):
            # Get densities for this combination
            subset = heatmap_data[
                (heatmap_data['graph_type'] == graph_type) &
                (heatmap_data['n_nodes'] == n_nodes)
            ]
            
            densities = sorted(subset['density'].unique())
            
            for k, density in enumerate(densities):
                # Create subplot at appropriate position
                ax = fig.add_subplot(gs[i, j*2 + k])
                axes.append(ax)
                
                density_subset = subset[subset['density'] == density]
                
                if not density_subset.empty:
                    # Pivot for heatmap
                    pivot = density_subset.pivot(
                        index='method_initial_tau',
                        columns='c',
                        values='symmetry'
                    )
                    
                    # Create heatmap
                    sns.heatmap(
                        pivot,
                        ax=ax,
                        cmap=cmap,
                        vmin=vmin,
                        vmax=vmax,
                        cbar=False,
                        linewidths=0.5,
                        linecolor='white',
                        square=False
                    )
                    
                    # Tufte-inspired minimal labels
                    # Y-axis labels only on leftmost column
                    if j == 0 and k == 0:
                        ax.set_ylabel(f'{graph_type}\n' + r'$\tau$', fontsize=11, weight='bold')
                    else:
                        ax.set_ylabel('')
                        ax.set_yticklabels([])
                    
                    # X-axis labels only on bottom row
                    if i == n_rows - 1:
                        ax.set_xlabel('$c$', fontsize=10)
                        ax.tick_params(axis='x', labelsize=8)
                    else:
                        ax.set_xlabel('')
                        ax.set_xticklabels([])
                    
                    # Title showing n and density
                    ax.set_title(f'$n={n_nodes}$, $\\rho={density}$', fontsize=10)
                    ax.tick_params(axis='y', labelsize=8)
                else:
                    ax.axis('off')
    
    # Add single colorbar for entire figure
    cbar = fig.colorbar(
        plt.cm.ScalarMappable(
            norm=mcolors.Normalize(vmin=vmin, vmax=vmax),
            cmap=cmap
        ),
        ax=axes,
        label='Mean Symmetry Coefficient',
        aspect=40,
        pad=0.01,
        shrink=0.8
    )
    cbar.ax.tick_params(labelsize=10)
    
    fig.suptitle(
        f"Temperature vs Penalty Parameter Space: {method} Method",
        fontsize=16, weight='bold'
    )
    
    save_figure(fig, f"temperature-tau-unified-{method.lower()}")
    plt.show()
    plt.close(fig)
```

### Penalty Parameter Effects {#sec-temperature-penalty-effects}

We analyze how the penalty parameter c affects symmetry quality across different 
temperature values.

```{python}
#| label: fig-temperature-penalty-unified
#| fig-cap: "Unified heatmap view of penalty vs temperature effects across graph types. Each figure shows one method-density combination. Rows represent graph types, columns show network sizes. Within each heatmap, penalty c is on y-axis and temperature τ on x-axis. Darker colors indicate better (lower) symmetry coefficients."
#| fig-width: 16
#| fig-height: 22

# Create unified heatmap plots for each method-density combination
for method in ["DR", "OR"]:
    for density in sorted(temp_best_basic['density'].unique()):
        unified_data = temp_best_basic[
            (temp_best_basic["graph_type"].isin(standard_graph_types)) &
            (temp_best_basic["method_display"] == method) &
            (temp_best_basic["density"] == density)
        ].copy()
        
        if unified_data.empty:
            continue
        
        # Ensure proper ordering
        unified_data['graph_type'] = pd.Categorical(
            unified_data['graph_type'],
            categories=standard_graph_types,
            ordered=True
        )
        
        # Compute mean symmetry for each combination
        heatmap_data = (
            unified_data
            .groupby(['graph_type', 'n_nodes', 'c', 'method_initial_tau'])['symmetry']
            .mean()
            .reset_index()
        )
        
        # Get unique values for layout
        graph_types_present = sorted(heatmap_data['graph_type'].unique())
        n_nodes_vals = sorted(heatmap_data['n_nodes'].unique())
        
        # Create subplot grid: rows = graph types, cols = n_nodes
        n_rows = len(graph_types_present)
        n_cols = len(n_nodes_vals)
        
        fig = plt.figure(figsize=(2.5*n_cols, 1.75*n_rows), constrained_layout=True)
        gs = fig.add_gridspec(n_rows, n_cols, hspace=0.10, wspace=0.10)
        
        # Use global min/max for consistent colorbar
        vmin = global_vmin
        vmax = global_vmax
        cmap = 'viridis_r'  # Tufte-inspired: dark = better (lower symmetry)
        
        axes = []
        for i, graph_type in enumerate(graph_types_present):
            for j, n_nodes in enumerate(n_nodes_vals):
                # Create subplot at appropriate position
                ax = fig.add_subplot(gs[i, j])
                axes.append(ax)
                
                subset = heatmap_data[
                    (heatmap_data['graph_type'] == graph_type) &
                    (heatmap_data['n_nodes'] == n_nodes)
                ]
                
                if not subset.empty:
                    # Pivot for heatmap: c on rows (y-axis), tau on columns (x-axis)
                    pivot = subset.pivot(
                        index='c',
                        columns='method_initial_tau',
                        values='symmetry'
                    )
                    
                    # Create heatmap
                    sns.heatmap(
                        pivot,
                        ax=ax,
                        cmap=cmap,
                        vmin=vmin,
                        vmax=vmax,
                        cbar=False,
                        linewidths=0.5,
                        linecolor='white',
                        square=False
                    )
                    
                    # Tufte-inspired minimal labels
                    # Y-axis labels only on leftmost column
                    if j == 0:
                        ax.set_ylabel(f'{graph_type}\n$c$', fontsize=11, weight='bold')
                    else:
                        ax.set_ylabel('')
                        ax.set_yticklabels([])
                    
                    # X-axis labels only on bottom row
                    if i == n_rows - 1:
                        ax.set_xlabel(r'$\tau$', fontsize=10)
                        ax.tick_params(axis='x', labelsize=8)
                    else:
                        ax.set_xlabel('')
                        ax.set_xticklabels([])
                    
                    # Title showing network size
                    ax.set_title(f'$n={n_nodes}$', fontsize=10)
                    ax.tick_params(axis='y', labelsize=8)
                else:
                    ax.axis('off')
        
        # Add single colorbar for entire figure
        cbar = fig.colorbar(
            plt.cm.ScalarMappable(
                norm=mcolors.Normalize(vmin=vmin, vmax=vmax),
                cmap=cmap
            ),
            ax=axes,
            label='Mean Symmetry Coefficient',
            aspect=40,
            pad=0.05,
            shrink=0.8
        )
        cbar.ax.tick_params(labelsize=10)
        
        fig.suptitle(
            f"Penalty vs Temperature Parameter Space: {method} Method, $\\rho={density}$",
            fontsize=16, weight='bold'
        )
        
        save_figure(fig, f"temperature-penalty-unified-{method.lower()}-density{density}")
        plt.show()
        plt.close(fig)
```

### Convergence Analysis {#sec-temperature-convergence}

We examine iteration distributions to understand convergence behavior across 
different parameter settings.

```{python}
#| label: fig-temperature-iteration-distribution
#| fig-cap: "Distribution of iterations at which the best solution was found for temperature-based methods. Separate distributions are shown for each temperature value τ."
#| fig-width: 14
#| fig-height: 10

# Create iteration distribution plots for each method
for method in ["DR", "OR"]:
    method_data = df_temp_basic[df_temp_basic['method_display'] == method]
    
    if not method_data.empty:
        g = sns.displot(
            data=method_data,
            x='metric_best_iteration',
            col='method_initial_tau',
            row='graph_type',
            hue='density',
            kind='hist',
            multiple='stack',
            height=2.5,
            aspect=1.2,
            bins=30,
            facet_kws={'sharex': False}
        )
        
        g._legend.set_title("Density")
        g.set_axis_labels("Iteration number", "Count")
        g.set_titles("{row_name}, $\\tau={col_name}$")
        g.fig.suptitle(
            f"Convergence Behavior: {method} Method",
            y=1.02, weight='bold'
        )
        
        save_figure(g.fig, f"temperature-iterations-{method.lower()}")
        plt.show()
```

### Optimal Parameter Selection {#sec-temperature-optimal}

Based on the identity avoidance and symmetry quality analyses, we identify 
optimal temperature and penalty parameters for each method.

```{python}
#| label: tbl-temperature-optimal-params
#| tbl-cap: "Optimal temperature (τ) and penalty (c) parameters for temperature-based methods selected using ranked voting across graph topologies. Condorcet winner (beats all other candidates in pairwise comparisons) is preferred when available."

def find_condorcet_winner(rankings):
    """
    Find Condorcet winner from ranked preferences.
    
    A Condorcet winner is a candidate that would win in a head-to-head 
    comparison against every other candidate.
    
    Parameters
    ----------
    rankings : dict
        Dictionary mapping graph_type to list of (config, score) tuples,
        ordered by preference (best first)
        
    Returns
    -------
    tuple or None
        (tau, c) configuration that is Condorcet winner, or None if no winner exists
    """
    # Collect all unique configurations
    all_configs = set()
    for ranking in rankings.values():
        for config, _ in ranking:
            all_configs.add(config)
    
    all_configs = list(all_configs)
    
    if len(all_configs) == 0:
        return None
    
    # Compute pairwise victories
    for candidate in all_configs:
        is_condorcet = True
        
        for opponent in all_configs:
            if candidate == opponent:
                continue
            
            # Count how many "voters" (graph types) prefer candidate over opponent
            candidate_wins = 0
            opponent_wins = 0
            
            for graph_type, ranking in rankings.items():
                candidate_rank = None
                opponent_rank = None
                
                for rank, (config, _) in enumerate(ranking):
                    if config == candidate:
                        candidate_rank = rank
                    if config == opponent:
                        opponent_rank = rank
                
                # If both found, compare ranks (lower is better)
                if candidate_rank is not None and opponent_rank is not None:
                    if candidate_rank < opponent_rank:
                        candidate_wins += 1
                    elif opponent_rank < candidate_rank:
                        opponent_wins += 1
            
            # Candidate must beat opponent in pairwise comparison
            if opponent_wins >= candidate_wins:
                is_condorcet = False
                break
        
        if is_condorcet:
            return candidate
    
    return None


def borda_count(rankings):
    """
    Select winner using Borda count voting.
    
    Each graph type assigns points: highest rank gets n-1 points,
    second gets n-2, etc., where n is number of candidates.
    
    Parameters
    ----------
    rankings : dict
        Dictionary mapping graph_type to list of (config, score) tuples
        
    Returns
    -------
    tuple
        (tau, c) configuration with highest Borda score
    """
    scores = {}
    
    for graph_type, ranking in rankings.items():
        n = len(ranking)
        for rank, (config, _) in enumerate(ranking):
            points = n - rank - 1  # Top gets n-1, second gets n-2, etc.
            scores[config] = scores.get(config, 0) + points
    
    if not scores:
        return None
    
    return max(scores.items(), key=lambda x: x[1])[0]


# Find optimal parameters for each method using ranked voting
optimal_temp_params = []
voting_details = []

for method in ["DR", "OR"]:
    method_success = success_by_sim_temp[
        success_by_sim_temp["method_display"] == method
    ]
    
    method_symmetry = temp_best_basic[
        temp_best_basic["method_display"] == method
    ]
    
    # Get rankings for each graph type separately
    rankings_by_graph = {}
    
    for graph_type in standard_graph_types:
        graph_success = method_success[method_success['graph_type'] == graph_type]
        graph_symmetry = method_symmetry[method_symmetry['graph_type'] == graph_type]
        
        if graph_success.empty or graph_symmetry.empty:
            continue
        
        # Count successful simulations for each (tau, c) combination
        config_metrics = (
            graph_success
            .groupby(['method_initial_tau', 'c'])
            .agg(
                successful_sims=('success', 'sum'),
                total_sims=('success', 'count'),
                success_rate=('success', 'mean')
            )
            .reset_index()
        )
        
        # Filter to only perfect success rate configurations
        config_metrics = config_metrics[config_metrics['success_rate'] == 1.0]
        
        if config_metrics.empty:
            continue
        
        # Add mean symmetry for each configuration
        config_scores = []
        for _, config in config_metrics.iterrows():
            tau = config['method_initial_tau']
            c = config['c']
            
            config_sym = graph_symmetry[
                (graph_symmetry['method_initial_tau'] == tau) &
                (graph_symmetry['c'] == c)
            ]['symmetry']
            
            if len(config_sym) > 0:
                # Score: negative symmetry (so lower symmetry = higher score)
                score = -config_sym.mean()
                config_scores.append({
                    'config': (tau, c),
                    'score': score,
                    'success_rate': config['success_rate'],
                    'mean_symmetry': config_sym.mean()
                })
        
        # Rank configurations by score (higher is better)
        config_scores = sorted(config_scores, key=lambda x: x['score'], reverse=True)
        rankings_by_graph[graph_type] = [(x['config'], x['score']) for x in config_scores]
        
        # Store top 3 for each graph type for display
        for rank, item in enumerate(config_scores[:1]):
            voting_details.append({
                'Method': method,
                'Graph Type': graph_type,
                'Rank': rank + 1,
                'tau': item['config'][0],
                'c': item['config'][1],
                'Success Rate': item['success_rate'],
                'Mean Symmetry': item['mean_symmetry'],
                'Score': item['score']
            })
    
    # Find Condorcet winner
    condorcet = find_condorcet_winner(rankings_by_graph)
    
    # If no Condorcet winner, use Borda count
    if condorcet is None:
        winner = borda_count(rankings_by_graph)
        winner_type = "Borda Count"
    else:
        winner = condorcet
        winner_type = "Condorcet"
    
    if winner is None:
        continue
    
    # Calculate overall statistics for winner
    winner_tau, winner_c = winner
    winner_data = method_symmetry[
        (method_symmetry['method_initial_tau'] == winner_tau) &
        (method_symmetry['c'] == winner_c)
    ]
    
    winner_success = method_success[
        (method_success['method_initial_tau'] == winner_tau) &
        (method_success['c'] == winner_c)
    ]
    
    optimal_temp_params.append({
        'Method': method,
        'Selection': winner_type,
        'Optimal tau': winner_tau,
        'Optimal c': winner_c,
        'Success Rate': winner_success['success'].mean() if len(winner_success) > 0 else np.nan,
        'Mean Symmetry': winner_data['symmetry'].mean() if len(winner_data) > 0 else np.nan,
        'Median Symmetry': winner_data['symmetry'].median() if len(winner_data) > 0 else np.nan
    })

optimal_temp_df = pd.DataFrame(optimal_temp_params)
print(optimal_temp_df.to_markdown(index=False, floatfmt=".4f"))
save_table(
    optimal_temp_df,
    "optimal_parameters_temperature",
    "Optimal parameters for temperature-based methods (ranked voting)",
    label="tab:optimal-parameters-temperature"
)

# Display voting details
voting_details_df = pd.DataFrame(voting_details)
print("\n" + "="*80)
print("Top 3 Parameter Configurations per Graph Type:")
print("="*80)
for method in ["DR", "OR"]:
    print(f"\n{method} Method:")
    method_votes = voting_details_df[voting_details_df['Method'] == method]
    print(method_votes[['Graph Type', 'Rank', 'tau', 'c', 'Success Rate', 'Mean Symmetry']].to_markdown(index=False, floatfmt=".4f"))
```

### Method Comparison at Optimal Parameters {#sec-temperature-method-comparison}

We compare DR and OR methods using their respective optimal parameters.

```{python}
#| label: temperature-optimal-data
#| code-summary: "Filter data for optimal parameter configurations"

# Create optimal parameter dictionary
optimal_temp_dict = {
    row['Method']: {'tau': row['Optimal tau'], 'c': row['Optimal c']}
    for _, row in optimal_temp_df.iterrows()
}

# Filter data for optimal configurations
optimal_temp_data = temp_best_basic[
    temp_best_basic.apply(
        lambda x: (
            x['method_initial_tau'] == optimal_temp_dict.get(x['method_display'], {}).get('tau') and
            x['c'] == optimal_temp_dict.get(x['method_display'], {}).get('c')
        ),
        axis=1
    )
]
```

```{python}
#| label: fig-temperature-comparison-separate
#| fig-cap: "Comparison of DR and OR methods at their optimal parameter settings across different graph types. Box plots show the distribution of symmetry coefficients, with lower values indicating better performance."
#| fig-width: 12
#| fig-height: 8

# Create comparison plots for standard graph types
for graph_type in standard_graph_types:
    data_subset = optimal_temp_data[
        optimal_temp_data["graph_type"] == graph_type
    ]
    
    if data_subset.empty:
        continue
    
    g = sns.catplot(
        data=data_subset,
        x='n_nodes',
        y='symmetry',
        hue='method_display',
        palette={METHOD_NAMES[m]: METHOD_COLORS[m] for m in ["DimensionalityReduction", "OrthogonalRelaxation"]},
        col='density',
        kind='box',
        showfliers=False,
        height=4,
        aspect=1.5,
        dodge=True
    )
    
    g._legend.set_title("Method")
    g.set_axis_labels("Number of nodes", "Symmetry coefficient")
    g.set_titles("$\\rho={col_name}$")
    g.fig.suptitle(
        f"Method Comparison: {graph_type} Networks (Optimal Parameters)",
        y=1.04, weight='bold'
    )
    
    save_figure(g.fig, f"temperature-comparison-{graph_type.lower()}")
    plt.show()

# LRM comparison
lrm_optimal = optimal_temp_data[optimal_temp_data["graph_type"] == "LRM"]
if not lrm_optimal.empty:
    g = sns.catplot(
        data=lrm_optimal,
        x='rew',
        y='symmetry',
        hue='method_display',
        palette={METHOD_NAMES[m]: METHOD_COLORS[m] for m in ["DimensionalityReduction", "OrthogonalRelaxation"]},
        col='n_nodes',
        row='density',
        kind='box',
        showfliers=False,
        height=3,
        aspect=1.5,
        dodge=True
    )
    
    g._legend.set_title("Method")
    g.set_axis_labels("Number of rewirings", "Symmetry coefficient")
    g.set_titles("$n={col_name}$, $\\rho={row_name}$")
    g.fig.suptitle(
        "Method Comparison: LRM Networks (Optimal Parameters)",
        y=1.04, weight='bold'
    )
    
    save_figure(g.fig, f"temperature-comparison-lrm")
    plt.show()
```

```{python}
#| label: fig-temperature-comparison-unified
#| fig-cap: "Unified comparison of DR and OR methods at optimal parameters across graph types. Each row represents a graph type, columns show edge densities. Methods are distinguished by color."
#| fig-width: 14
#| fig-height: 18

# Create unified comparison plot
unified_optimal = optimal_temp_data[
    optimal_temp_data["graph_type"].isin(standard_graph_types)
].copy()

if not unified_optimal.empty:
    # Ensure proper ordering
    unified_optimal['graph_type'] = pd.Categorical(
        unified_optimal['graph_type'],
        categories=standard_graph_types,
        ordered=True
    )
    
    g = sns.catplot(
        data=unified_optimal,
        x='n_nodes',
        y='symmetry',
        hue='method_display',
        palette={METHOD_NAMES[m]: METHOD_COLORS[m] for m in ["DimensionalityReduction", "OrthogonalRelaxation"]},
        row='graph_type',
        col='density',
        kind='box',
        showfliers=False,
        height=2.5,
        aspect=1.2,
        dodge=True
    )
    
    g._legend.set_title("Method")
    
    # Move legend to the right
    sns.move_legend(g, "center left", bbox_to_anchor=(1.02, 0.5), frameon=True)
    
    g.set_titles("{row_name} ($\\rho={col_name}$)")
    g.fig.suptitle(
        "Temperature Method Comparison at Optimal Parameters",
        y=1.00, weight='bold', fontsize=24
    )
    
    # Adjust layout
    plt.tight_layout()
    
    # Add common axis labels
    g.set_axis_labels("", "")  # Remove individual axis labels
    g.fig.text(0.5, 0.0, "Number of nodes", ha='center', fontsize=18)
    g.fig.text(0.0, 0.5, "Symmetry coefficient", va='center', rotation='vertical', fontsize=18)
    
    save_figure(g.fig, f"temperature-comparison-unified")
    plt.show()
```

## Loss Function Comparison {#sec-loss-functions}

We compare convex vs indefinite loss formulations.

### Statistical Analysis of Convex vs Indefinite Loss Functions {#sec-loss-statistical}

To rigorously assess the performance difference between convex and indefinite
loss formulations, we conduct a comprehensive statistical analysis using paired
t-tests and Cohen's D effect sizes. This analysis is stratified across multiple
dimensions to account for the various experimental factors.

```{python}
from scipy import stats
import numpy as np
import pingouin as pg
```

```{python}
#| label: loss-statistical-setup
#| code-summary: "Prepare data and functions for loss function statistical comparison"

def cohens_d(x, y):
    """
    Calculate Cohen's D effect size between two samples
    """

    return pg.compute_effsize(x, y, eftype='cohen')

def interpret_cohens_d(d):
    """Interpret Cohen's D effect size magnitude"""

    d_abs = abs(d)
    if d_abs < 0.2:
        return "negligible"
    elif d_abs < 0.5:
        return "small"
    elif d_abs < 0.8:
        return "medium"
    else:
        return "large"

# Prepare data for loss function comparison
loss_comparison_data = df_temp[~df_temp['is_anneal']].copy()
# Choose only success
loss_comparison_data = loss_comparison_data[
    loss_comparison_data["n_nodes"] != loss_comparison_data["fixed_points"]
]

# Get best symmetry per simulation
loss_best = (
    loss_comparison_data
    .groupby(
        [
            "method_display", "graph_type", "n_nodes", "density", 
            "method_loss", "method_initial_tau", "c", "rew", "sim_id"
        ],
        dropna=False
    )['symmetry']
    .min()
    .reset_index()
)

loss_best.sample(10)
```

```{python}
#| label: tbl-loss-statistical-overall
#| tbl-cap: "Statistical comparison of convex vs indefinite loss functions across all conditions. Negative mean differences and Cohen's D values indicate convex loss outperforms indefinite. Effect sizes are interpreted as negligible (<0.2), small (0.2-0.5), medium (0.5-0.8), or large (>0.8)."

# Overall comparison for each method
overall_results = []

for method in ["OR", "DR"]:
    method_data = loss_best[loss_best["method_display"] == method]
    
    groups = method_data.groupby(
        [
            "graph_type", "n_nodes", "density", "rew", "c", "method_initial_tau"
        ],
        dropna=False,
    )
    for group_id, subset in groups:
        graph_type, n_nodes, density, rew, c, tau = group_id

        convex_data = subset[subset["method_loss"] == "convex"]["symmetry"]
        indefinite_data = subset[subset["method_loss"] == "indefinite"]["symmetry"]
    
        if len(convex_data) > 0 and len(indefinite_data) > 0:
            # Paired t-test (matching on instance)
            t_stat, p_value = stats.ttest_ind(convex_data, indefinite_data, equal_var=True)
            cohen_d = cohens_d(convex_data, indefinite_data)

            overall_results.append({
                'Method': method,
                'Graph Type': graph_type,
                'n': n_nodes,
                'rho': density,
                'k': rew,
                'tau': tau,
                'c': c,
                'Convex Mean': convex_data.mean(),
                'Indef Mean': indefinite_data.mean(),
                'Diff': convex_data.mean() - indefinite_data.mean(),
                'p-value': p_value,
                'Cohen\'s D': cohen_d,
                'Effect': interpret_cohens_d(cohen_d)
            })
            

overall_df = pd.DataFrame(overall_results)
print("\nOverall Loss Function Comparison")
print("=" * 70)
print(overall_df.to_string(index=False))
save_table(
    overall_df,
    "loss_function_overall_statistical",
    "Overall statistical comparison of convex vs indefinite loss functions",
    show_index=False
)
```

Loss tables: show means, mean difference, p‑value, effect size. Check if direction stays the same as size or density increases.

#### Visual Summary: Statistical Comparison Heatmaps

Purpose: generate heatmaps of p‑values and Cohen's D across parameter grids for
DR and OR.

```{python}
#| label: fig-loss-comparison-heatmaps-lrm
#| fig-cap: "Heatmap visualization of statistical comparisons between convex and indefinite loss functions for LRM networks. Each panel shows p-values (left) and Cohen's D effect sizes (right) for different combinations of parameters. Rows represent different densities (ρ), columns represent different temperatures (τ). Within each cell, the heatmap shows penalty parameter (c) on y-axis and rewiring level (k) on x-axis. Darker colors in p-value heatmaps indicate stronger statistical significance; in Cohen's D heatmaps, more negative values (blue) indicate convex superiority."
#| fig-width: 18
#| fig-height: 12
#| code-summary: "Create comprehensive heatmap visualization for LRM networks"

def plot_loss_pvalue_heatmap(ax, data_pivot, vmin, vmax, xlabel, ylabel):
    """
    Plot a p-value heatmap for loss function comparison.
    
    Parameters
    ----------
    ax : matplotlib.axes.Axes
        Axes to plot on
    data_pivot : pd.DataFrame
        Pivot table of p-values
    vmin, vmax : float
        Color scale limits
    xlabel, ylabel : str
        Axis labels
        
    Returns
    -------
    matplotlib.collections.QuadMesh
        The heatmap mappable for colorbar
    """
    im = sns.heatmap(
        data_pivot,
        ax=ax,
        cmap='RdYlGn_r',
        vmin=vmin,
        vmax=vmax,
        cbar=False,
        annot=True,
        fmt='.3f',
        square=True,
        linewidths=0.5
    )
    ax.set_title('p-values', fontsize=12)
    ax.set_ylabel(ylabel, fontsize=10)
    ax.set_xlabel(xlabel, fontsize=10)
    
    return ax.collections[0]


def plot_loss_cohens_d_heatmap(ax, data_pivot, abs_max, xlabel):
    """
    Plot a Cohen's D heatmap for loss function comparison.
    
    Parameters
    ----------
    ax : matplotlib.axes.Axes
        Axes to plot on
    data_pivot : pd.DataFrame
        Pivot table of Cohen's D values
    abs_max : float
        Absolute maximum for symmetric color scale
    xlabel : str
        X-axis label
        
    Returns
    -------
    matplotlib.collections.QuadMesh
        The heatmap mappable for colorbar
    """
    im = sns.heatmap(
        data_pivot,
        ax=ax,
        cmap='RdBu_r',
        center=0,
        vmin=-abs_max,
        vmax=abs_max,
        cbar=False,
        annot=True,
        fmt='.2f',
        square=True,
        linewidths=0.5
    )
    ax.set_title('Cohen\'s D', fontsize=12)
    ax.set_ylabel('')
    ax.set_xlabel(xlabel, fontsize=10)
    
    return ax.collections[0]


def create_lrm_loss_comparison_heatmap(data, method_name, save_name):
    """
    Create a comprehensive heatmap visualization comparing convex vs indefinite loss
    for LRM networks (includes rewiring parameter k).
    
    Parameters
    ----------
    data : pd.DataFrame
        Statistical results with columns: Method, Graph Type, n, ρ, k, τ, c, p-value, Cohen's D
    method_name : str
        Name of the method (DR or OR)
    save_name : str
        Base name for saving the figure
    """
    # Filter for this method and LRM networks
    method_data = data[
        (data['Method'] == method_name) & 
        (data['Graph Type'] == 'LRM')
    ].copy()
    
    if method_data.empty:
        print(f"No LRM data for {method_name}")
        return
    
    # Get unique values for each dimension
    densities = sorted(method_data['rho'].unique())
    taus = sorted(method_data['tau'].unique())
    cs = sorted(method_data['c'].unique())
    ks = sorted(method_data['k'].dropna().unique())
    n_nodes_list = [100]  # Can be changed to include multiple sizes
    
    # Create figure for each n_nodes value
    for n_nodes in n_nodes_list:
        n_data = method_data[method_data['n'] == n_nodes]
        
        # Create figure with grid: rows=densities, cols=taus
        n_rows = len(densities)
        n_cols = len(taus)
        
        # Create main figure with constrained layout
        fig = plt.figure(figsize=(7 * n_cols + 1.5, 5.5 * n_rows), layout='constrained')
        
        # Create outer gridspec: main plot area + colorbar column
        gs_outer = fig.add_gridspec(
            1, 2,
            width_ratios=[n_cols, 0.25],
            wspace=0.05
        )
        
        # Create inner gridspec for the plot area
        gs_plots = gs_outer[0].subgridspec(
            n_rows, n_cols,
            hspace=0.05,
            wspace=0.05
        )
        
        # Create subfigures for the heatmaps
        subfigs = []
        for i in range(n_rows):
            row_subfigs = []
            for j in range(n_cols):
                subfig = fig.add_subfigure(gs_plots[i, j])
                row_subfigs.append(subfig)
            subfigs.append(row_subfigs)
        
        # Convert to numpy array for easier indexing
        subfigs = np.array(subfigs)
        
        # Track min/max for consistent colorbars
        all_pvalues = []
        all_cohens_d = []
        
        # First pass: collect all values for colorbar scaling
        for rho_idx, rho in enumerate(densities):
            for tau_idx, tau in enumerate(taus):
                subset = n_data[
                    (n_data['rho'] == rho) & 
                    (n_data['tau'] == tau)
                ]
                
                if not subset.empty:
                    # Create pivot tables for heatmaps
                    pvalue_pivot = subset.pivot_table(
                        values='p-value',
                        index='c',
                        columns='k',
                        aggfunc='mean'
                    )
                    cohens_pivot = subset.pivot_table(
                        values='Cohen\'s D',
                        index='c',
                        columns='k',
                        aggfunc='mean'
                    )
                    
                    # Convert to numeric and collect values
                    pvalue_vals = pd.to_numeric(pvalue_pivot.values.flatten(), errors='coerce')
                    cohens_vals = pd.to_numeric(cohens_pivot.values.flatten(), errors='coerce')
                    
                    all_pvalues.extend(pvalue_vals[~np.isnan(pvalue_vals)])
                    all_cohens_d.extend(cohens_vals[~np.isnan(cohens_vals)])
        
        # Set color ranges
        pvalue_vmin, pvalue_vmax = 0, max(all_pvalues) if all_pvalues else 0.05
        pvalue_vmax = min(pvalue_vmax, 0.1)  # Cap at 0.1 for better visualization
        
        cohens_vmin = min(all_cohens_d) if all_cohens_d else -2
        cohens_vmax = max(all_cohens_d) if all_cohens_d else 2
        cohens_abs_max = max(abs(cohens_vmin), abs(cohens_vmax))
        
        # Second pass: create heatmaps
        pval_mappable = None
        cohen_mappable = None
        
        for rho_idx, rho in enumerate(densities):
            for tau_idx, tau in enumerate(taus):
                subset = n_data[
                    (n_data['rho'] == rho) & 
                    (n_data['tau'] == tau)
                ]
                
                if subset.empty:
                    continue
                
                # Get the subfigure for this cell
                subfig = subfigs[rho_idx, tau_idx]
                
                # Set subfigure title
                subfig.suptitle(
                    f'$\\rho={rho}$, $\\tau={tau}$', 
                    fontsize=14, weight='bold', y=0.95
                )
                
                # Create 2 subplots in this subfigure (side by side)
                axs = subfig.subplots(1, 2, sharey=True)
                
                # Create pivot tables for heatmaps
                pvalue_pivot = subset.pivot_table(
                    values='p-value',
                    index='c',
                    columns='k',
                    aggfunc='mean'
                )
                cohens_pivot = subset.pivot_table(
                    values='Cohen\'s D',
                    index='c',
                    columns='k',
                    aggfunc='mean'
                )
                
                # Reindex to ensure all c and k values are present
                pvalue_pivot = pvalue_pivot.reindex(index=cs, columns=ks)
                cohens_pivot = cohens_pivot.reindex(index=cs, columns=ks)
                
                # Convert to float
                pvalue_pivot = pvalue_pivot.astype(float)
                cohens_pivot = cohens_pivot.astype(float)
                
                # P-value heatmap (left)
                mappable = plot_loss_pvalue_heatmap(
                    axs[0], pvalue_pivot, pvalue_vmin, pvalue_vmax,
                    'Rewirings $k$', 'Penalty $c$'
                )
                if pval_mappable is None:
                    pval_mappable = mappable
                
                # Cohen's D heatmap (right)
                mappable = plot_loss_cohens_d_heatmap(
                    axs[1], cohens_pivot, cohens_abs_max,
                    'Rewirings $k$'
                )
                if cohen_mappable is None:
                    cohen_mappable = mappable
        
        # Create subfigure for colorbars
        cbar_subfig = fig.add_subfigure(gs_outer[1])
        cbar_axes = cbar_subfig.subplots(1, 2, gridspec_kw={'wspace': 0.5})
        
        # Add colorbars
        if pval_mappable is not None:
            cbar_pval = plt.colorbar(pval_mappable, cax=cbar_axes[0], orientation='vertical')
            cbar_pval.set_label('p-value', fontsize=12, weight='bold')
        
        if cohen_mappable is not None:
            cbar_cohen = plt.colorbar(cohen_mappable, cax=cbar_axes[1], orientation='vertical')
            cbar_cohen.set_label('Cohen\'s D', fontsize=12, weight='bold')
        
        # Add overall title
        fig.suptitle(
            f'{method_name} Method: Statistical Comparison of Convex vs Indefinite Loss (n={n_nodes})',
            fontsize=18,
            weight='bold',
            y=0.995
        )
        
        # Save figure
        save_figure(fig, f'{save_name}-lrm-{method_name.lower()}-n{n_nodes}')
        plt.show()

# Create visualizations for both methods (LRM networks)
for method in ['DR', 'OR']:
    create_lrm_loss_comparison_heatmap(overall_df, method, 'loss-comparison-heatmap')
```

```{python}
#| label: fig-loss-comparison-heatmaps-standard
#| fig-cap: "Heatmap visualization of statistical comparisons between convex and indefinite loss functions for standard graph types (ER, BA, GEO2D, GEO3D, DD, HK). Each panel shows p-values (left) and Cohen's D effect sizes (right) for different combinations of parameters. Rows represent different densities (ρ), columns represent different temperatures (τ). Within each cell, the heatmap shows penalty parameter (c) on y-axis and network size (n) on x-axis. Darker colors in p-value heatmaps indicate stronger statistical significance; in Cohen's D heatmaps, more negative values (blue) indicate convex superiority."
#| fig-width: 18
#| fig-height: 12
#| code-summary: "Create comprehensive heatmap visualization for standard graph types"

def create_er_ba_loss_comparison_heatmap(data, method_name, graph_type, save_name):
    """
    Create a comprehensive heatmap visualization comparing convex vs indefinite loss
    for standard graph types (no rewiring parameter, includes network size n).
    
    Parameters
    ----------
    data : pd.DataFrame
        Statistical results with columns: Method, Graph Type, n, ρ, τ, c, p-value, Cohen's D
    method_name : str
        Name of the method (DR or OR)
    graph_type : str
        Type of graph ('ER' or 'BA')
    save_name : str
        Base name for saving the figure
    """
    # Filter for this method and graph type
    method_data = data[
        (data['Method'] == method_name) & 
        (data['Graph Type'] == graph_type)
    ].copy()
    
    if method_data.empty:
        print(f"No {graph_type} data for {method_name}")
        return
    
    # Get unique values for each dimension
    densities = sorted(method_data['rho'].unique())
    taus = sorted(method_data['tau'].unique())
    cs = sorted(method_data['c'].unique())
    ns = sorted(method_data['n'].unique())
    
    # Create figure with grid: rows=densities, cols=taus
    n_rows = len(densities)
    n_cols = len(taus)
    
    # Create main figure with constrained layout
    fig = plt.figure(figsize=(7 * n_cols + 1.5, 5.5 * n_rows), layout='constrained')
    
    # Create outer gridspec: main plot area + colorbar column
    gs_outer = fig.add_gridspec(
        1, 2,
        width_ratios=[n_cols, 0.25],
        wspace=0.05
    )
    
    # Create inner gridspec for the plot area
    gs_plots = gs_outer[0].subgridspec(
        n_rows, n_cols,
        hspace=0.05,
        wspace=0.05
    )
    
    # Create subfigures for the heatmaps
    subfigs = []
    for i in range(n_rows):
        row_subfigs = []
        for j in range(n_cols):
            subfig = fig.add_subfigure(gs_plots[i, j])
            row_subfigs.append(subfig)
        subfigs.append(row_subfigs)
    
    # Convert to numpy array for easier indexing
    subfigs = np.array(subfigs)
    
    # Track min/max for consistent colorbars
    all_pvalues = []
    all_cohens_d = []
    
    # First pass: collect all values for colorbar scaling
    for rho_idx, rho in enumerate(densities):
        for tau_idx, tau in enumerate(taus):
            subset = method_data[
                (method_data['rho'] == rho) & 
                (method_data['tau'] == tau)
            ]
            
            if not subset.empty:
                # Create pivot tables for heatmaps (c × n)
                pvalue_pivot = subset.pivot_table(
                    values='p-value',
                    index='c',
                    columns='n',
                    aggfunc='mean'
                )
                cohens_pivot = subset.pivot_table(
                    values='Cohen\'s D',
                    index='c',
                    columns='n',
                    aggfunc='mean'
                )
                
                # Convert to numeric and collect values
                pvalue_vals = pd.to_numeric(pvalue_pivot.values.flatten(), errors='coerce')
                cohens_vals = pd.to_numeric(cohens_pivot.values.flatten(), errors='coerce')
                
                all_pvalues.extend(pvalue_vals[~np.isnan(pvalue_vals)])
                all_cohens_d.extend(cohens_vals[~np.isnan(cohens_vals)])
    
    # Set color ranges
    pvalue_vmin, pvalue_vmax = 0, max(all_pvalues) if all_pvalues else 0.05
    pvalue_vmax = min(pvalue_vmax, 0.1)  # Cap at 0.1 for better visualization
    
    cohens_vmin = min(all_cohens_d) if all_cohens_d else -2
    cohens_vmax = max(all_cohens_d) if all_cohens_d else 2
    cohens_abs_max = max(abs(cohens_vmin), abs(cohens_vmax))
    
    # Second pass: create heatmaps
    pval_mappable = None
    cohen_mappable = None
    
    for rho_idx, rho in enumerate(densities):
        for tau_idx, tau in enumerate(taus):
            subset = method_data[
                (method_data['rho'] == rho) & 
                (method_data['tau'] == tau)
            ]
            
            if subset.empty:
                continue
            
            # Get the subfigure for this cell
            subfig = subfigs[rho_idx, tau_idx]
            
            # Set subfigure title
            subfig.suptitle(
                f'$\\rho={rho}$, $\\tau={tau}$', 
                fontsize=14, weight='bold', y=0.95
            )
            
            # Create 2 subplots in this subfigure (side by side)
            axs = subfig.subplots(1, 2, sharey=True)
            
            # Create pivot tables for heatmaps (c × n)
            pvalue_pivot = subset.pivot_table(
                values='p-value',
                index='c',
                columns='n',
                aggfunc='mean'
            )
            cohens_pivot = subset.pivot_table(
                values='Cohen\'s D',
                index='c',
                columns='n',
                aggfunc='mean'
            )
            
            # Reindex to ensure all c and n values are present
            pvalue_pivot = pvalue_pivot.reindex(index=cs, columns=ns)
            cohens_pivot = cohens_pivot.reindex(index=cs, columns=ns)
            
            # Convert to float
            pvalue_pivot = pvalue_pivot.astype(float)
            cohens_pivot = cohens_pivot.astype(float)
            
            # P-value heatmap (left)
            mappable = plot_loss_pvalue_heatmap(
                axs[0], pvalue_pivot, pvalue_vmin, pvalue_vmax,
                'Network size $n$', 'Penalty $c$'
            )
            if pval_mappable is None:
                pval_mappable = mappable
            
            # Cohen's D heatmap (right)
            mappable = plot_loss_cohens_d_heatmap(
                axs[1], cohens_pivot, cohens_abs_max,
                'Network size $n$'
            )
            if cohen_mappable is None:
                cohen_mappable = mappable
    
    # Create subfigure for colorbars
    cbar_subfig = fig.add_subfigure(gs_outer[1])
    cbar_axes = cbar_subfig.subplots(1, 2, gridspec_kw={'wspace': 0.5})
    
    # Add colorbars
    if pval_mappable is not None:
        cbar_pval = plt.colorbar(pval_mappable, cax=cbar_axes[0], orientation='vertical')
        cbar_pval.set_label('p-value', fontsize=12, weight='bold')
    
    if cohen_mappable is not None:
        cbar_cohen = plt.colorbar(cohen_mappable, cax=cbar_axes[1], orientation='vertical')
        cbar_cohen.set_label('Cohen\'s D', fontsize=12, weight='bold')
    
    # Add overall title
    fig.suptitle(
        f'{method_name} Method: Statistical Comparison of Convex vs Indefinite Loss ({graph_type} Networks)',
        fontsize=18,
        weight='bold',
        y=0.995
    )
    
    # Save figure
    save_figure(fig, f'{save_name}-{graph_type.lower()}-{method_name.lower()}')
    plt.show()

# Create visualizations for standard graph types (excluding LRM which has different structure)
standard_graph_types = STANDARD_GRAPH_TYPES
for method in ['DR', 'OR']:
    for graph_type in standard_graph_types:
        # Skip if no data available for this graph type
        graph_subset = overall_df[overall_df['Graph Type'] == graph_type]
        if graph_subset.empty:
            continue
        create_er_ba_loss_comparison_heatmap(overall_df, method, graph_type, 'loss-comparison-heatmap')
```

Heatmaps: rows = density, columns = temperature. Each cell contains a small grid: penalty $c$ vs size (or rewiring). Left panel shows p-values (dark = low p). Right panel shows Cohen's d (color = direction, intensity = magnitude). Use blocks of low p with consistent effect size to spot broad parameter regions. Single dark cells mark narrow cases.

### Detailed Visual Analysis

This subsection produces distribution plots for iteration of best solution.

```{python}
#| label: fig-iteration-distribution-temp
#| fig-cap: "Iteration at which the best solution was first found for temperature-based methods, split by loss type and temperature. Maximum iteration limits: 3000 (DR), 2000 (OR)."
#| fig-width: 12
#| fig-height: 10

# Filter for non-annealing runs
non_anneal_data = df_temp[~df_temp['is_anneal']].copy()

# Create iteration distribution plots for each method
for method in ["DimensionalityReduction", "OrthogonalRelaxation"]:
    method_data = non_anneal_data[non_anneal_data['method'] == method]
    
    if not method_data.empty:
        g = sns.displot(
            data=method_data,
            x='metric_best_iteration',
            col="method_initial_tau",
            row="method_loss",
            hue="graph_type",
            kind="hist",
            palette=sns.color_palette("deep")[7:],
            multiple="stack",
            height=3,
            #aspect=1.2,
            bins=30
        )
        g._legend.set_title("Graph Type")
        
        g.set_axis_labels("Iteration number", "Count")
        g.set_titles("Loss: {row_name}, $\\tau={col_name}$")
        g.fig.suptitle(
            f"Iteration when best solution was found: {METHOD_NAMES[method]} Method", y=1.04, weight='bold'
            )
        
        save_figure(g.fig, f"iteration-dist-temp-{METHOD_NAMES[method].lower()}")
        plt.show()
```

Iteration distribution (temperature methods): Inspect whether best iterations cluster early (efficient) or near max iterations (potential overrun). Compare stratification by temperature to see how relaxation strength affects search speed.

## Temperature Parameter Analysis {#sec-temperature-params}

Temperature parameters control the degree of continuous relaxation, with lower
values enforcing stricter adherence to permutation constraints. We examine how
this critical parameter affects solution quality.

```{python}
#| label: fig-symmetry-vs-temperature
#| fig-cap: "Effect of temperature parameter on symmetry quality for different loss functions. Lower symmetry coefficients indicate better solutions. Box plots show distributions across all non-identity solutions."
#| fig-width: 14
#| fig-height: 12

# Filter non-identity solutions
temp_symmetry = non_anneal_data[
    non_anneal_data["n_nodes"] != non_anneal_data["fixed_points"]
].copy()

# Get best symmetry per simulation
temp_best = (
    temp_symmetry
    .groupby(["method_display", "graph_type", "n_nodes", "density", 
              "method_loss", "method_initial_tau", "rew", "sim_id"], dropna=False)['symmetry']
    .min()
    .reset_index()
)

# Filter non-identity solutions
temp_symmetry = non_anneal_data[
    non_anneal_data["n_nodes"] != non_anneal_data["fixed_points"]
].copy()

# Get best symmetry per simulation
temp_best = (
    temp_symmetry
    .groupby(["method_display", "graph_type", "n_nodes", "density", 
              "method_loss", "c", "rew", "method_initial_tau", "sim_id"], dropna=False)['symmetry']
    .min()
    .reset_index()
)

# Create composite variable combining method_loss and rew
def create_method_rew_key(row):
    if row['graph_type'] == 'LRM' and not pd.isna(row['rew']):
        return f"{row['method_loss']}, $k={int(row['rew'])}$"
    else:
        return row['method_loss']

temp_best['method_rew_display'] = temp_best.apply(create_method_rew_key, axis=1)

# Custom palette function
def create_loss_rew_palette(data, base_colors=None):
    """Create palette with 2 base colors and shading by rew values"""
    
    if base_colors is None:
        base_colors = sns.color_palette("deep")[5:7]
    
    # Get unique method_loss values and rew values
    method_losses = data['method_loss'].unique()
    
    # Get rew values for LRM graphs
    lrm_data = data[data['graph_type'] == 'LRM']
    rew_values = sorted(lrm_data['rew'].dropna().unique()) if not lrm_data.empty else []
    
    palette = {}
    
    # Handle cases without rew (non-LRM graphs)
    for i, method_loss in enumerate(method_losses):
        palette[method_loss] = base_colors[i % len(base_colors)]
    
    # Handle cases with rew (LRM graphs)
    if rew_values:
        for i, method_loss in enumerate(method_losses):
            base_color = base_colors[i % len(base_colors)]
            
            # Create gradient from light to dark
            n_shades = len(rew_values)
            if n_shades > 1:
                # Create shades from lighter to darker
                color_shades = sns.light_palette(base_color, n_colors=n_shades + 2)[1:-1]
                #color_shades = color_shades[::-1]  # Reverse to go light to dark
            else:
                color_shades = [base_color]
            
            for j, rew in enumerate(rew_values):
                palette[f"{method_loss}, $k={int(rew)}$"] = color_shades[j]
    
    return palette

# Apply in your plotting loop
for method in ["DimensionalityReduction", "OrthogonalRelaxation"]:
    method_data = temp_best[temp_best["method_display"] == METHOD_NAMES[method]]
    
    for graph_type in graph_types:
        graph_data = method_data[method_data["graph_type"] == graph_type]
        
        if not graph_data.empty:
            # Create custom palette for this specific dataset
            custom_palette = create_loss_rew_palette(
                graph_data, 
            )
            
            g = sns.catplot(
                data=graph_data,
                y="symmetry",
                x="method_initial_tau",
                hue="method_rew_display",  # Composite variable
                palette=custom_palette,     # Custom palette
                row="density",
                col="n_nodes",
                kind="box",
                showfliers=False,
                height=3,
                aspect=1.0,
                dodge=True
            )
            g._legend.set_title("Loss" + (", Rewirings" if graph_type == "LRM" else ""))

            g.set_axis_labels("Temperature $\\tau$", "Symmetry coefficient")
            g.set_titles("$n={col_name}$, $\\rho={row_name}$")
            g.fig.suptitle(f"Symmetry Quality vs. Temperature for {METHOD_NAMES[method]} Method on {graph_type} Networks", y=1.04, weight='bold')
            
            save_figure(g.fig, f"symmetry-vs-temp-{METHOD_NAMES[method].lower()}-{graph_type.lower()}")
            plt.show()
```


Temperature sweep symmetry plots: For DR/OR panels, note trend of symmetry coefficient with temperature. Identify temperature intervals where performance stabilizes; unstable or jagged profiles may imply need for annealing or revised schedule.

## Penalty parameter analysis


```{python}
#| label: fig-symmetry-vs-penalty
#| fig-cap: "Effect of penalty parameter on symmetry quality for different loss functions. Lower symmetry coefficients indicate better solutions. Box plots show distributions across all non-identity solutions."
#| fig-width: 14
#| fig-height: 12

# Filter non-identity solutions
temp_symmetry = non_anneal_data[
    non_anneal_data["n_nodes"] != non_anneal_data["fixed_points"]
].copy()

# Get best symmetry per simulation
temp_best = (
    temp_symmetry
    .groupby(["method_display", "graph_type", "n_nodes", "density", 
              "method_loss", "method_initial_tau", "rew", "sim_id"], dropna=False)['symmetry']
    .min()
    .reset_index()
)

# Filter non-identity solutions
temp_symmetry = non_anneal_data[
    non_anneal_data["n_nodes"] != non_anneal_data["fixed_points"]
].copy()

# Get best symmetry per simulation
temp_best = (
    temp_symmetry
    .groupby(["method_display", "graph_type", "n_nodes", "density", 
              "method_loss", "c", "rew", "method_initial_tau", "sim_id"], dropna=False)['symmetry']
    .min()
    .reset_index()
)

# Create composite variable combining method_loss and rew
def create_method_rew_key(row):
    if row['graph_type'] == 'LRM' and not pd.isna(row['rew']):
        return f"{row['method_loss']}, $k={int(row['rew'])}$"
    else:
        return row['method_loss']

temp_best['method_rew_display'] = temp_best.apply(create_method_rew_key, axis=1)

# Custom palette function
def create_loss_rew_palette(data, base_colors=None):
    """Create palette with 2 base colors and shading by rew values"""
    
    if base_colors is None:
        base_colors = sns.color_palette("deep")[5:7]
    
    # Get unique method_loss values and rew values
    method_losses = data['method_loss'].unique()
    
    # Get rew values for LRM graphs
    lrm_data = data[data['graph_type'] == 'LRM']
    rew_values = sorted(lrm_data['rew'].dropna().unique()) if not lrm_data.empty else []
    
    palette = {}
    
    # Handle cases without rew (non-LRM graphs)
    for i, method_loss in enumerate(method_losses):
        palette[method_loss] = base_colors[i % len(base_colors)]
    
    # Handle cases with rew (LRM graphs)
    if rew_values:
        for i, method_loss in enumerate(method_losses):
            base_color = base_colors[i % len(base_colors)]
            
            # Create gradient from light to dark
            n_shades = len(rew_values)
            if n_shades > 1:
                # Create shades from lighter to darker
                color_shades = sns.light_palette(base_color, n_colors=n_shades + 2)[1:-1]
                #color_shades = color_shades[::-1]  # Reverse to go light to dark
            else:
                color_shades = [base_color]
            
            for j, rew in enumerate(rew_values):
                palette[f"{method_loss}, $k={int(rew)}$"] = color_shades[j]
    
    return palette

# Apply in your plotting loop
for method in ["DimensionalityReduction", "OrthogonalRelaxation"]:
    method_data = temp_best[temp_best["method_display"] == METHOD_NAMES[method]]
    
    for graph_type in graph_types:
        graph_data = method_data[method_data["graph_type"] == graph_type]
        
        if not graph_data.empty:
            # Create custom palette for this specific dataset
            custom_palette = create_loss_rew_palette(
                graph_data, 
            )

            g = sns.catplot(
                data=graph_data,
                y="symmetry",
                x="c",
                hue="method_rew_display",  # Composite variable
                palette=custom_palette,     # Custom palette
                row="density",
                col="n_nodes",
                kind="box",
                showfliers=False,
                height=3,
                aspect=1.0,
                dodge=True
            )
            g._legend.set_title("Loss" + (", Rewirings" if graph_type == "LRM" else ""))
            
            g.set_axis_labels("Penalty $c$", "Symmetry coefficient")
            g.set_titles("$n={col_name}$, $\\rho={row_name}$")
            g.fig.suptitle(f"Symmetry Quality vs. Penalty for {METHOD_NAMES[method]} Method on {graph_type} Networks", y=1.04, weight='bold')
            
            save_figure(g.fig, f"symmetry-vs-c-{METHOD_NAMES[method].lower()}-{graph_type.lower()}")
            plt.show()
```

Penalty sweep symmetry plots: Examine monotonicity vs $c$; look for minimal $c$ achieving stable median without expanding variance. Diverging behavior across graph types can inform per‑family tuning.


## Annealing Effects {#sec-annealing}

Temperature annealing gradually decreases the temperature parameter during
optimization, potentially balancing exploration and exploitation. We compare
fixed temperature strategies against cosine annealing schedules.

```{python}
#| label: fig-annealing-comparison
#| fig-cap: "Comparison of fixed temperature versus annealing strategies for temperature-based methods. Annealing uses cosine schedule: DR (10.0→0.1), OR (0.7→0.3)."
#| fig-width: 12
#| fig-height: 10

# Select best fixed temperature configurations
# Note: Correcting tau value for DimensionalityReduction based on earlier optimal configuration analysis
best_fixed = df_temp[
    (~df_temp['is_anneal']) & 
    (((df_temp['method'] == 'DimensionalityReduction') & (df_temp['method_initial_tau'] == 0.1) & (df_temp['c'] == 0.1)) |
     ((df_temp['method'] == 'OrthogonalRelaxation') & (df_temp['method_initial_tau'] == 0.7) & (df_temp['c'] == 0.1)))
].copy()

# Combine with annealing data
annealing_comparison = pd.concat([best_fixed, df_temp[df_temp['is_anneal']]])

# Filter non-identity solutions
annealing_comparison = annealing_comparison[
    annealing_comparison["n_nodes"] != annealing_comparison["fixed_points"]
]

# Get best per simulation
annealing_best = (
    annealing_comparison
    .groupby(["method_display", "method_loss", "is_anneal", 
              "graph_type", "density", "n_nodes", "rew", "sim_id"], dropna=False)["symmetry"]
    .min()
    .reset_index()
)

# Create combined identifier
annealing_best['strategy'] = annealing_best.apply(
    lambda x: f"{x['method_display']}, {x['method_loss']}, {'Anneal' if x['is_anneal'] else 'Fixed'}", 
    axis=1
)

# Plot comparison for each method
for method in ["DimensionalityReduction", "OrthogonalRelaxation"]:
    method_data = annealing_best[
        annealing_best["method_display"] == METHOD_NAMES[method]
    ]
    
    if not method_data.empty:
        # Separate standard graph types from LRM networks (LRM has rewiring parameter)
        standard_types = STANDARD_GRAPH_TYPES
        er_ba_data = method_data[method_data["graph_type"].isin(standard_types)]
        lrm_data = method_data[method_data["graph_type"] == "LRM"]
        
        # Plot standard graph types (faceted by graph_type and density)
        if not er_ba_data.empty:
            g = sns.catplot(
                data=er_ba_data,
                y="symmetry",
                x="n_nodes",
                hue="strategy",
                row="graph_type",
                col="density",
                kind="box",
                showfliers=False,
                height=3,
                aspect=1.5,
                dodge=True,
                sharex=True,
                sharey=True,
                palette=sns.color_palette("deep")[::-1],
            )
            g._legend.set_title("Method, Loss, $\\tau$ Strategy")
            
            g.set_axis_labels("Number of nodes", "Symmetry coefficient")
            g.set_titles("{row_name}, $\\rho = {col_name}$")
            g.fig.suptitle(f"Annealing Effect vs. Fixed $\\tau$ for {METHOD_NAMES[method]} on Standard Graph Types", y=1.04, weight='bold')
            
            save_figure(g.fig, f"annealing-comparison-{METHOD_NAMES[method].lower()}-standard")
            plt.show()
        
        # Plot LRM networks separately with rewiring faceting
        if not lrm_data.empty:
            g = sns.catplot(
                data=lrm_data,
                y="symmetry",
                x="rew",
                hue="strategy",
                row="n_nodes",
                col="density",
                kind="box",
                showfliers=False,
                height=3,
                aspect=1.5,
                dodge=True,
                sharex=True,
                sharey=True,
                palette=sns.color_palette("deep")[::-1],
            )
            g._legend.set_title("Method, Loss, $\\tau$ Strategy")
            
            g.set_axis_labels("Number of rewirings ($k$)", "Symmetry coefficient")
            g.set_titles("$n={row_name}$, $\\rho={col_name}$")
            g.fig.suptitle(f"Annealing Effect vs. Fixed $\\tau$ for {METHOD_NAMES[method]} on LRM Networks", y=1.04, weight='bold')
            
            save_figure(g.fig, f"annealing-comparison-{METHOD_NAMES[method].lower()}-lrm-alt")
            plt.show()
```

Annealing comparison: Fixed vs scheduled temperature strategies summarized in boxplots. Look for systematic shifts in median or variance indicating benefit of gradual temperature change. Strategy labels encode method, loss, and schedule.


```{python}
#| label: tbl-annealing-summary
#| tbl-cap: "Statistical comparison of fixed temperature versus annealing strategies"

# Calculate summary statistics for annealing comparison
annealing_stats = []
for method in ["DimensionalityReduction", "OrthogonalRelaxation"]:
    method_data = annealing_best[annealing_best["method_display"] == METHOD_NAMES[method]]

    groups = method_data.groupby(
        ['method_loss', 'is_anneal', 'graph_type', 'n_nodes', 'density', 'rew'], dropna=False
    )
    for group_id, group_data in groups:
        (method_loss, is_anneal, graph_type, n_nodes, density, rew_label) = group_id
        symmetry_data = group_data["symmetry"]

        if not subset.empty:
            annealing_stats.append({
                'Method': METHOD_NAMES[method],
                'Loss': method_loss,
                'Strategy': 'Annealing' if is_anneal else 'Fixed',
                'Graph Type': graph_type,
                'Nodes': n_nodes,
                'Density': density,
                'Rewirings': '' if pd.isna(rew) else str(rew),
                'Median': symmetry_data.median(),
                'Mean': symmetry_data.mean(),
                'SD': symmetry_data.std(),
            })

annealing_df = pd.DataFrame(annealing_stats)
print(annealing_df.to_markdown(index=False, floatfmt=".4f"))
save_table(
    annealing_df.set_index(["Method", "Loss", "Strategy", "Graph Type", "Nodes", "Density", "Rewirings"]),
    "annealing_summary",
    "Performance comparison of fixed versus annealing temperature strategies",
    label="tab:annealing-summary"
)
```

Section intentionally omits synthetic summary; integrate insights externally by juxtaposing optimal temperature, penalty, and loss choices.

# Comparative Analysis {#sec-comparison}

Having analyzed iterative and temperature-based methods separately, we now
present a unified comparison across all five methods.

## Parameter Selection Infrastructure {#sec-parameter-infrastructure}

This infrastructure provides flexible parameter selection with two modes:

- **Global mode**: Parameters optimized across all graph types (strict, conservative)
- **Per-graph-type mode**: Topology-specific parameters (allows per-topology tuning)

The global mode finds parameters that work well everywhere, which may be suboptimal 
for specific topologies. The per-graph-type mode allows each topology to use its 
best parameters, which can reveal method performance without cross-topology constraints.

```{python}
#| label: parameter-selection-infrastructure
#| code-summary: "Infrastructure for selecting optimal parameters globally or per-graph-type"

def build_optimal_configs_per_graph_type(df, df_temp, success_by_sim, success_by_sim_temp):
    """
    Build optimal parameter configurations on a per-graph-type basis.
    
    For each graph type, finds optimal parameters that work well across all 
    network sizes and densities within that specific topology. This allows
    for topology-specific tuning rather than forcing global parameters.
    
    Parameters
    ----------
    df : pd.DataFrame
        Main results dataframe for iterative methods
    df_temp : pd.DataFrame
        Results dataframe for temperature methods
    success_by_sim : pd.DataFrame
        Success rate data for iterative methods
    success_by_sim_temp : pd.DataFrame
        Success rate data for temperature methods
        
    Returns
    -------
    dict
        Nested structure: {method: {graph_type: params}}
    """
    configs = {}
    standard_graph_types = STANDARD_WITH_LRM
    
    # Process iterative methods
    # Map between method names in df ('QSA', 'Manifold', 'InteriorPoint') 
    # and display names in success_by_sim ('QSA', 'MF', 'IP')
    method_mapping = {
        'QSA': 'QSA',
        'Manifold': 'MF', 
        'InteriorPoint': 'IP'
    }
    
    for method in ['QSA', 'Manifold', 'InteriorPoint']:
        configs[method] = {}
        method_display = method_mapping[method]
        
        for graph_type in standard_graph_types:
            method_success = success_by_sim[
                (success_by_sim['method_display'] == method_display) &
                (success_by_sim['graph_type'] == graph_type)
            ]
            
            method_symmetry = df[
                (df['method'] == method) &
                (df['graph_type'] == graph_type)
            ]
            
            if method_success.empty or method_symmetry.empty:
                continue
            
            # Find c values with perfect success rate for this graph type
            c_metrics = (
                method_success
                .groupby('c')
                .agg(
                    success_rate=('success', 'mean'),
                    total_sims=('success', 'count')
                )
                .reset_index()
            )
            
            perfect_c = c_metrics[c_metrics['success_rate'] >= 0.99]['c'].values
            
            if len(perfect_c) == 0:
                continue
            
            # Among perfect c values, find the one with best symmetry
            best_c = None
            best_symmetry = float('inf')
            
            for c_val in perfect_c:
                c_sym = method_symmetry[method_symmetry['c'] == c_val]['symmetry']
                if len(c_sym) > 0:
                    mean_sym = c_sym.mean()
                    if mean_sym < best_symmetry:
                        best_symmetry = mean_sym
                        best_c = c_val
            
            if best_c is not None:
                configs[method][graph_type] = {'c': best_c}
    
    # Process temperature methods
    for method in ['DimensionalityReduction', 'OrthogonalRelaxation']:
        configs[method] = {}
        method_display = "DR" if method == "DimensionalityReduction" else "OR"
        
        for graph_type in standard_graph_types:
            method_success = success_by_sim_temp[
                (success_by_sim_temp['method_display'] == method_display) &
                (success_by_sim_temp['graph_type'] == graph_type)
            ]
            
            method_symmetry = df_temp[
                (df_temp['method'] == method) &
                (df_temp['graph_type'] == graph_type)
            ]
            
            if method_success.empty or method_symmetry.empty:
                continue
            
            # Find (tau, c) combinations with perfect success rate
            config_metrics = (
                method_success
                .groupby(['method_initial_tau', 'c'])
                .agg(
                    success_rate=('success', 'mean')
                )
                .reset_index()
            )
            
            perfect_configs = config_metrics[config_metrics['success_rate'] >= 0.99]
            
            if perfect_configs.empty:
                continue
            
            # Among perfect configs, find best symmetry
            best_tau = None
            best_c = None
            best_symmetry = float('inf')
            
            for _, row in perfect_configs.iterrows():
                tau_val = row['method_initial_tau']
                c_val = row['c']
                
                config_sym = method_symmetry[
                    (method_symmetry['method_initial_tau'] == tau_val) &
                    (method_symmetry['c'] == c_val) &
                    (method_symmetry['method_loss'] == 'convex') &
                    (method_symmetry['method_final_tau'].isna())
                ]['symmetry']
                
                if len(config_sym) > 0:
                    mean_sym = config_sym.mean()
                    if mean_sym < best_symmetry:
                        best_symmetry = mean_sym
                        best_tau = tau_val
                        best_c = c_val
            
            if best_tau is not None and best_c is not None:
                configs[method][graph_type] = {
                    'method_initial_tau': best_tau,
                    'c': best_c,
                    'method_loss': 'convex'
                }
    
    return configs


def build_optimal_configs(mode, df=None, df_temp=None, success_by_sim=None, success_by_sim_temp=None, 
                         optimal_c_df=None, optimal_temp_df=None):
    """
    Factory function to build optimal parameter configurations.
    
    Parameters
    ----------
    mode : str
        'global' - Use globally optimal parameters from ranked voting
        'per_graph_type' - Use graph-type-specific optimal parameters
    df, df_temp : pd.DataFrame
        Required for 'per_graph_type' mode
    success_by_sim, success_by_sim_temp : pd.DataFrame
        Required for 'per_graph_type' mode
    optimal_c_df, optimal_temp_df : pd.DataFrame
        Required for 'global' mode
        
    Returns
    -------
    dict
        If mode='global': {method: params}
        If mode='per_graph_type': {method: {graph_type: params}}
    """
    if mode == "global":
        # Extract global optimal values from ranked voting DataFrames
        qsa_optimal_c = optimal_c_df[optimal_c_df['Method'] == 'QSA']['Optimal c'].iloc[0]
        manifold_optimal_c = optimal_c_df[optimal_c_df['Method'] == 'MF']['Optimal c'].iloc[0]
        ip_optimal_c = optimal_c_df[optimal_c_df['Method'] == 'IP']['Optimal c'].iloc[0]
        
        dr_optimal_tau = optimal_temp_df[optimal_temp_df['Method'] == 'DR']['Optimal tau'].iloc[0]
        dr_optimal_c = optimal_temp_df[optimal_temp_df['Method'] == 'DR']['Optimal c'].iloc[0]
        
        or_optimal_tau = optimal_temp_df[optimal_temp_df['Method'] == 'OR']['Optimal tau'].iloc[0]
        or_optimal_c = optimal_temp_df[optimal_temp_df['Method'] == 'OR']['Optimal c'].iloc[0]
        
        return {
            'QSA': {
                'c': qsa_optimal_c
            },
            'Manifold': {
                'c': manifold_optimal_c
            },
            'InteriorPoint': {
                'c': ip_optimal_c
            },
            'DimensionalityReduction': {
                'method_initial_tau': dr_optimal_tau,
                'c': dr_optimal_c,
                'method_loss': 'convex'
            },
            'OrthogonalRelaxation': {
                'method_initial_tau': or_optimal_tau,
                'c': or_optimal_c,
                'method_loss': 'convex'
            },
        }
    
    elif mode == "per_graph_type":
        return build_optimal_configs_per_graph_type(df, df_temp, success_by_sim, success_by_sim_temp)
    
    else:
        raise ValueError(f"Unknown mode: {mode}. Use 'global' or 'per_graph_type'")


def get_optimal_params(method, graph_type, configs, mode="global"):
    """
    Retrieve optimal parameters for a method and graph type.
    
    Parameters
    ----------
    method : str
        Method name (e.g., 'QSA', 'DimensionalityReduction')
    graph_type : str
        Graph type (e.g., 'ER', 'BA', 'GEO2D')
    configs : dict
        Configuration dictionary from build_optimal_configs()
    mode : str
        'global' or 'per_graph_type'
        
    Returns
    -------
    dict or None
        Parameter dictionary, or None if not found
    """
    if mode == "global":
        return configs.get(method, None)
    
    elif mode == "per_graph_type":
        method_configs = configs.get(method, {})
        return method_configs.get(graph_type, None)
    
    else:
        raise ValueError(f"Unknown mode: {mode}")
```

## Method Comparison Across Graph Types {#sec-unified-comparison}

```{python}
#| label: prepare-unified-data
#| code-summary: "Prepare data for unified comparison using optimal configurations"

# ==============================================================================
# PARAMETER SELECTION MODE
# ==============================================================================
# Choose 'global' for parameters optimized across all graph types
# Choose 'per_graph_type' for topology-specific parameters
PARAMETER_MODE = "per_graph_type"
# ==============================================================================

# Build optimal configurations based on selected mode
if PARAMETER_MODE == "global":
    optimal_configs = build_optimal_configs(
        mode="global",
        optimal_c_df=optimal_c_df,
        optimal_temp_df=optimal_temp_df
    )
    print(f"Using GLOBAL optimal parameters (optimized across all graph types):")
    print("="*80)
    for method, config in optimal_configs.items():
        print(f"  {METHOD_NAMES.get(method, method)}: {config}")
    
elif PARAMETER_MODE == "per_graph_type":
    optimal_configs = build_optimal_configs(
        mode="per_graph_type",
        df=df,
        df_temp=df_temp,
        success_by_sim=success_by_sim,
        success_by_sim_temp=success_by_sim_temp
    )
    print(f"Using PER-GRAPH-TYPE optimal parameters (topology-specific tuning):")
    print("="*80)
    for method, graph_configs in optimal_configs.items():
        print(f"\n{METHOD_NAMES.get(method, method)}:")
        for graph_type, config in graph_configs.items():
            print(f"  {graph_type}: {config}")

# Filter data for optimal configurations
unified_data = []

if PARAMETER_MODE == "global":
    # Global mode: use same parameters for all graph types
    for method in ['QSA', 'Manifold', 'InteriorPoint']:
        params = get_optimal_params(method, None, optimal_configs, mode=PARAMETER_MODE)
        if params is None:
            continue
        
        method_df = df[
            (df['method'] == method) & 
            (df['c'] == params['c'])
        ].copy()
        method_df['config'] = f"c={params['c']}"
        unified_data.append(method_df)
    
    for method in ['DimensionalityReduction', 'OrthogonalRelaxation']:
        params = get_optimal_params(method, None, optimal_configs, mode=PARAMETER_MODE)
        if params is None:
            continue
        
        method_df = df[
            (df['method'] == method) & 
            (df['method_initial_tau'] == params['method_initial_tau']) &
            (df['method_loss'] == params['method_loss']) &
            (df['c'] == params['c']) &
            (df['method_final_tau'].isna())
        ].copy()
        method_df['config'] = f"tau={params['method_initial_tau']}, {params['method_loss']}"
        unified_data.append(method_df)

elif PARAMETER_MODE == "per_graph_type":
    # Per-graph-type mode: use different parameters for each topology
    standard_graph_types = STANDARD_WITH_LRM
    
    for method in ['QSA', 'Manifold', 'InteriorPoint']:
        for graph_type in standard_graph_types:
            params = get_optimal_params(method, graph_type, optimal_configs, mode=PARAMETER_MODE)
            if params is None:
                continue
            
            method_df = df[
                (df['method'] == method) &
                (df['graph_type'] == graph_type) &
                (df['c'] == params['c'])
            ].copy()
            method_df['config'] = f"{graph_type}: c={params['c']}"
            unified_data.append(method_df)
    
    for method in ['DimensionalityReduction', 'OrthogonalRelaxation']:
        for graph_type in standard_graph_types:
            params = get_optimal_params(method, graph_type, optimal_configs, mode=PARAMETER_MODE)
            if params is None:
                continue
            
            method_df = df[
                (df['method'] == method) &
                (df['graph_type'] == graph_type) &
                (df['method_initial_tau'] == params['method_initial_tau']) &
                (df['method_loss'] == params['method_loss']) &
                (df['c'] == params['c']) &
                (df['method_final_tau'].isna())
            ].copy()
            method_df['config'] = f"{graph_type}: tau={params['method_initial_tau']}, {params['method_loss']}"
            unified_data.append(method_df)

# Combine all data
if unified_data:
    unified_df = pd.concat(unified_data, ignore_index=True)
    unified_df['method_display'] = unified_df['method'].map(METHOD_NAMES)
    
    # Filter non-identity solutions
    unified_df = unified_df[unified_df["n_nodes"] != unified_df["fixed_points"]]
    
    # Get best result per simulation
    unified_best = (
        unified_df
        .groupby(["method_display", "graph_type", "n_nodes", "density", "rew", "sim_id"], dropna=False)["symmetry"]
        .min()
        .reset_index()
    )
else:
    print("Warning: No data found for selected parameter mode")
    unified_df = pd.DataFrame()
    unified_best = pd.DataFrame()
```

```{python}
#| label: tbl-optimal-parameters-global
#| tbl-cap: "Global optimal parameters for all methods selected using ranked voting across all graph topologies."

# Build global optimal parameters table
global_configs = build_optimal_configs(
    mode="global",
    optimal_c_df=optimal_c_df,
    optimal_temp_df=optimal_temp_df
)

global_params = []

# Add iterative methods (QSA, MF, IP)
for method in ['QSA', 'Manifold', 'InteriorPoint']:
    if method in global_configs:
        params = global_configs[method]
        global_params.append({
            'Method': METHOD_NAMES[method],
            'c': params['c'],
            'tau': None,
            'Loss': None
        })

# Add temperature methods (DR, OR)
for method in ['DimensionalityReduction', 'OrthogonalRelaxation']:
    if method in global_configs:
        params = global_configs[method]
        global_params.append({
            'Method': METHOD_NAMES[method],
            'c': params['c'],
            'tau': params['method_initial_tau'],
            'Loss': params['method_loss']
        })

global_params_df = pd.DataFrame(global_params)
column_order = ['Method', 'c', 'tau', 'Loss']
global_params_df = global_params_df[column_order]

print("Global Optimal Parameters (used across all graph types):")
print("="*80)
print(global_params_df.to_markdown(index=False, floatfmt=".4f"))

save_table(
    global_params_df,
    "optimal_parameters_global",
    "Global optimal parameters for all methods (ranked voting across all topologies)",
    label="tab:optimal-parameters-global"
)
```

```{python}
#| label: tbl-optimal-parameters-per-graph-type
#| tbl-cap: "Per-graph-type optimal parameters for all methods, allowing topology-specific tuning."

# Build per-graph-type optimal parameters
per_graph_configs = build_optimal_configs(
    mode="per_graph_type",
    df=df,
    df_temp=df_temp,
    success_by_sim=success_by_sim,
    success_by_sim_temp=success_by_sim_temp
)

# Create comprehensive table with all method-graph_type combinations
per_graph_params = []
standard_graph_types = STANDARD_WITH_LRM

# Add iterative methods
for method in ['QSA', 'Manifold', 'InteriorPoint']:
    method_display = METHOD_NAMES[method]
    if method in per_graph_configs:
        for graph_type in standard_graph_types:
            if graph_type in per_graph_configs[method]:
                params = per_graph_configs[method][graph_type]
                per_graph_params.append({
                    'Method': method_display,
                    'Graph Type': graph_type,
                    'c': params['c'],
                    'tau': None,
                    'Loss': None
                })
            else:
                # Mark missing configurations
                per_graph_params.append({
                    'Method': method_display,
                    'Graph Type': graph_type,
                    'c': None,
                    'tau': None,
                    'Loss': None
                })

# Add temperature methods
for method in ['DimensionalityReduction', 'OrthogonalRelaxation']:
    method_display = METHOD_NAMES[method]
    if method in per_graph_configs:
        for graph_type in standard_graph_types:
            if graph_type in per_graph_configs[method]:
                params = per_graph_configs[method][graph_type]
                per_graph_params.append({
                    'Method': method_display,
                    'Graph Type': graph_type,
                    'c': params['c'],
                    'tau': params['method_initial_tau'],
                    'Loss': params['method_loss']
                })
            else:
                # Mark missing configurations
                per_graph_params.append({
                    'Method': method_display,
                    'Graph Type': graph_type,
                    'c': None,
                    'tau': None,
                    'Loss': None
                })

per_graph_params_df = pd.DataFrame(per_graph_params)
column_order = ['Method', 'Graph Type', 'c', 'tau', 'Loss']
per_graph_params_df = per_graph_params_df[column_order]

print("\nPer-Graph-Type Optimal Parameters (topology-specific tuning):")
print("="*80)
print(per_graph_params_df.to_markdown(index=False, floatfmt=".4f"))

# Split into iterative methods (QSA, MF, IP) and temperature methods (DR, OR)
iterative_methods = ['QSA', 'MF', 'IP']
temperature_methods = ['DR', 'OR']

# Iterative methods table (c only, no tau)
iterative_params_df = per_graph_params_df[
    per_graph_params_df['Method'].isin(iterative_methods)
][['Method', 'Graph Type', 'c', 'Loss']].copy()
iterative_params_df_indexed = iterative_params_df.set_index(['Method', 'Graph Type'])

save_table(
    iterative_params_df_indexed,
    "optimal_parameters_per_graph_type_iterative",
    "Per-graph-type optimal parameters for iterative methods (QSA, MF, IP)",
    label="tab:optimal-parameters-per-graph-type-iterative",
)

# Temperature methods table (c and tau)
temperature_params_df = per_graph_params_df[
    per_graph_params_df['Method'].isin(temperature_methods)
][['Method', 'Graph Type', 'c', 'tau']].copy()
temperature_params_df_indexed = temperature_params_df.set_index(['Method', 'Graph Type'])

save_table(
    temperature_params_df_indexed,
    "optimal_parameters_per_graph_type_temperature",
    "Per-graph-type optimal parameters for temperature methods (DR, OR)",
    label="tab:optimal-parameters-per-graph-type-temperature",
)

# Summary statistics
print("\n" + "="*80)
print("Summary:")
print("="*80)
for method in ['QSA', 'MF', 'IP', 'DR', 'OR']:
    method_data = per_graph_params_df[per_graph_params_df['Method'] == method]
    total = len(method_data)
    available = method_data['c'].notna().sum()
    print(f"{method}: {available}/{total} graph types have optimal parameters")
```

Unified comparison: Cross‑method boxplots under each method's selected configuration allow visual assessment of relative symmetry distributions. Use consistent ordering to quickly contrast medians and dispersion across methods.


```{python}
#| label: fig-unified-comparison-separate
#| fig-cap: "Separate comparison plots for each graph type showing all five methods using their optimal configurations. Methods are compared on symmetry coefficient (lower is better)."
#| fig-width: 14
#| fig-height: 10

# Create separate comparison plots for each graph type
for graph_type in graph_types:
    data_subset = unified_best[unified_best["graph_type"] == graph_type]
    
    if not data_subset.empty:
        if graph_type != "LRM":
            g = sns.catplot(
                data=data_subset,
                x="n_nodes",
                y="symmetry",
                hue="method_display",
                hue_order=['QSA', 'MF', 'IP', 'DR', 'OR'],
                palette={m: METHOD_COLORS[k] for k, m in METHOD_NAMES.items()},
                col="density" if graph_type != "LRM" else "rew",
                kind="box",
                height=4,
                aspect=1.5,
                dodge=True,
                showfliers=False
            )
            g._legend.set_title("Method")
            
            g.set_axis_labels("Number of nodes", "Symmetry coefficient")
            g.set_titles("$\\rho={col_name}$")
            g.fig.suptitle(f"Symmetry Comparison - All Methods: {graph_type} Networks", y=1.04, weight='bold')
            
            # Format y-axis tick labels to avoid LaTeX rendering issues
            for ax in g.axes.flat:
                ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x:.3f}'))
            
            save_figure(g.fig, f"unified-comparison-{graph_type.lower()}")
            plt.show()
        else:
            g = sns.catplot(
                data=data_subset,
                x="rew",
                y="symmetry",
                hue="method_display",
                hue_order=['QSA', 'MF', 'IP', 'DR', 'OR'],
                palette={m: METHOD_COLORS[k] for k, m in METHOD_NAMES.items()},
                col="n_nodes",
                row="density",
                kind="box",
                height=4,
                aspect=1.5,
                dodge=True,
                showfliers=False
            )
            g._legend.set_title("Method")
            
            g.set_titles("$k={col_name}$")
            g._legend.set_title("Method")
            g.set_axis_labels("Number of rewirings", "Symmetry coefficient")
            g.set_titles("$n={col_name}$, $\\rho={row_name}$")
            g.fig.suptitle(f"Symmetry Comparison - All Methods: {graph_type} Networks", y=1.04, weight='bold')
            
            # Format y-axis tick labels to avoid LaTeX rendering issues
            for ax in g.axes.flat:
                ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x:.3f}'))
            
            save_figure(g.fig, f"unified-comparison-{graph_type.lower()}")
            plt.show()
```

```{python}
#| label: fig-unified-comparison-unified
#| fig-cap: "Unified comparison of all methods across standard graph types. Each row represents a graph type, columns show network sizes. Box plots display symmetry coefficient distributions with methods distinguished by color."
#| fig-width: 8
#| fig-height: 10

# Create unified plot combining all standard graph types
standard_graph_types = [gt for gt in STANDARD_GRAPH_TYPES if gt in unified_best['graph_type'].values]

unified_standard = unified_best[unified_best['graph_type'].isin(standard_graph_types)].copy()

if not unified_standard.empty:
    # Ensure proper ordering
    unified_standard['graph_type'] = pd.Categorical(
        unified_standard['graph_type'],
        categories=standard_graph_types,
        ordered=True
    )
    
    g = sns.catplot(
        data=unified_standard,
        x='n_nodes',
        y='symmetry',
        hue='method_display',
        hue_order=['QSA', 'MF', 'IP', 'DR', 'OR'],
        palette={m: METHOD_COLORS[k] for k, m in METHOD_NAMES.items()},
        col='density',
        row='graph_type',
        kind='box',
        height=2.5,
        aspect=1.5,
        dodge=True,
        showfliers=False
    )
    
    g._legend.set_title("Method")
    
    # Move legend to the right
    sns.move_legend(g, "center left", bbox_to_anchor=(1.02, 0.5), frameon=True)
    
    g.set_titles("{row_name} ($\\rho={col_name}$)")
    g.fig.suptitle(
        "Unified Method Comparison Across Graph Types",
        weight='bold', fontsize=24
    )
    
    # Adjust layout
    plt.tight_layout()
    
    # Add common axis labels
    g.set_axis_labels("", "")
    g.fig.text(0.5, 0.01, "Number of nodes", ha='center', fontsize=18)
    g.fig.text(0.01, 0.5, "Symmetry coefficient", va='center', rotation='vertical', fontsize=18)
    
    # Format y-axis tick labels to avoid LaTeX rendering issues
    for ax in g.axes.flat:
        ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x:.3f}'))
    
    save_figure(g.fig, "unified-comparison-all-methods")
    plt.show()
```

Below are the same stastistics written form table as a table

```{python}
#| label: tbl-unified-summary
#| tbl-cap: "Summary statistics for all methods across graph types using optimal configurations"

# Calculate comprehensive summary statistics
comprehensive_stats = []
for method in METHOD_NAMES.values():
    method_data = unified_best[unified_best["method_display"] == method]

    groups = method_data.groupby(
        ['graph_type', 'n_nodes', 'density', 'rew'], dropna=False
    )
    for group_id, group_data in groups:
        (graph_type, n_nodes, density, rew) = group_id
        symmetry_data = group_data["symmetry"]

        if not subset.empty:
            comprehensive_stats.append({
                'Method': method,
                'Graph Type': graph_type,
                'Nodes': n_nodes,
                'Density': density,
                'Rewirings': '' if pd.isna(rew) else str(rew),
                'Median': symmetry_data.median(),
                'Mean': symmetry_data.mean(),
                'SD': symmetry_data.std(),
            })

comprehensive_df = pd.DataFrame(comprehensive_stats)
print(comprehensive_df.to_markdown(index=False, floatfmt=".4f"))
save_table(
    comprehensive_df.set_index(["Method", "Graph Type", "Nodes", "Density", "Rewirings"]),
    "final_performance_summary",
    "Comprehensive performance metrics for all optimization methods"
)
```


```{python}
#| label: fig-computational-time
#| fig-cap: "Computational time required by each method across different network configurations. Time is shown on a logarithmic scale to accommodate the wide range of values. Box plots show the distribution across all runs, with outliers removed for clarity."
#| fig-width: 14
#| fig-height: 10
#| code-summary: "Compare computational time across methods and graph types"

# Filter for optimal configurations as used in unified comparison
time_data = unified_df.copy()

# Separate standard graph types from LRM due to different parameter structure
standard_types = STANDARD_GRAPH_TYPES
er_ba_time = time_data[time_data['graph_type'].isin(standard_types)]
lrm_time = time_data[time_data['graph_type'] == 'LRM']

# Plot standard graph types
if not er_ba_time.empty:
    g = sns.catplot(
        data=er_ba_time,
        x="n_nodes",
        y="metric_time",
        hue="method_display",
        hue_order=['QSA', 'MF', 'IP', 'DR', 'OR'],
        palette={m: METHOD_COLORS[k] for k, m in METHOD_NAMES.items()},
        row="graph_type",
        col="density",
        kind="point",
        #showfliers=False,
        height=3,
        aspect=2,
        legend_out=True
    )
    g._legend.set_title("Method")
    
    # Set log scale and labels
    g.set(yscale="log")
    g.set_axis_labels("Number of nodes", "Time (seconds)")
    g.set_titles("{row_name} ($\\rho={col_name}$)")
    
    # Adjust y-axis format for better readability
    for ax in g.axes.flat:
        ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:.1f}' if y >= 1 else f'{y:.2f}'))
    
    g.fig.suptitle("Computational Time: Standard Graph Types", y=1.04, weight='bold')
    
    save_figure(g.fig, "computational-time-standard")
    plt.show()

# Plot LRM networks separately
if not lrm_time.empty:
    # For LRM, use rewiring as columns
    g = sns.catplot(
        data=lrm_time,
        x="rew",
        y="metric_time",
        hue="method_display",
        hue_order=['QSA', 'MF', 'IP', 'DR', 'OR'],
        palette={m: METHOD_COLORS[k] for k, m in METHOD_NAMES.items()},
        row="density",
        col="n_nodes",
        kind="point",
        #showfliers=False,
        height=3,
        aspect=2,
        legend_out=True,
        sharex=True,
        sharey=True
    )
    g._legend.set_title("Method")
    
    # Set log scale and labels
    g.set(yscale="log")
    g.set_axis_labels("Number of rewirings ($k$)", "Time (seconds)")
    g.set_titles("$n={col_name}$, $\\rho={row_name}$")
    
    # Adjust y-axis format
    for ax in g.axes.flat:
        ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:.1f}' if y >= 1 else f'{y:.2f}'))
    
    g.fig.suptitle("Computational Time: LRM Networks", y=1.04, weight='bold')
    
    save_figure(g.fig, "computational-time-lrm")
    plt.show()
```

```{python}
#| label: tbl-computational-time
#| tbl-cap: "Summary of computational time requirements for all optimization methods for networks with n=100 nodes. Times are reported in seconds across all experimental runs using optimal parameter configurations. The 'Time Ratio vs QSA' column shows the relative computational cost of each method compared to the baseline QSA method (ratio > 1 indicates slower than QSA). Statistics are computed across all graph types with n=100."

# Filter for n=100 only
time_data_100 = time_data[time_data["n_nodes"] == 100]

# Create summary statistics table
time_summary = []
for method in METHOD_NAMES.values():
    method_times = time_data_100[time_data_100["method_display"] == method]["metric_time"]

    if not method_times.empty:
        time_summary.append(
            {
                "Method": method,
                "Median Time (s)": method_times.median(),
                "Mean Time (s)": method_times.mean(),
                "Time Ratio vs QSA": method_times.median()
                / time_data_100[time_data_100["method_display"] == "QSA"][
                    "metric_time"
                ].median(),
            }
        )

time_summary_df = pd.DataFrame(time_summary)
print(time_summary_df.to_markdown(index=False, floatfmt=".2f"))
save_table(
    time_summary_df,
    "computational_time",
    "Summary of computational time requirements for all optimization methods for networks with $n=100$ nodes. Times are reported in seconds across all experimental runs using optimal parameter configurations. The 'Time Ratio vs QSA' column shows the relative computational cost of each method compared to the baseline QSA method (ratio > 1 indicates slower than QSA). Statistics are computed across all graph types with $n=100$.",
    label="tab:computational-time",
)
```


## Computational Efficiency Analysis

Computational time plots: Point/box displays (log scale) show scaling of wall‑clock time with size, density, rewiring. Table reports central tendencies and ratios vs QSA baseline. Look for super‑linear growth or anomalous spikes indicating implementation bottlenecks.

# Statistical Analysis of Method Performance {#sec-statistical-analysis}

Statistical procedures section: Executes pairwise t‑tests (Bonferroni adjusted), computes Cohen's D, and (when sufficient methods) Friedman + Nemenyi ranks. Use rank diagrams to identify clusters of statistically indistinguishable methods; consult effect matrices for magnitude context.

```{python}
#| label: setup-statistical-analysis
#| code-summary: "Import statistical analysis libraries"

import pingouin as pg
from scipy import stats
from itertools import combinations
import warnings
import scikit_posthocs as sp
```

## Data Preparation for Statistical Testing {#sec-stat-data-prep}

```{python}
#| label: prepare-statistical-data
#| code-summary: "Prepare data for statistical analysis using optimal configurations"

# Use the unified_best data already prepared with optimal configurations
# Group by method and graph configuration to get average performance
stat_data = unified_best.copy()

# Create a unique identifier for each problem instance
stat_data['instance_id'] = (
    stat_data['graph_type'].astype(str) + '_' +
    stat_data['n_nodes'].astype(str) + '_' +
    stat_data['density'].astype(str) + '_' +
    stat_data['rew'].astype(str) + '_' +
    stat_data['sim_id'].astype(str)
)

# Display summary
print("Statistical Analysis Data Summary")
print("=" * 60)
print(f"Total problem instances: {stat_data['instance_id'].nunique()}")
print(f"\nInstances per graph type:")
print(stat_data.groupby('graph_type')['instance_id'].nunique())
print(f"\nMethods: {sorted(stat_data['method_display'].unique())}")
print(f"\nSample size per method-graph type combination:")
print(stat_data.groupby(['graph_type', 'method_display']).size().unstack(fill_value=0))
```

## Stratified Statistical Analysis {#sec-stratified-stats}

This section executes stratified computations.

### Helper Functions for Stratified Analysis

```{python}
#| label: stratified-analysis-helpers
#| code-summary: "Define helper functions for stratified statistical visualization"

def perform_pairwise_ttests(data, graph_type, alpha=0.05):
    """
    Perform pairwise t-tests between all method combinations for a specific graph type.
    
    Parameters
    ----------
    data : pd.DataFrame
        Statistical data with method_display and symmetry columns
    graph_type : str
        Graph type to analyze (ER, BA, or LRM)
    alpha : float
        Significance level (default: 0.05)
        
    Returns
    -------
    pd.DataFrame
        Results with p-values and significance indicators
    """
    subset = data[data['graph_type'] == graph_type].copy()
    methods = sorted(subset['method_display'].unique())
    
    results = []
    for method1, method2 in combinations(methods, 2):
        data1 = subset[subset['method_display'] == method1]['symmetry']
        data2 = subset[subset['method_display'] == method2]['symmetry']
        
        # Perform independent samples t-test
        ttest_result = pg.ttest(data1, data2, correction=False)
        
        results.append({
            'Method 1': method1,
            'Method 2': method2,
            'T-statistic': ttest_result['T'].values[0],
            'p-value': ttest_result['p-val'].values[0],
            'DoF': ttest_result['dof'].values[0],
            'Mean Diff': data1.mean() - data2.mean()
        })
    
    results_df = pd.DataFrame(results)
    
    # Apply Bonferroni correction
    n_tests = len(results_df)
    results_df['p-adjusted'] = results_df['p-value'] * n_tests
    results_df['p-adjusted'] = results_df['p-adjusted'].clip(upper=1.0)
    results_df['Significant'] = results_df['p-adjusted'] < alpha
    
    return results_df

# Perform tests for each graph type
pairwise_results = {}
graph_types = ALL_GRAPH_TYPES

# Store pairwise results for later use in stratified analysis
pairwise_results = {}
effect_size_results = {}
ranking_results = {}
posthoc_results = {}

for graph_type in graph_types:
    subset = stat_data[stat_data['graph_type'] == graph_type].copy()
    if subset.empty:
        continue
    
    # Store pairwise t-test results
    pairwise_results[graph_type] = perform_pairwise_ttests(stat_data, graph_type)
    
    # Store effect size results
    methods = sorted(subset['method_display'].unique())
    results = []
    for method1, method2 in combinations(methods, 2):
        data1 = subset[subset['method_display'] == method1]['symmetry']
        data2 = subset[subset['method_display'] == method2]['symmetry']
        cohens_d = pg.compute_effsize(data1, data2, eftype='cohen')
        abs_d = abs(cohens_d)
        if abs_d < 0.2:
            magnitude = 'Negligible'
        elif abs_d < 0.5:
            magnitude = 'Small'
        elif abs_d < 0.8:
            magnitude = 'Medium'
        else:
            magnitude = 'Large'
        results.append({
            'Method 1': method1,
            'Method 2': method2,
            'Cohen\'s D': cohens_d,
            'Magnitude': magnitude,
            'Mean 1': data1.mean(),
            'Mean 2': data2.mean()
        })
    effect_size_results[graph_type] = pd.DataFrame(results)
    
    # Store ranking results (Friedman-Nemenyi)
    pivot_data = subset.pivot_table(
        index='instance_id',
        columns='method_display',
        values='symmetry'
    ).dropna()
    
    if len(pivot_data) >= 2 and pivot_data.shape[1] >= 3:
        friedman_stat, friedman_pval = stats.friedmanchisquare(*pivot_data.T.values)
        if friedman_pval < 0.05:
            posthoc_matrix = sp.posthoc_nemenyi_friedman(pivot_data.values)
            posthoc_matrix.index = pivot_data.columns
            posthoc_matrix.columns = pivot_data.columns
            posthoc_results[graph_type] = posthoc_matrix
            avg_ranks = pivot_data.rank(axis=1).mean(axis=0)
            ranking_results[graph_type] = {
                'friedman_stat': friedman_stat,
                'friedman_pval': friedman_pval,
                'mean_ranks': avg_ranks,
                'posthoc': posthoc_matrix
            }
```

## Stratified Statistical Analysis (extended)

Extended stratified tests (size, density, rewiring).

### Helper Functions for Stratified Analysis

```{python}
#| label: stratified-analysis-helpers
#| code-summary: "Define helper functions for stratified statistical visualization"

def compute_pairwise_pvalues(data, methods):
    """
    Compute pairwise t-test p-values between methods.
    
    Parameters
    ----------
    data : pd.DataFrame
        Data with 'method_display' and 'symmetry' columns
    methods : list
        Ordered list of method names
        
    Returns
    -------
    pd.DataFrame
        Matrix of adjusted p-values
    """
    pval_matrix = pd.DataFrame(
        np.ones((len(methods), len(methods))),
        index=methods,
        columns=methods
    )
    
    # Compute all pairwise comparisons
    comparisons = []
    for m1, m2 in combinations(methods, 2):
        data1 = data[data['method_display'] == m1]['symmetry']
        data2 = data[data['method_display'] == m2]['symmetry']
        
        if len(data1) > 1 and len(data2) > 1:
            ttest_result = pg.ttest(data1, data2, correction=False)
            pval = ttest_result['p-val'].values[0]
            comparisons.append((m1, m2, pval))
    
    # Apply Bonferroni correction
    n_tests = len(comparisons)
    for m1, m2, pval in comparisons:
        pval_adj = min(pval * n_tests, 1.0)
        pval_matrix.at[m1, m2] = pval_adj
        pval_matrix.at[m2, m1] = pval_adj
    
    return pval_matrix


def compute_pairwise_cohens_d(data, methods):
    """
    Compute pairwise Cohen's D effect sizes between methods.
    
    Parameters
    ----------
    data : pd.DataFrame
        Data with 'method_display' and 'symmetry' columns
    methods : list
        Ordered list of method names
        
    Returns
    -------
    pd.DataFrame
        Matrix of Cohen's D values
    """
    cohens_matrix = pd.DataFrame(
        np.zeros((len(methods), len(methods))),
        index=methods,
        columns=methods
    )
    
    for m1, m2 in combinations(methods, 2):
        data1 = data[data['method_display'] == m1]['symmetry']
        data2 = data[data['method_display'] == m2]['symmetry']
        
        if len(data1) > 1 and len(data2) > 1:
            d = pg.compute_effsize(data1, data2, eftype='cohen')
            cohens_matrix.at[m1, m2] = d
            cohens_matrix.at[m2, m1] = -d
    
    return cohens_matrix


def plot_pvalue_heatmap(ax, pval_matrix, title):
    """
    Plot a p-value heatmap on given axes.
    
    Parameters
    ----------
    ax : matplotlib.axes.Axes
        Axes to plot on
    pval_matrix : pd.DataFrame
        Matrix of p-values
    title : str
        Subplot title
    """
    methods = pval_matrix.index.tolist()
    
    # Use -log10(p-value) for better visualization
    log_pval_matrix = -np.log10(pval_matrix.values.astype(float))
    
    im = ax.imshow(log_pval_matrix, cmap='YlOrRd', aspect='auto', vmin=0, vmax=6)
    
    # Add significance stars
    for i in range(len(methods)):
        for j in range(len(methods)):
            if i != j:
                pval = pval_matrix.iloc[i, j]
                text_color = 'white' if log_pval_matrix[i, j] > 3 else 'black'
                
                if pval < 0.001:
                    text = '***'
                elif pval < 0.01:
                    text = '**'
                elif pval < 0.05:
                    text = '*'
                else:
                    text = ''
                
                if text:
                    ax.text(j, i, text, ha='center', va='center',
                           color=text_color, fontsize=16)
    
    ax.set_xticks(range(len(methods)))
    ax.set_yticks(range(len(methods)))
    ax.set_xticklabels(methods, fontsize=10)
    ax.set_yticklabels(methods, fontsize=10)
    ax.set_title(title, fontsize=11, weight='bold')
    
    return im


def plot_cohens_d_heatmap(ax, cohens_matrix, title):
    """
    Plot a Cohen's D heatmap on given axes.
    
    Parameters
    ----------
    ax : matplotlib.axes.Axes
        Axes to plot on
    cohens_matrix : pd.DataFrame
        Matrix of Cohen's D values
    title : str
        Subplot title
    """
    methods = cohens_matrix.index.tolist()
    
    im = ax.imshow(cohens_matrix.values.astype(float), 
                   cmap='RdBu_r', aspect='auto', 
                   vmin=-3, vmax=3)
    
    # Add text annotations
    for i in range(len(methods)):
        for j in range(len(methods)):
            if i != j:
                d = cohens_matrix.iloc[i, j]
                abs_d = abs(d)
                
                # Determine magnitude symbol
                if abs_d < 0.2:
                    symbol = ''
                elif abs_d < 0.5:
                    symbol = '*'
                elif abs_d < 0.8:
                    symbol = '**'
                else:
                    symbol = '***'
                
                text_color = 'white' if abs_d > 1.5 else 'black'
                text = f'{d:.2f}\n{symbol}'
                
                ax.text(j, i, text, ha='center', va='center',
                       color=text_color, fontsize=8)
    
    ax.set_xticks(range(len(methods)))
    ax.set_yticks(range(len(methods)))
    ax.set_xticklabels(methods, fontsize=10)
    ax.set_yticklabels(methods, fontsize=10)
    ax.set_title(title, fontsize=11, weight='bold')
    
    return im


def create_cd_diagram(ax, data, title):
    """
    Create a critical difference diagram on given axes.
    
    Parameters
    ----------
    ax : matplotlib.axes.Axes
        Axes to plot on
    data : pd.DataFrame
        Data with 'method_display', 'instance_id', and 'symmetry' columns
    title : str
        Subplot title
        
    Returns
    -------
    bool
        True if diagram was created, False if insufficient data
    """
    # Map abbreviated method names to colors
    method_display_colors = {
        'QSA': METHOD_COLORS['QSA'],
        'IP': METHOD_COLORS['InteriorPoint'],
        'MF': METHOD_COLORS['Manifold'],
        'OR': METHOD_COLORS['OrthogonalRelaxation'],
        'DR': METHOD_COLORS['DimensionalityReduction']
    }
    
    # Pivot data for Friedman test
    pivot_data = data.pivot_table(
        index='instance_id',
        columns='method_display',
        values='symmetry'
    ).dropna()
    
    if len(pivot_data) < 2:
        ax.text(0.5, 0.5, f'Insufficient data\n(n={len(pivot_data)})',
                ha='center', va='center', transform=ax.transAxes)
        ax.set_title(title, fontsize=11, weight='bold')
        ax.axis('off')
        return False
    
    # Check if we have at least 3 methods (required for Friedman test)
    n_methods = pivot_data.shape[1]
    if n_methods < 3:
        ax.text(0.5, 0.5, f'Insufficient methods\nfor Friedman test\n(need at least 3, got {n_methods})',
                ha='center', va='center', transform=ax.transAxes, fontsize=10)
        ax.set_title(title, fontsize=11, weight='bold')
        ax.axis('off')
        return False
    
    # Perform Friedman test
    friedman_stat, friedman_pval = stats.friedmanchisquare(*pivot_data.T.values)
    
    if friedman_pval >= 0.05:
        ax.text(0.5, 0.5, f'No significant\ndifferences\n(p={friedman_pval:.3f})',
                ha='center', va='center', transform=ax.transAxes)
        ax.set_title(title, fontsize=11, weight='bold')
        ax.axis('off')
        return False
    
    # Perform post-hoc Nemenyi test
    posthoc_matrix = sp.posthoc_nemenyi_friedman(pivot_data.values)
    posthoc_matrix.index = pivot_data.columns
    posthoc_matrix.columns = pivot_data.columns
    
    # Calculate average ranks
    avg_ranks = pivot_data.rank(axis=1).mean(axis=0)
    
    # Plot critical difference diagram
    plt.sca(ax)
    sp.critical_difference_diagram(
        ranks=avg_ranks, 
        sig_matrix=posthoc_matrix,
        color_palette=method_display_colors,
        label_fmt_left='{label} [{rank:.3f}]  ',
        label_fmt_right='  [{rank:.3f}] {label}',
    )
    ax.set_title(f'{title}\n(n={len(pivot_data)}, p={friedman_pval:.3e})', 
                fontsize=11, weight='bold', pad=5)
    
    return True
```

### Stratification by Network Size and Density (ER/BA Networks)

```{python}
#| label: fig-stratified-standard-pvalues
#| fig-cap: "P-value heatmaps from pairwise t-tests stratified by network size and density for standard graph types (ER, BA, GEO2D, GEO3D, DD, HK). Each row represents a different network size (n), each column represents a different density (ρ). Darker colors and asterisks indicate stronger statistical significance."
#| fig-width: 16
#| fig-height: 14
#| code-summary: "Create stratified p-value heatmaps for ER/BA networks"

def create_stratified_pvalue_heatmaps_er_ba(data, graph_type, save_name):
    """
    Create p-value heatmaps stratified by network size and density.
    
    Parameters
    ----------
    data : pd.DataFrame
        Statistical data
    graph_type : str
        'ER' or 'BA'
    save_name : str
        Base name for saving figure
    """
    methods = ['QSA', 'IP', 'MF', 'OR', 'DR']
    
    # Filter for graph type
    subset = data[data['graph_type'] == graph_type].copy()
    
    # Get unique sizes and densities
    sizes = sorted(subset['n_nodes'].unique())
    densities = sorted(subset['density'].unique())
    
    n_rows = len(sizes)
    n_cols = len(densities)
    
    # Create figure
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(4.5 * n_cols, 4 * n_rows),
                            squeeze=False, layout='constrained')
    
    # Track for colorbar
    im = None
    
    for size_idx, size in enumerate(sizes):
        for dens_idx, density in enumerate(densities):
            ax = axes[size_idx, dens_idx]
            
            # Filter data
            cell_data = subset[
                (subset['n_nodes'] == size) &
                (subset['density'] == density)
            ]
            
            if len(cell_data) < 10:  # Minimum samples
                ax.text(0.5, 0.5, 'Insufficient\ndata',
                       ha='center', va='center', transform=ax.transAxes)
                ax.set_title(f'$n={size}$, $\\rho={density}$', fontsize=11, weight='bold')
                ax.axis('off')
                continue
            
            # Compute p-values
            pval_matrix = compute_pairwise_pvalues(cell_data, methods)
            
            # Plot
            im = plot_pvalue_heatmap(ax, pval_matrix, f'$n={size}$, $\\rho={density}$')
            
            # Only show y-labels on leftmost column
            if dens_idx > 0:
                ax.set_yticklabels([])
    
    # Add overall title
    fig.suptitle(
        f'Pairwise T-Test P-Values: {graph_type} Networks\n'
        f'(Stratified by Network Size and Density)',
        fontsize=16, weight='bold'
    )
    
    # Add colorbar
    if im is not None:
        cbar = fig.colorbar(im, ax=axes, orientation='vertical',
                           fraction=0.02, pad=0.02, aspect=30)
        cbar.set_label('$-\\log_{10}(p)$', fontsize=12)
    
    save_figure(fig, f'{save_name}-{graph_type.lower()}-pvalues')
    plt.show()


# Create visualizations for standard graph types (excluding LRM which has different structure)
standard_graph_types = STANDARD_GRAPH_TYPES
for graph_type in standard_graph_types:
    # Skip if no data available for this graph type
    graph_subset = stat_data[stat_data['graph_type'] == graph_type]
    if graph_subset.empty:
        continue
    create_stratified_pvalue_heatmaps_er_ba(stat_data, graph_type, 'statistical-stratified')
```

```{python}
#| label: fig-stratified-standard-cohens-d
#| fig-cap: "Cohen's D effect size heatmaps stratified by network size and density for standard graph types (ER, BA, GEO2D, GEO3D, DD, HK). Each row represents a different network size (n), each column represents a different density (ρ). Blue indicates row method outperforms column method."
#| fig-width: 16
#| fig-height: 14
#| code-summary: "Create stratified Cohen's D heatmaps for ER/BA networks"

def create_stratified_cohens_d_heatmaps_er_ba(data, graph_type, save_name):
    """
    Create Cohen's D heatmaps stratified by network size and density.
    
    Parameters
    ----------
    data : pd.DataFrame
        Statistical data
    graph_type : str
        'ER' or 'BA'
    save_name : str
        Base name for saving figure
    """
    methods = ['QSA', 'IP', 'MF', 'OR', 'DR']
    
    subset = data[data['graph_type'] == graph_type].copy()
    sizes = sorted(subset['n_nodes'].unique())
    densities = sorted(subset['density'].unique())
    
    n_rows = len(sizes)
    n_cols = len(densities)
    
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(4.5 * n_cols, 4 * n_rows),
                            squeeze=False, layout='constrained')
    
    im = None
    
    for size_idx, size in enumerate(sizes):
        for dens_idx, density in enumerate(densities):
            ax = axes[size_idx, dens_idx]
            
            cell_data = subset[
                (subset['n_nodes'] == size) &
                (subset['density'] == density)
            ]
            
            if len(cell_data) < 10:
                ax.text(0.5, 0.5, 'Insufficient\ndata',
                       ha='center', va='center', transform=ax.transAxes)
                ax.set_title(f'$n={size}$, $\\rho={density}$', fontsize=11, weight='bold')
                ax.axis('off')
                continue
            
            # Compute Cohen's D
            cohens_matrix = compute_pairwise_cohens_d(cell_data, methods)
            
            # Plot
            im = plot_cohens_d_heatmap(ax, cohens_matrix, f'$n={size}$, $\\rho={density}$')
            
            if dens_idx > 0:
                ax.set_yticklabels([])
    
    fig.suptitle(
        f'Cohen\'s D Effect Sizes: {graph_type} Networks\n'
        f'(Stratified by Network Size and Density)',
        fontsize=16, weight='bold'
    )
    
    if im is not None:
        cbar = fig.colorbar(im, ax=axes, orientation='vertical',
                           fraction=0.02, pad=0.02, aspect=30)
        cbar.set_label('Cohen\'s D', fontsize=12)
    
    save_figure(fig, f'{save_name}-{graph_type.lower()}-cohens-d')
    plt.show()


# Use standard_graph_types defined earlier
for graph_type in standard_graph_types:
    # Skip if no data available for this graph type
    graph_subset = stat_data[stat_data['graph_type'] == graph_type]
    if graph_subset.empty:
        continue
    create_stratified_cohens_d_heatmaps_er_ba(stat_data, graph_type, 'statistical-stratified')
```

```{python}
#| label: fig-stratified-standard-cd
#| fig-cap: "Critical difference diagrams stratified by network size and density for standard graph types (ER, BA, GEO2D, GEO3D, DD, HK). Each subplot shows the performance ranking for a specific network size and density combination. Groups connected by horizontal bars are not significantly different."
#| fig-width: 16
#| fig-height: 14
#| code-summary: "Create stratified critical difference diagrams for ER/BA networks"

def create_stratified_cd_diagrams_er_ba(data, graph_type, save_name):
    """
    Create critical difference diagrams stratified by network size and density.
    
    Parameters
    ----------
    data : pd.DataFrame
        Statistical data
    graph_type : str
        'ER' or 'BA'
    save_name : str
        Base name for saving figure
    """
    subset = data[data['graph_type'] == graph_type].copy()
    sizes = sorted(subset['n_nodes'].unique())
    densities = sorted(subset['density'].unique())
    
    n_rows = len(sizes)
    n_cols = len(densities)
    
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 3 * n_rows),
                            squeeze=False, layout='constrained')
    
    for size_idx, size in enumerate(sizes):
        for dens_idx, density in enumerate(densities):
            ax = axes[size_idx, dens_idx]
            
            cell_data = subset[
                (subset['n_nodes'] == size) &
                (subset['density'] == density)
            ].copy()
            
            create_cd_diagram(ax, cell_data, f'$n={size}$, $\\rho={density}$')
    
    fig.suptitle(
        f'Critical Difference Diagrams: {graph_type} Networks\n'
        f'(Stratified by Network Size and Density)',
        fontsize=16, weight='bold'
    )
    
    save_figure(fig, f'{save_name}-{graph_type.lower()}-cd')
    plt.show()


# Use standard_graph_types defined earlier
for graph_type in standard_graph_types:
    # Skip if no data available for this graph type
    graph_subset = stat_data[stat_data['graph_type'] == graph_type]
    if graph_subset.empty:
        continue
    create_stratified_cd_diagrams_er_ba(stat_data, graph_type, 'statistical-stratified')
```

```{python}
#| label: fig-stratified-standard-sign-plot
#| fig-cap: "Sign plots showing pairwise significance from Nemenyi post-hoc tests stratified by network size and density for standard graph types. Each subplot shows significance patterns for a specific size-density combination. Colored cells indicate significant differences (p < 0.05)."
#| fig-width: 16
#| fig-height: 14
#| code-summary: "Create stratified sign plots for standard graph types"

def create_stratified_sign_plots_standard(data, graph_type, save_name):
    """
    Create sign plots stratified by network size and density for standard graph types.
    
    Parameters
    ----------
    data : pd.DataFrame
        Statistical data
    graph_type : str
        Graph type to analyze
    save_name : str
        Base name for saving figure
    """
    methods = ['QSA', 'IP', 'MF', 'OR', 'DR']
    
    subset = data[data['graph_type'] == graph_type].copy()
    sizes = sorted(subset['n_nodes'].unique())
    densities = sorted(subset['density'].unique())
    
    n_rows = len(sizes)
    n_cols = len(densities)
    
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 4 * n_rows),
                            squeeze=False, layout='constrained')
    
    for size_idx, size in enumerate(sizes):
        for dens_idx, density in enumerate(densities):
            ax = axes[size_idx, dens_idx]
            
            cell_data = subset[
                (subset['n_nodes'] == size) &
                (subset['density'] == density)
            ].copy()
            
            # Check if we have enough data
            pivot_data = cell_data.pivot_table(
                index='instance_id',
                columns='method_display',
                values='symmetry'
            ).dropna()
            
            # Reorder columns to match methods list
            available_methods = [m for m in methods if m in pivot_data.columns]
            pivot_data = pivot_data[available_methods]
            
            if len(pivot_data) < 2 or pivot_data.shape[1] < 3:
                ax.text(0.5, 0.5, f'Insufficient\ndata',
                       ha='center', va='center', transform=ax.transAxes)
                ax.set_title(f'$n={size}$, $\\rho={density}$', fontsize=11, weight='bold')
                ax.axis('off')
                continue
            
            # Perform Friedman and Nemenyi tests
            try:
                friedman_stat, friedman_pval = stats.friedmanchisquare(*pivot_data.T.values)
                
                if friedman_pval >= 0.05:
                    ax.text(0.5, 0.5, f'No significant\ndifferences\n(p={friedman_pval:.3f})',
                           ha='center', va='center', transform=ax.transAxes)
                    ax.set_title(f'$n={size}$, $\\rho={density}$', fontsize=11, weight='bold')
                    ax.axis('off')
                    continue
                
                # Perform Nemenyi test
                posthoc_matrix = sp.posthoc_nemenyi_friedman(pivot_data.values)
                posthoc_matrix.index = pivot_data.columns
                posthoc_matrix.columns = pivot_data.columns
                
                # Reorder posthoc_matrix to match methods list
                posthoc_matrix = posthoc_matrix.loc[available_methods, available_methods]
                
                # Plot sign plot
                heatmap_args = {
                    'linewidths': 0.5,
                    'linecolor': '0.5',
                    'clip_on': False,
                    'square': True,
                    'cbar': False
                }
                sp.sign_plot(posthoc_matrix, ax=ax, **heatmap_args)
                ax.set_title(f'$n={size}$, $\\rho={density}$', fontsize=11, weight='bold')
                ax.set_xlabel('')
                ax.set_ylabel('')
                
                if dens_idx > 0:
                    ax.set_yticklabels([])
                if size_idx < n_rows - 1:
                    ax.set_xticklabels([])
                    
            except Exception as e:
                ax.text(0.5, 0.5, f'Error:\n{str(e)[:20]}',
                       ha='center', va='center', transform=ax.transAxes)
                ax.set_title(f'$n={size}$, $\\rho={density}$', fontsize=11, weight='bold')
                ax.axis('off')
    
    fig.suptitle(
        f'Nemenyi Post-hoc Significance: {graph_type} Networks\n'
        f'(Stratified by Network Size and Density)',
        fontsize=16, weight='bold'
    )
    
    save_figure(fig, f'{save_name}-{graph_type.lower()}-sign-plot')
    plt.show()


# Use standard_graph_types defined earlier
for graph_type in standard_graph_types:
    # Skip if no data available for this graph type
    graph_subset = stat_data[stat_data['graph_type'] == graph_type]
    if graph_subset.empty:
        continue
    create_stratified_sign_plots_standard(stat_data, graph_type, 'statistical-stratified')
```

```{python}
#| label: fig-stratified-standard-mann-whitney
#| fig-cap: "Mann-Whitney U test significance heatmaps stratified by network size and density for standard graph types (ER, BA, GEO2D, GEO3D, DD, HK). Each row represents a different network size (n), each column represents a different density (rho). Color intensity indicates significance level."
#| fig-width: 16
#| fig-height: 14
#| code-summary: "Create stratified Mann-Whitney U test heatmaps for standard graph types"

from matplotlib.colors import ListedColormap
from mpl_toolkits.axes_grid1 import make_axes_locatable

def calculate_pairwise_mann_whitney(data, methods):
    """
    Calculate pairwise Mann-Whitney U test p-values.
    
    Parameters
    ----------
    data : pd.DataFrame
        Data with columns: instance_id, method_display, symmetry
    methods : list
        List of method names to compare
        
    Returns
    -------
    pd.DataFrame
        Matrix of p-values
    """
    n_methods = len(methods)
    p_values = pd.DataFrame(index=methods, columns=methods, dtype=float)
    
    for m1 in methods:
        for m2 in methods:
            if m1 == m2:
                p_values.at[m1, m2] = np.nan
                continue
            
            d1 = data[data['method_display'] == m1]['symmetry']
            d2 = data[data['method_display'] == m2]['symmetry']
            
            if len(d1) < 2 or len(d2) < 2:
                p_values.at[m1, m2] = np.nan
                continue
            
            # Mann-Whitney U test
            u_stat, p_val = stats.mannwhitneyu(d1, d2, alternative='two-sided')
            p_values.at[m1, m2] = p_val
            
    return p_values

def plot_mann_whitney_heatmap(ax, p_values, title):
    """
    Plot Mann-Whitney U test p-values as categorical significance heatmap.
    
    Parameters
    ----------
    ax : matplotlib.axes.Axes
        Axes to plot on
    p_values : pd.DataFrame
        Matrix of p-values
    title : str
        Title for the subplot
        
    Returns
    -------
    matplotlib.collections.QuadMesh
        The heatmap object
    """
    # Generate cubehelix colors for different significance levels
    cubehelix_colors = sns.cubehelix_palette(n_colors=5, start=0.5, rot=-0.75, light=0.85, dark=0.15)
    cmap_list = [
        "1",  # diagonal (white)
        cubehelix_colors[0].hex() if hasattr(cubehelix_colors[0], 'hex') else '#fbd7d4',  # non-significant
        cubehelix_colors[4].hex() if hasattr(cubehelix_colors[4], 'hex') else '#005a32',  # p < 0.001
        cubehelix_colors[3].hex() if hasattr(cubehelix_colors[3], 'hex') else '#238b45',  # p < 0.01
        cubehelix_colors[2].hex() if hasattr(cubehelix_colors[2], 'hex') else '#a1d99b',  # p < 0.05
    ]
    
    # Transform p-values to categorical bins
    p_values_cat = p_values.copy()
    xc = p_values.values.copy()
    p_values_cat[(xc < 0.001) & (xc >= 0)] = 1
    p_values_cat[(xc < 0.01) & (xc >= 0.001)] = 2
    p_values_cat[(xc < 0.05) & (xc >= 0.01)] = 3
    p_values_cat[(xc >= 0.05)] = 0
    np.fill_diagonal(p_values_cat.values, -1)
    
    # Create the heatmap without colorbar
    heatmap = sns.heatmap(
        p_values_cat,
        cmap=ListedColormap(cmap_list),
        vmin=-1,
        vmax=3,
        center=1,
        cbar=False,
        ax=ax,
        linewidths=0.5,
        linecolor='0',
        clip_on=False,
        square=True
    )
    
    ax.set_title(title, fontsize=11, weight='bold')
    ax.set_xlabel('')
    ax.set_ylabel('')
    
    return heatmap

def create_stratified_mann_whitney_heatmaps(data, graph_type, save_name):
    """
    Create Mann-Whitney U test heatmaps stratified by network size and density.
    
    Parameters
    ----------
    data : pd.DataFrame
        Statistical data
    graph_type : str
        Graph type to analyze
    save_name : str
        Base name for saving figure
    """
    methods = ['QSA', 'IP', 'MF', 'OR', 'DR']
    
    # Filter for graph type
    subset = data[data['graph_type'] == graph_type].copy()
    
    # Get unique sizes and densities
    sizes = sorted(subset['n_nodes'].unique())
    densities = sorted(subset['density'].unique())
    
    n_rows = len(sizes)
    n_cols = len(densities)
    
    # Create figure
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(4.5 * n_cols, 4 * n_rows),
                            squeeze=False, layout='constrained')
    
    # Track for shared colorbar
    heatmap_obj = None
    
    for size_idx, size in enumerate(sizes):
        for dens_idx, density in enumerate(densities):
            ax = axes[size_idx, dens_idx]
            
            # Filter data
            cell_data = subset[
                (subset['n_nodes'] == size) &
                (subset['density'] == density)
            ]
            
            if len(cell_data) < 10:  # Minimum samples
                ax.text(0.5, 0.5, 'Insufficient\ndata',
                       ha='center', va='center', transform=ax.transAxes)
                ax.set_title(f'$n={size}$, $\\rho={density}$', fontsize=11, weight='bold')
                ax.axis('off')
                continue
            
            # Compute Mann-Whitney p-values
            p_values = calculate_pairwise_mann_whitney(cell_data, methods)
            
            # Plot
            heatmap = plot_mann_whitney_heatmap(ax, p_values, f'$n={size}$, $\\rho={density}$')
            if heatmap_obj is None:
                heatmap_obj = heatmap
            
            # Only show y-labels on leftmost column
            if dens_idx > 0:
                ax.set_yticklabels([])
    
    # Add overall title
    fig.suptitle(
        f'Pairwise Mann-Whitney U Test: {graph_type} Networks\n'
        f'(Stratified by Network Size and Density)',
        fontsize=16, weight='bold'
    )
    
    # Add shared colorbar with proper labels
    if heatmap_obj is not None:
        # Create colorbar axis
        cbar = fig.colorbar(
            heatmap_obj.collections[0],
            ax=axes,
            orientation='vertical',
            fraction=0.02,
            pad=0.02,
            aspect=30,
            ticks=[0, 1, 2, 3],
            boundaries=[-0.5, 0.5, 1.5, 2.5, 3.5]
        )
        
        # Customize the colorbar to show significance levels
        cbar.set_label('Significance', fontsize=12)
        cbar.set_ticklabels([r'$p \geq 0.05$', r'$p < 0.001$', r'$p < 0.01$', r'$p < 0.05$'])
        cbar.outline.set_linewidth(1)
        cbar.outline.set_edgecolor('0.5')
        cbar.ax.tick_params(size=0)
    
    save_figure(fig, f'{save_name}-{graph_type.lower()}-mann-whitney')
    plt.show()


# Create visualizations for standard graph types
for graph_type in standard_graph_types:
    # Skip if no data available for this graph type
    graph_subset = stat_data[stat_data['graph_type'] == graph_type]
    if graph_subset.empty:
        continue
    create_stratified_mann_whitney_heatmaps(stat_data, graph_type, 'statistical-stratified')
```

### Stratification by Network Size, Density, and Rewiring (LRM Networks)

```{python}
#| label: fig-stratified-lrm-pvalues
#| fig-cap: "P-value heatmaps from pairwise t-tests stratified by rewiring level and density for LRM networks. Each row represents a different rewiring level (k), each column represents a different density (ρ). Network size n=100 only."
#| fig-width: 16  
#| fig-height: 12
#| code-summary: "Create stratified p-value heatmaps for LRM networks"

def create_stratified_pvalue_heatmaps_lrm(data, save_name, n_nodes=100):
    """
    Create p-value heatmaps for LRM networks stratified by rewiring and density.
    
    Parameters
    ----------
    data : pd.DataFrame
        Statistical data
    save_name : str
        Base name for saving figure
    n_nodes : int
        Network size to analyze (default: 100)
    """
    methods = ['QSA', 'IP', 'MF', 'OR', 'DR']
    
    subset = data[
        (data['graph_type'] == 'LRM') &
        (data['n_nodes'] == n_nodes)
    ].copy()
    
    rewirings = sorted(subset['rew'].unique())
    densities = sorted(subset['density'].unique())
    
    n_rows = len(rewirings)
    n_cols = len(densities)
    
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(4.5 * n_cols, 4 * n_rows),
                            squeeze=False, layout='constrained')
    
    im = None
    
    for rew_idx, rew in enumerate(rewirings):
        for dens_idx, density in enumerate(densities):
            ax = axes[rew_idx, dens_idx]
            
            cell_data = subset[
                (subset['rew'] == rew) &
                (subset['density'] == density)
            ]
            
            if len(cell_data) < 10:
                ax.text(0.5, 0.5, 'Insufficient\ndata',
                       ha='center', va='center', transform=ax.transAxes)
                ax.set_title(f'$k={rew}$, $\\rho={density}$', fontsize=11, weight='bold')
                ax.axis('off')
                continue
            
            pval_matrix = compute_pairwise_pvalues(cell_data, methods)
            im = plot_pvalue_heatmap(ax, pval_matrix, f'$k={rew}$, $\\rho={density}$')
            
            if dens_idx > 0:
                ax.set_yticklabels([])
    
    fig.suptitle(
        f'Pairwise T-Test P-Values: LRM Networks (n={n_nodes})\n'
        f'(Stratified by Rewiring Level and Density)',
        fontsize=16, weight='bold'
    )
    
    if im is not None:
        cbar = fig.colorbar(im, ax=axes, orientation='vertical',
                           fraction=0.02, pad=0.02, aspect=30)
        cbar.set_label('$-\\log_{10}(p)$', fontsize=12)
    
    save_figure(fig, f'{save_name}-lrm-pvalues')
    plt.show()


create_stratified_pvalue_heatmaps_lrm(stat_data, 'statistical-stratified')
```

```{python}
#| label: fig-stratified-lrm-cohens-d
#| fig-cap: "Cohen's D effect size heatmaps stratified by rewiring level and density for LRM networks. Each row represents a different rewiring level (k), each column represents a different density (ρ). Network size n=100 only."
#| fig-width: 16
#| fig-height: 12
#| code-summary: "Create stratified Cohen's D heatmaps for LRM networks"

def create_stratified_cohens_d_heatmaps_lrm(data, save_name, n_nodes=100):
    """
    Create Cohen's D heatmaps for LRM networks stratified by rewiring and density.
    
    Parameters
    ----------
    data : pd.DataFrame
        Statistical data
    save_name : str
        Base name for saving figure
    n_nodes : int
        Network size to analyze (default: 100)
    """
    methods = ['QSA', 'IP', 'MF', 'OR', 'DR']
    
    subset = data[
        (data['graph_type'] == 'LRM') &
        (data['n_nodes'] == n_nodes)
    ].copy()
    
    rewirings = sorted(subset['rew'].unique())
    densities = sorted(subset['density'].unique())
    
    n_rows = len(rewirings)
    n_cols = len(densities)
    
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(4.5 * n_cols, 4 * n_rows),
                            squeeze=False, layout='constrained')
    
    im = None
    
    for rew_idx, rew in enumerate(rewirings):
        for dens_idx, density in enumerate(densities):
            ax = axes[rew_idx, dens_idx]
            
            cell_data = subset[
                (subset['rew'] == rew) &
                (subset['density'] == density)
            ]
            
            if len(cell_data) < 10:
                ax.text(0.5, 0.5, 'Insufficient\ndata',
                       ha='center', va='center', transform=ax.transAxes)
                ax.set_title(f'$k={rew}$, $\\rho={density}$', fontsize=11, weight='bold')
                ax.axis('off')
                continue
            
            cohens_matrix = compute_pairwise_cohens_d(cell_data, methods)
            im = plot_cohens_d_heatmap(ax, cohens_matrix, f'$k={rew}$, $\\rho={density}$')
            
            if dens_idx > 0:
                ax.set_yticklabels([])
    
    fig.suptitle(
        f'Cohen\'s D Effect Sizes: LRM Networks (n={n_nodes})\n'
        f'(Stratified by Rewiring Level and Density)',
        fontsize=16, weight='bold'
    )
    
    if im is not None:
        cbar = fig.colorbar(im, ax=axes, orientation='vertical',
                           fraction=0.02, pad=0.02, aspect=30)
        cbar.set_label('Cohen\'s D', fontsize=12)
    
    save_figure(fig, f'{save_name}-lrm-cohens-d')
    plt.show()


create_stratified_cohens_d_heatmaps_lrm(stat_data, 'statistical-stratified')
```

```{python}
#| label: fig-stratified-lrm-cd
#| fig-cap: "Critical difference diagrams stratified by rewiring level and density for LRM networks. Each subplot shows the performance ranking for a specific rewiring and density combination. Network size n=100 only."
#| fig-width: 16
#| fig-height: 12
#| code-summary: "Create stratified critical difference diagrams for LRM networks"

def create_stratified_cd_diagrams_lrm(data, save_name, n_nodes=100):
    """
    Create critical difference diagrams for LRM networks stratified by rewiring and density.
    
    Parameters
    ----------
    data : pd.DataFrame
        Statistical data
    save_name : str
        Base name for saving figure
    n_nodes : int
        Network size to analyze (default: 100)
    """
    subset = data[
        (data['graph_type'] == 'LRM') &
        (data['n_nodes'] == n_nodes)
    ].copy()
    
    rewirings = sorted(subset['rew'].unique())
    densities = sorted(subset['density'].unique())
    
    n_rows = len(rewirings)
    n_cols = len(densities)
    
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 3 * n_rows),
                            squeeze=False, layout='constrained')
    
    for rew_idx, rew in enumerate(rewirings):
        for dens_idx, density in enumerate(densities):
            ax = axes[rew_idx, dens_idx]
            
            cell_data = subset[
                (subset['rew'] == rew) &
                (subset['density'] == density)
            ].copy()
            
            create_cd_diagram(ax, cell_data, f'$k={rew}$, $\\rho={density}$')
    
    fig.suptitle(
        f'Critical Difference Diagrams: LRM Networks (n={n_nodes})\n'
        f'(Stratified by Rewiring Level and Density)',
        fontsize=16, weight='bold'
    )
    
    save_figure(fig, f'{save_name}-lrm-cd')
    plt.show()


create_stratified_cd_diagrams_lrm(stat_data, 'statistical-stratified')
```

```{python}
#| label: fig-stratified-lrm-sign-plot
#| fig-cap: "Sign plots showing pairwise significance from Nemenyi post-hoc tests stratified by rewiring level and density for LRM networks. Each subplot shows significance patterns for a specific rewiring-density combination. Colored cells indicate significant differences (p < 0.05). Network size n=100 only."
#| fig-width: 16
#| fig-height: 12
#| code-summary: "Create stratified sign plots for LRM networks"

def create_stratified_sign_plots_lrm(data, save_name, n_nodes=100):
    """
    Create sign plots for LRM networks stratified by rewiring and density.
    
    Parameters
    ----------
    data : pd.DataFrame
        Statistical data
    save_name : str
        Base name for saving figure
    n_nodes : int
        Network size to analyze (default: 100)
    """
    methods = ['QSA', 'IP', 'MF', 'OR', 'DR']
    
    subset = data[
        (data['graph_type'] == 'LRM') &
        (data['n_nodes'] == n_nodes)
    ].copy()
    
    rewirings = sorted(subset['rew'].unique())
    densities = sorted(subset['density'].unique())
    
    n_rows = len(rewirings)
    n_cols = len(densities)
    
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 4 * n_rows),
                            squeeze=False, layout='constrained')
    
    for rew_idx, rew in enumerate(rewirings):
        for dens_idx, density in enumerate(densities):
            ax = axes[rew_idx, dens_idx]
            
            cell_data = subset[
                (subset['rew'] == rew) &
                (subset['density'] == density)
            ].copy()
            
            # Check if we have enough data
            pivot_data = cell_data.pivot_table(
                index='instance_id',
                columns='method_display',
                values='symmetry'
            ).dropna()
            
            # Reorder columns to match methods list
            available_methods = [m for m in methods if m in pivot_data.columns]
            pivot_data = pivot_data[available_methods]
            
            if len(pivot_data) < 2 or pivot_data.shape[1] < 3:
                ax.text(0.5, 0.5, f'Insufficient\ndata',
                       ha='center', va='center', transform=ax.transAxes)
                ax.set_title(f'$k={rew}$, $\\rho={density}$', fontsize=11, weight='bold')
                ax.axis('off')
                continue
            
            # Perform Friedman and Nemenyi tests
            try:
                friedman_stat, friedman_pval = stats.friedmanchisquare(*pivot_data.T.values)
                
                if friedman_pval >= 0.05:
                    ax.text(0.5, 0.5, f'No significant\ndifferences\n(p={friedman_pval:.3f})',
                           ha='center', va='center', transform=ax.transAxes)
                    ax.set_title(f'$k={rew}$, $\\rho={density}$', fontsize=11, weight='bold')
                    ax.axis('off')
                    continue
                
                # Perform Nemenyi test
                posthoc_matrix = sp.posthoc_nemenyi_friedman(pivot_data.values)
                posthoc_matrix.index = pivot_data.columns
                posthoc_matrix.columns = pivot_data.columns
                
                # Reorder posthoc_matrix to match methods list
                posthoc_matrix = posthoc_matrix.loc[available_methods, available_methods]
                
                # Plot sign plot
                heatmap_args = {
                    'linewidths': 0.5,
                    'linecolor': '0.5',
                    'clip_on': False,
                    'square': True,
                    'cbar': False
                }
                sp.sign_plot(posthoc_matrix, ax=ax, **heatmap_args)
                ax.set_title(f'$k={rew}$, $\\rho={density}$', fontsize=11, weight='bold')
                ax.set_xlabel('')
                ax.set_ylabel('')
                
                if dens_idx > 0:
                    ax.set_yticklabels([])
                if rew_idx < n_rows - 1:
                    ax.set_xticklabels([])
                    
            except Exception as e:
                ax.text(0.5, 0.5, f'Error:\n{str(e)[:20]}',
                       ha='center', va='center', transform=ax.transAxes)
                ax.set_title(f'$k={rew}$, $\\rho={density}$', fontsize=11, weight='bold')
                ax.axis('off')
    
    fig.suptitle(
        f'Nemenyi Post-hoc Significance: LRM Networks (n={n_nodes})\n'
        f'(Stratified by Rewiring Level and Density)',
        fontsize=16, weight='bold'
    )
    
    save_figure(fig, f'{save_name}-lrm-sign-plot')
    plt.show()


create_stratified_sign_plots_lrm(stat_data, 'statistical-stratified')
```

```{python}
#| label: fig-stratified-lrm-mann-whitney
#| fig-cap: "Mann-Whitney U test significance heatmaps stratified by rewiring level and density for LRM networks. Each row represents a different rewiring level (k), each column represents a different density (rho). Color intensity indicates significance level. Network size n=100 only."
#| fig-width: 16
#| fig-height: 12
#| code-summary: "Create stratified Mann-Whitney U test heatmaps for LRM networks"

def create_stratified_mann_whitney_heatmaps_lrm(data, save_name, n_nodes=100):
    """
    Create Mann-Whitney U test heatmaps for LRM networks stratified by rewiring and density.
    
    Parameters
    ----------
    data : pd.DataFrame
        Statistical data
    save_name : str
        Base name for saving figure
    n_nodes : int
        Network size to analyze (default: 100)
    """
    methods = ['QSA', 'IP', 'MF', 'OR', 'DR']
    
    subset = data[
        (data['graph_type'] == 'LRM') &
        (data['n_nodes'] == n_nodes)
    ].copy()
    
    rewirings = sorted(subset['rew'].unique())
    densities = sorted(subset['density'].unique())
    
    n_rows = len(rewirings)
    n_cols = len(densities)
    
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(4.5 * n_cols, 4 * n_rows),
                            squeeze=False, layout='constrained')
    
    heatmap_obj = None
    
    for rew_idx, rew in enumerate(rewirings):
        for dens_idx, density in enumerate(densities):
            ax = axes[rew_idx, dens_idx]
            
            cell_data = subset[
                (subset['rew'] == rew) &
                (subset['density'] == density)
            ]
            
            if len(cell_data) < 10:
                ax.text(0.5, 0.5, 'Insufficient\ndata',
                       ha='center', va='center', transform=ax.transAxes)
                ax.set_title(f'$k={rew}$, $\\rho={density}$', fontsize=11, weight='bold')
                ax.axis('off')
                continue
            
            # Compute Mann-Whitney p-values
            p_values = calculate_pairwise_mann_whitney(cell_data, methods)
            
            # Plot
            heatmap = plot_mann_whitney_heatmap(ax, p_values, f'$k={rew}$, $\\rho={density}$')
            if heatmap_obj is None:
                heatmap_obj = heatmap
            
            if dens_idx > 0:
                ax.set_yticklabels([])
    
    fig.suptitle(
        f'Pairwise Mann-Whitney U Test: LRM Networks (n={n_nodes})\n'
        f'(Stratified by Rewiring Level and Density)',
        fontsize=16, weight='bold'
    )
    
    if heatmap_obj is not None:
        cbar = fig.colorbar(
            heatmap_obj.collections[0],
            ax=axes,
            orientation='vertical',
            fraction=0.02,
            pad=0.02,
            aspect=30,
            ticks=[0, 1, 2, 3],
            boundaries=[-0.5, 0.5, 1.5, 2.5, 3.5]
        )
        
        cbar.set_label('Significance', fontsize=12)
        cbar.set_ticklabels([r'$p \geq 0.05$', r'$p < 0.001$', r'$p < 0.01$', r'$p < 0.05$'])
        cbar.outline.set_linewidth(1)
        cbar.outline.set_edgecolor('0.5')
        cbar.ax.tick_params(size=0)
    
    save_figure(fig, f'{save_name}-lrm-mann-whitney')
    plt.show()


create_stratified_mann_whitney_heatmaps_lrm(stat_data, 'statistical-stratified')
```

### Insights from Stratified Analysis

Stratified statistical visuals: critical difference diagrams and heatmaps are generated per slice (size, density, rewiring, temperature). Purpose: show how rankings and pairwise test outcomes vary when holding one parameter constant. How to read: bars connecting methods in CD diagrams mean no significant rank difference (Nemenyi). Separate clusters indicate rank separation. Heatmaps encode p-values and effect sizes for the same slices.

## Summary Tables

```{python}
#| label: tbl-statistical-summary
#| tbl-cap: "Summary of statistical significance and effect sizes for all method comparisons across graph types. Significant differences (p < 0.05, Bonferroni corrected) are marked with asterisks. Effect sizes: S=small, M=medium, L=large."

def create_summary_table(graph_type):
    """Create a summary table combining p-values and effect sizes."""
    # Check if data exists for this graph type
    if graph_type not in pairwise_results or graph_type not in effect_size_results:
        return None
    
    pval_df = pairwise_results[graph_type]
    effect_df = effect_size_results[graph_type]
    
    # Merge results
    merged = pval_df.merge(
        effect_df[['Method 1', 'Method 2', 'Cohen\'s D', 'Magnitude']],
        on=['Method 1', 'Method 2']
    )
    
    # Format for display
    merged['Comparison'] = merged['Method 1'] + ' vs ' + merged['Method 2']
    merged['Sig'] = merged['Significant'].map({True: '***', False: ''})
    merged['Effect'] = merged['Magnitude'].map({
        'Negligible': '', 'Small': 'S', 'Medium': 'M', 'Large': 'L'
    })
    
    summary = merged[[
        'Comparison', 'Mean Diff', 'p-adjusted', 'Sig', 
        'Cohen\'s D', 'Effect'
    ]].copy()
    
    summary.columns = ['Comparison', 'Mean Diff', 'p-value', 'Sig.', 'Cohen\'s D', 'Effect']
    
    return summary

# Create and display summary tables
for graph_type in graph_types:
    # Skip if no data available
    if graph_type not in pairwise_results or graph_type not in effect_size_results:
        print(f"\n{'='*80}")
        print(f"Statistical Summary: {graph_type} Networks")
        print('='*80)
        print(f"No data available for {graph_type} networks - skipping.")
        continue
    
    print(f"\n{'='*80}")
    print(f"Statistical Summary: {graph_type} Networks")
    print('='*80)
    
    summary = create_summary_table(graph_type)
    
    if summary is None or summary.empty:
        print("No summary data available.")
        continue
    
    # Save to LaTeX
    save_table(
        summary,
        f'statistical-summary-{graph_type.lower()}',
        caption=f"Statistical significance and effect sizes for method comparisons on {graph_type} networks. "
                f"Sig.: *** = p < 0.05 (Bonferroni corrected). Effect: S=small (|d|≥0.2), M=medium (|d|≥0.5), L=large (|d|≥0.8).",
        show_index=False
    )
    
    # Display top results
    sig_summary = summary[summary['Sig.'] == '***'].sort_values('p-value')
    if not sig_summary.empty:
        print(f"\nSignificant comparisons with effect sizes:")
        print(sig_summary.to_string(index=False))
    else:
        print("\nNo significant comparisons found.")
```

## Performance Rankings Summary

```{python}
#| label: tbl-performance-rankings
#| tbl-cap: "Summary of method performance rankings across graph types based on Friedman-Nemenyi analysis. Mean ranks are shown with lower values indicating better performance. Methods are ordered by average rank across all graph types."

# Create ranking summary table
ranking_summary = []

for graph_type in graph_types:
    if graph_type in ranking_results:
        result = ranking_results[graph_type]
        avg_ranks = result['mean_ranks']
        
        for method, rank in avg_ranks.items():
            ranking_summary.append({
                'Graph Type': graph_type,
                'Method': method,
                'Mean Rank': rank,
                'Friedman p-value': result['friedman_pval']
            })

ranking_df = pd.DataFrame(ranking_summary)

# Pivot for better display
ranking_pivot = ranking_df.pivot(
    index='Method',
    columns='Graph Type',
    values='Mean Rank'
)

# Add overall average
ranking_pivot['Average'] = ranking_pivot.mean(axis=1)
ranking_pivot = ranking_pivot.sort_values('Average')

# Format and save
ranking_pivot_formatted = ranking_pivot.round(2)

save_table(
    ranking_pivot_formatted,
    'performance-rankings',
    caption="Mean performance ranks across graph types (lower is better). "
            "Average column shows the mean rank across all graph types. "
            "All Friedman tests show p < 0.001, indicating significant differences in method performance.",
    show_index=True
)

print("\nPerformance Rankings Summary")
print("="*60)
print(ranking_pivot_formatted)
```

# Composite Figures for Individual Graph Types

This section generates comprehensive composite figures for each graph type, combining:
1. Symmetry comparison across methods
2. Pairwise statistical tests (p-values and Cohen's D)
3. Critical difference diagrams

```{python}
#| label: composite-figure-functions
#| code-summary: "Define composite figure creation functions"

import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
import seaborn as sns
import pandas as pd
import numpy as np
import scipy.stats as stats
import scikit_posthocs as sp
import pingouin as pg
from mpl_toolkits.axes_grid1 import make_axes_locatable

def calculate_pairwise_stats_composite(data, value_col='symmetry', method_col='method_display'):
    """
    Calculate pairwise Mann-Whitney U / Wilcoxon p-values and Cohen's D effect sizes.
    Uses Wilcoxon signed-rank test for paired data, Mann-Whitney U for unpaired.
    """
    methods = sorted(data[method_col].unique())
    n_methods = len(methods)
    p_values = pd.DataFrame(index=methods, columns=methods, dtype=float)
    cohen_d_matrix = pd.DataFrame(index=methods, columns=methods, dtype=float)
    
    for m1 in methods:
        for m2 in methods:
            if m1 == m2:
                p_values.at[m1, m2] = np.nan
                cohen_d_matrix.at[m1, m2] = np.nan
                continue
            
            d1 = data[data[method_col] == m1][value_col]
            d2 = data[data[method_col] == m2][value_col]
            
            if len(d1) < 2 or len(d2) < 2:
                p_values.at[m1, m2] = np.nan
                cohen_d_matrix.at[m1, m2] = np.nan
                continue

            # Try to pair by instance_id if possible
            common_instances = np.intersect1d(
                data[data[method_col] == m1]['instance_id'],
                data[data[method_col] == m2]['instance_id']
            )
            
            if len(common_instances) < 2:
                # Fallback to independent Mann-Whitney U test
                u_stat, p_val = stats.mannwhitneyu(d1, d2, alternative='two-sided')
                d = pg.compute_effsize(d1, d2, eftype='cohen')
            else:
                # Paired Wilcoxon signed-rank test
                d1_paired = data[(data[method_col] == m1) & (data['instance_id'].isin(common_instances))].set_index('instance_id').loc[common_instances][value_col]
                d2_paired = data[(data[method_col] == m2) & (data['instance_id'].isin(common_instances))].set_index('instance_id').loc[common_instances][value_col]
                
                w_stat, p_val = stats.wilcoxon(d1_paired, d2_paired)
                d = pg.compute_effsize(d1_paired, d2_paired, eftype='cohen', paired=True)

            p_values.at[m1, m2] = p_val
            cohen_d_matrix.at[m1, m2] = d
            
    return p_values, cohen_d_matrix

def create_composite_figure(
    df, 
    graph_type, 
    n_nodes=None, 
    density=None, 
    rew=None, 
    method_colors=None,
    figsize=(14, 8)
):
    """
    Create a composite figure for a specific graph type and parameters.
    
    Parameters
    ----------
    df : pd.DataFrame
        The dataframe containing the results.
    graph_type : str
        The graph type to filter by (e.g., 'BA', 'ER', 'LRM').
    n_nodes : int, optional
        Filter by number of nodes.
    density : float or str, optional
        Filter by density.
    rew : int, optional
        Filter by rewiring (for LRM).
    method_colors : dict, optional
        Dictionary mapping method names to colors.
    figsize : tuple, optional
        Figure size.
    """
    
    # Filter data
    subset = df[df['graph_type'] == graph_type].copy()
    
    # Ensure method_display exists
    if 'method_display' not in subset.columns:
        subset['method_display'] = subset['method']

    # Ensure instance_id exists for pairing
    if 'instance_id' not in subset.columns:
        if 'sim_id' in subset.columns:
             subset['instance_id'] = subset['sim_id']
        else:
             subset['instance_id'] = subset.index

    title_parts = [f"Graph Type: {graph_type}"]
    
    if n_nodes is not None:
        subset = subset[subset['n_nodes'] == n_nodes]
        title_parts.append(rf"$n={n_nodes}$")
    
    if density is not None:
        # Handle density as string or float
        if isinstance(density, str):
             subset = subset[subset['density'] == density]
        else:
             # Try matching float or string representation
             subset = subset[np.isclose(subset['density'].astype(float), float(density))]
        title_parts.append(rf"$\rho={density}$")
        
    if rew is not None:
        subset = subset[subset['rew'] == rew]
        title_parts.append(rf"$k={rew}$")
        
    if subset.empty:
        print("No data found for the specified parameters.")
        return None

    # Setup figure with constrained layout
    # Use a context manager to set the style locally
    #with sns.axes_style("white"):
    fig = plt.figure(figsize=figsize, constrained_layout=True)
    
    # Set constrained layout padding
    fig.set_constrained_layout_pads(w_pad=0.05, h_pad=0.05, hspace=0.05, wspace=0.05)
    
    # GridSpec: 1 row, 2 columns. 
    # Column 0: Left side (Symmetry Boxplot)
    # Column 1: Right side (Heatmaps and CD Diagram)
    gs = gridspec.GridSpec(1, 2, figure=fig, width_ratios=[1, 2])
    
    # 1. Symmetry Comparison (Boxplot) - Left Side
    ax_box = fig.add_subplot(gs[0])
    
    # Use deep palette if method_colors is not provided
    if method_colors is None:
        method_colors = sns.color_palette("deep")
    
    method_order = ['QSA', 'MF', 'IP', 'DR', 'OR']
    
    sns.boxplot(
        data=subset,
        x='method_display',
        y='symmetry',
        hue='method_display',
        order=method_order,
        hue_order=method_order,
        palette=method_colors,
        ax=ax_box,
        legend=False,
        linewidth=1.2
    )
    #sns.despine(ax=ax_box, trim=True) # Remove top and right spines
    #ax_box.grid(axis='y', linestyle=':', alpha=0.5) # Minimal grid
    
    # Add panel label A
    ax_box.text(-0.15, 1.05, 'A', transform=ax_box.transAxes, 
                fontsize=24, weight='bold', va='top', ha='right')
    
    ax_box.set_title("Symmetry Comparison", fontsize=16, weight='bold', pad=15)
    ax_box.set_xlabel("", fontsize=14)
    ax_box.set_ylabel("Symmetry Coefficient", fontsize=14)
    
    # Add legend with automatic best location
    handles = [plt.matplotlib.patches.Patch(color=method_colors.get(m, 'gray'), label=m) 
               for m in method_order if m in subset['method_display'].values]
    ax_box.legend(handles=handles, loc='best', fontsize=12, frameon=True, 
                  fancybox=False, shadow=False, framealpha=0.9)
    
    # Right Side: Nested GridSpec
    gs_right = gs[1].subgridspec(2, 1, height_ratios=[1, 1])
    
    # 2. Pairwise Statistics (Heatmaps) - Right Top
    # Add more space between the two heatmaps to accommodate colorbars
    gs_right_top = gs_right[0].subgridspec(1, 2, wspace=0.15)
    ax_pval = fig.add_subplot(gs_right_top[0])
    ax_cohen = fig.add_subplot(gs_right_top[1])
    
    p_values, cohen_d_matrix = calculate_pairwise_stats_composite(subset)
    
    # Reorder heatmaps to match method_order
    available_methods = [m for m in method_order if m in p_values.index]
    p_values = p_values.loc[available_methods, available_methods]
    cohen_d_matrix = cohen_d_matrix.loc[available_methods, available_methods]
    
    # Create mask for diagonal
    mask = np.eye(len(p_values), dtype=bool)
    
    # P-values Sign Plot
    # Create sign plot manually with seaborn heatmap + custom colorbar
    from matplotlib.colors import ListedColormap, NoNorm
    from matplotlib.colorbar import ColorbarBase
    import matplotlib.colors as colors
    
    # Generate cubehelix colors for different significance levels
    cubehelix_colors = sns.cubehelix_palette(n_colors=5, start=0.5, rot=-0.75, light=0.85, dark=0.15)
    cmap_list = [
        "1",  # diagonal (white)
        cubehelix_colors[0].hex() if hasattr(cubehelix_colors[0], 'hex') else '#fbd7d4',  # non-significant
        cubehelix_colors[4].hex() if hasattr(cubehelix_colors[4], 'hex') else '#005a32',  # p < 0.001
        cubehelix_colors[3].hex() if hasattr(cubehelix_colors[3], 'hex') else '#238b45',  # p < 0.01
        cubehelix_colors[2].hex() if hasattr(cubehelix_colors[2], 'hex') else '#a1d99b',  # p < 0.05
    ]
    
    # Transform p-values to categorical bins like sign_plot does
    p_values_cat = p_values.copy()
    xc = p_values.values.copy()
    p_values_cat[(xc < 0.001) & (xc >= 0)] = 1
    p_values_cat[(xc < 0.01) & (xc >= 0.001)] = 2
    p_values_cat[(xc < 0.05) & (xc >= 0.01)] = 3
    p_values_cat[(xc >= 0.05)] = 0
    np.fill_diagonal(p_values_cat.values, -1)
    
    # Create the heatmap without colorbar first
    # Heatmap data uses full range (-1 to 3), but colorbar will show only 0 to 3
    heatmap_pval = sns.heatmap(
        p_values_cat,
        cmap=ListedColormap(cmap_list),
        vmin=-1,
        vmax=3,
        center=1,
        cbar=False,  # Create colorbar manually for proper sizing
        ax=ax_pval,
        linewidths=0.5,
        linecolor='0',
        clip_on=False,
        square=True
    )
    
    # Create colorbar with same height as the heatmap
    divider_pval = make_axes_locatable(ax_pval)
    cax_pval = divider_pval.append_axes("right", size="5%", pad=0.1)
    cbar = plt.colorbar(
        heatmap_pval.collections[0],
        cax=cax_pval,
        ticks=[0, 1, 2, 3],
        boundaries=[-0.5, 0.5, 1.5, 2.5, 3.5]
    )
    
    # Customize the colorbar to show significance levels
    # Each tick is centered in its color band: 0=NS, 1=p<0.001, 2=p<0.01, 3=p<0.05
    cbar.set_label('Significance', fontsize=12)
    cbar.set_ticklabels([r'$p \geq 0.05$', r'$p < 0.001$', r'$p < 0.01$', r'$p < 0.05$'])
    cbar.outline.set_linewidth(1)
    cbar.outline.set_edgecolor('0.5')
    cbar.ax.tick_params(size=0)
    
    # Add panel label B
    ax_pval.text(-0.25, 1.05, 'B', transform=ax_pval.transAxes, 
                 fontsize=24, weight='bold', va='top', ha='right')
    
    ax_pval.set_title("Pairwise Mann-Whitney U Test", fontsize=14, weight='bold')
    
    # Cohen's D Heatmap
    # Use a muted diverging palette with symmetric range
    # Calculate symmetric range around 0
    max_abs_d = max(abs(cohen_d_matrix.min().min()), abs(cohen_d_matrix.max().max()))
    heatmap_cohen = sns.heatmap(
        cohen_d_matrix,
        annot=True,
        fmt=".2f",
        cmap='RdBu_r',
        center=0,
        vmin=-max_abs_d,
        vmax=max_abs_d,
        mask=mask,
        ax=ax_cohen,
        cbar=False,  # Create colorbar manually for proper sizing
        linewidths=0.5,
        linecolor='0',
        clip_on=False,
        square=True
    )
    
    # Create colorbar with same height as the heatmap
    divider_cohen = make_axes_locatable(ax_cohen)
    cax_cohen = divider_cohen.append_axes("right", size="5%", pad=0.1)
    cbar_cohen = plt.colorbar(
        heatmap_cohen.collections[0],
        cax=cax_cohen
    )
    cbar_cohen.set_label("Cohen's D", fontsize=12)
    
    ax_cohen.set_title("Pairwise Cohen's D", fontsize=14, weight='bold')
    #ax_cohen.tick_params(labelsize=12)

    # 3. Confidence Diagram (CD Diagram) - Right Bottom
    ax_cd = fig.add_subplot(gs_right[1])
    
    # Prepare data for CD diagram
    pivot_data = subset.pivot_table(
        index='instance_id',
        columns='method_display',
        values='symmetry'
    ).dropna()
    
    if len(pivot_data) >= 2 and pivot_data.shape[1] >= 3:
        # Friedman test
        friedman_stat, friedman_pval = stats.friedmanchisquare(*pivot_data.T.values)
        
        if friedman_pval < 0.05:
            # Nemenyi test
            posthoc_matrix = sp.posthoc_nemenyi_friedman(pivot_data.values)
            posthoc_matrix.index = pivot_data.columns
            posthoc_matrix.columns = pivot_data.columns
            
            # Average ranks
            avg_ranks = pivot_data.rank(axis=1).mean(axis=0)
            
            # Calculate critical difference
            n_methods = len(avg_ranks)
            n_datasets = len(pivot_data)
            # Critical difference at alpha=0.05 using Nemenyi test
            # CD = q_alpha * sqrt(k(k+1)/(6N))
            # q_alpha for Nemenyi is approximately 2.569 for k=5, alpha=0.05
            from scipy.stats import studentized_range
            q_alpha = studentized_range.ppf(0.95, n_methods, np.inf) / np.sqrt(2)
            cd = q_alpha * np.sqrt(n_methods * (n_methods + 1) / (6 * n_datasets))
            
            # Plot
            # We need to handle the color palette mapping for CD diagram
            # sp.critical_difference_diagram expects a dict or list
            # If method_colors is provided, we filter it for available methods
            cd_colors = None
            if isinstance(method_colors, dict):
                cd_colors = {m: method_colors.get(m, 'gray') for m in avg_ranks.index}
            elif isinstance(method_colors, (list, np.ndarray)):
                    # If list, map to methods in order (assuming consistent ordering)
                    # This is risky if order differs, but best effort
                    unique_methods = sorted(subset['method_display'].unique())
                    cd_colors = {m: method_colors[i % len(method_colors)] for i, m in enumerate(unique_methods) if m in avg_ranks.index}

            sp.critical_difference_diagram(
                ranks=avg_ranks,
                sig_matrix=posthoc_matrix,
                color_palette=cd_colors,
                ax=ax_cd,
                label_fmt_left='{label} [{rank:.2f}]  ',
                label_fmt_right='  [{rank:.2f}] {label}',
                text_h_margin=0.3,
                crossbar_props={'linewidth': 4},
                elbow_props={'linewidth': 4},
                label_props={'fontsize': 18},
                marker_props={'s': 100}
            )
            
            # Add panel label C
            ax_cd.text(-0.13, 1.05, 'C', transform=ax_cd.transAxes, 
                       fontsize=24, weight='bold', va='top', ha='right')
            
            # Enhanced title with statistical info
            cd_title = (f"Critical Difference Diagram\n"
                       f"Friedman: $p$={friedman_pval:.2e}; "
                       f"CD={cd:.3f}")
            ax_cd.set_title(cd_title, fontsize=16, weight='bold', pad=15)
        else:
            ax_cd.text(0.5, 0.5, f"Friedman test not significant\n(p={friedman_pval:.2e})", 
                        ha='center', va='center', transform=ax_cd.transAxes)
            ax_cd.axis('off')
    else:
        ax_cd.text(0.5, 0.5, "Insufficient data for CD Diagram", 
                    ha='center', va='center', transform=ax_cd.transAxes, fontsize=14)
        ax_cd.axis('off')
    
    fig.suptitle(", ".join(title_parts), fontsize=20, weight='bold', y=1.05)
    
    return fig
```

## Generate Composite Figures for All Combinations

```{python}
#| label: fig-composite-all-combinations
#| fig-cap: "Comprehensive performance comparison of optimization methods across graph instances. **A. Symmetry Comparison**: Box plots showing the distribution of symmetry coefficients (S) for each optimization method (QSA=Quadratic Symmetry Approximator, MF=Manifold, IP=Interior Point, DR=Dimensionality Reduction, OR=Orthogonal Relaxation) across multiple independent graph instances. Lower symmetry values indicate better performance (closer to non-trivial permutation that preserves graph structure). Box shows median and interquartile range (IQR), whiskers extend to 1.5×IQR, and outliers are displayed as individual points. **B. Pairwise Statistical Comparison**: Left heatmap displays Mann-Whitney U test results indicating statistical significance of performance differences between method pairs (darker colors = stronger evidence of difference; p < 0.001 darkest, p ≥ 0.05 lightest). Right heatmap shows Cohen's D effect sizes quantifying the magnitude of differences (red = row method worse than column method, blue = row method better; |D| > 0.8 indicates large effect). Read by selecting row method and comparing across columns. **C. Critical Difference Diagram**: Methods connected by horizontal bars show no statistically significant difference (Nemenyi post-hoc test at α=0.05 following Friedman test). Numbers in brackets show average ranks (lower is better). Critical Difference (CD) value indicates minimum rank difference required for statistical significance. Friedman p-value tests the null hypothesis that all methods perform equivalently."
#| fig-width: 16
#| fig-height: 10

# Get all available combinations from the data
standard_graph_types = STANDARD_GRAPH_TYPES

# Get unique parameter combinations for standard graph types
standard_combinations = unified_best[unified_best['graph_type'].isin(standard_graph_types)][
    ['graph_type', 'n_nodes', 'density']
].drop_duplicates().sort_values(['graph_type', 'n_nodes', 'density'])

print(f"Generating composite figures for {len(standard_combinations)} standard graph type combinations...")

for idx, row in standard_combinations.iterrows():
    graph_type = row['graph_type']
    n_nodes = row['n_nodes']
    density = row['density']
    
    fig = create_composite_figure(
        unified_best,
        graph_type=graph_type,
        n_nodes=n_nodes,
        density=density,
        method_colors={m: METHOD_COLORS[k] for k, m in METHOD_NAMES.items()}
    )
    
    if fig:
        # Format density for filename (e.g., 0.40 -> 040, 0.15 -> 015)
        density_str = f"{int(float(density)*100):03d}"
        filename = f"composite-{graph_type.lower()}-n{n_nodes}-d{density_str}"
        save_figure(fig, filename)
        plt.show()
        print(f"[OK] Generated: {filename}")
    else:
        print(f"[SKIP] {graph_type} n={n_nodes} density={density}: no data available")

print("\nGenerating composite figures for LRM networks...")

# Get unique parameter combinations for LRM
lrm_combinations = unified_best[unified_best['graph_type'] == 'LRM'][
    ['n_nodes', 'density', 'rew']
].drop_duplicates().sort_values(['n_nodes', 'density', 'rew'])

print(f"Found {len(lrm_combinations)} LRM combinations...")

for idx, row in lrm_combinations.iterrows():
    n_nodes = row['n_nodes']
    density = row['density']
    rew = row['rew']
    
    fig = create_composite_figure(
        unified_best,
        graph_type='LRM',
        n_nodes=n_nodes,
        density=density,
        rew=rew,
        method_colors={m: METHOD_COLORS[k] for k, m in METHOD_NAMES.items()}
    )
    
    if fig:
        density_str = f"{int(float(density)*100):d}"
        filename = f"composite-lrm-n{n_nodes}-d{density_str}-k{int(rew)}"
        save_figure(fig, filename)
        plt.show()
        print(f"[OK] Generated: {filename}")
    else:
        print(f"[SKIP] LRM n={n_nodes} density={density} k={rew}: no data available")

print("\n[DONE] All composite figures generated!")
```