### OT4P
[SUBTITLE: Moving into a better behaved space]

This research introduces OT4P, a novel method for relaxing permutation matrices onto the orthogonal group, offering potential benefits over existing techniques which often rely on the Birkhoff polytope. OT4P employs a temperature-controlled differentiable transformation that maps unconstrained vector space to the orthogonal group, where temperature influences the proximity of the resulting matrices to permutation matrices. This allows for gradient-based optimization of permutation-related problems and, with a re-parameterization trick, facilitates efficient stochastic optimization over latent permutations. The paper presents experimental results demonstrating OT4P's effectiveness in various tasks, including finding mode connectivity in neural networks, inferring neuron identities, and solving permutation synchronization problems.

- this research allows us to explore a qualitatively new relaxation of the space of permutation matrices. While until now we have focused on the relaxation of the space to the convex hull of permutation matrices -- the birkhoff polytope, we can use another property of the permutation matrices and that is that they are orthogonal matrices. the Ot4p paper opens up a path to exploration of the space through the relaxation to permutation matrices

- exploring relaxation methods within the orthogonal group


> ## Overview of OT4P: A Gentle Exposition

 <!-- - [ ] Present results of the OT4P paper -->


 <!-- - [ ] Core Idea -->

> ### Core Idea
> OT4P relaxes the discrete permutation matrix optimization problem by embedding permutation matrices into the continuous space of orthogonal matrices, which can then be parameterized using unconstrained vectors. This enables gradient-based optimization while maintaining key geometric properties.
> 

 <!-- - [ ] Definitions
    - [ ] Skew-Symmetric Matrices
    - [ ] Matrix Exponential, Matrix logarithm
    - [ ] Lie group
    - [ ] Special Orthogonal, Orthogonal Group -->

> **1.2 Skew-Symmetrization**
> - Transform A to skew-symmetric: S = A - A^T
> - This creates S âˆˆ so(n), the Lie algebra of SO(n)
> - Key property: S^T = -S
 
> ## Lie Groups: Definition and Properties
> 
> **Definition:** A **Lie group** G is a set that is simultaneously:
> 1. A **smooth manifold** (has coordinates, calculus works)
> 2. A **group** (has multiplication, identity, inverses)
> 3. The group operations are **smooth** (differentiable)
> 
> **Key properties:**
> - **Tangent space at identity**: Called the **Lie algebra** ð”¤
> - **Exponential map**: exp: ð”¤ â†’ G (connects algebra to group)
> - **Logarithm map**: log: G â†’ ð”¤ (inverse of exponential, locally)
> 
> **Examples:**
> - â„â¿ under addition
> - SÂ¹ = {e^{iÎ¸} : Î¸ âˆˆ â„} under complex multiplication  
> - GL(n) = invertible nÃ—n matrices under matrix multiplication
> - **SO(n)** = special orthogonal group (our main focus)
> 

> ## Special Orthogonal Group SO(n)
> 
> **Definition:**
> ```
> SO(n) = {Q âˆˆ â„â¿Ë£â¿ : Qáµ€Q = I, det(Q) = +1}
> ```
> 
> **What this means:**
> - **Orthogonal**: Qáµ€Q = I (columns are orthonormal)
> - **Special**: det(Q) = +1 (orientation-preserving)
> - **Geometric interpretation**: Rotations in n-dimensional space
> 
> **Properties:**
> - **Compact manifold** of dimension n(n-1)/2
> - **Connected** (for n â‰¥ 2): can rotate from any orientation to any other
> - **Matrix group**: inherits matrix operations
> 
> **Examples:**
> - **SO(2)**: 2D rotations, isomorphic to SÂ¹
>   ```
>   [cos Î¸  -sin Î¸]
>   [sin Î¸   cos Î¸]
>   ```
> - **SO(3)**: 3D rotations (orientation of rigid bodies)
> 

> ## The Lie Algebra so(n)
> 
> **Definition:** The tangent space at the identity:
> ```
> so(n) = {A âˆˆ â„â¿Ë£â¿ : Aáµ€ = -A}
> ```
> 

 <!-- - [ ] Two-Step Framework -->

<!-- - [ ] The two step framework summary -->

> ### The Two-Step Framework
> 
> **Step 1: Vector â†’ Orthogonal Matrix**
> - Start with an unconstrained vector in â„^(n(n-1)/2) 
> - Transform it to a skew-symmetric matrix A (by computing A - A^T)
> - Apply matrix exponential to get an orthogonal matrix O = exp(A - A^T)
> 
> **Step 2: Moving Towards Permutations**
> - Find the closest permutation matrix P to O using the Hungarian algorithm
> - Move O towards P along a geodesic path, controlled by temperature Ï„
> - Result: Ã• = P(P^T O)^Ï„, which approaches P as Ï„ â†’ 0

> You're off to a strong start in understanding the OT4P (Orthogonal Group-based Transformation for Permutation Relaxation) framework! Your understanding that OT4P considers permutations as being embedded in the Special Orthogonal Manifold $SO(n)$ is correct. You've also correctly identified that $SO(n)$ is a Lie group that possesses an exponential map, making it a "well-behaved" space for optimization. This underlying structure allows for the design of gradient-based optimization algorithms for problems involving permutations
> .
> Now, let's address your core question: how do you move closer to permutation matrices within this framework?
> The OT4P algorithm, proposed by Guo et al.
> , operates in two main steps to achieve this, which are summarized in Algorithm 1 of the paper
> :
> The OT4P Framework: Two Key Steps
> Step I: Mapping to an Orthogonal Matrix
> The first step is to map an unconstrained vector space $R^{n(n-1)/2}$ to the special orthogonal group $SO(n)$. This is done through a composition of two mappings:
> 1.
> From vector to skew-symmetric matrix: A vector a from the $R^{n(n-1)/2}$ space is transformed into a skew-symmetric matrix A (i.e., $A^T = -A$)
> . This skew-symmetric matrix A belongs to the Lie algebra $so(n)$
> .
> 2.
> From skew-symmetric to orthogonal matrix: The matrix exponential (Lie exponential) expm(Â·) is then applied to this skew-symmetric matrix $A - A^T$ to map it into the special orthogonal group $SO(n)$
> .
> â—¦
> This combined mapping is denoted as $\phi: R^{n(n-1)/2} \to SO(n)$, where $\phi(A) = \text{expm}(A - A^T)$
> .
> â—¦
> The mapping $\phi(Â·)$ is differentiable and surjective, meaning every orthogonal matrix in $SO(n)$ can be reached. It's also injective on a specific domain U
> .
> â—¦

<!-- - [ ] Step 1 -->

> ### Step 1: Vector â†’ Orthogonal Matrix (Detailed)
> 
> The first step creates a differentiable map from unconstrained vector space to SO(n):
> 
> **1.1 Vector Representation**
> - Start with a vector **a** âˆˆ â„^(n(n-1)/2)
> - This vector fills the upper triangular part of an nÃ—n matrix A (with zeros on diagonal)
> - Example for n=3: [aâ‚, aâ‚‚, aâ‚ƒ] â†’ A = [[0, aâ‚, aâ‚‚], [0, 0, aâ‚ƒ], [0, 0, 0]]
> 
> **1.2 Skew-Symmetrization**
> - Transform A to skew-symmetric: S = A - A^T
> - This creates S âˆˆ so(n), the Lie algebra of SO(n)
> - Key property: S^T = -S
> 
> **1.3 Matrix Exponential**
> - Apply matrix exponential: O = exp(S) = I + Î£(S^k/k!)
> - This maps so(n) â†’ SO(n)
> - Result: O^T O = I and det(O) = +1
> 
> **Mathematical Properties:**
> - The mapping Ï•: â„^(n(n-1)/2) â†’ SO(n) defined by Ï•(A) = exp(A - A^T) is:
>   - Differentiable everywhere
>   - Surjective (onto)
>   - Injective on domain U = {A | Im(Î»â‚–(A - A^T)) âˆˆ (-Ï€, Ï€)}
> 

> **2. Skew-Symmetric Transformation Details**
> 
> The mapping A â†’ A - A^T is carefully chosen:
> - **Completeness**: Every skew-symmetric S can be written as S = B - B^T for some B
>   - Proof: Take B = S/2, then B - B^T = S/2 - S^T/2 = S/2 + S/2 = S
> - **Dimension matching**: Upper triangular has n(n-1)/2 free parameters, same as so(n)
> - **Isomorphism**: This creates a vector space isomorphism between â„^(n(n-1)/2) and so(n)
> 

<!-- - [ ] Step 2 -->

> ### Step 2: Moving Towards Permutations (Detailed)
> 
> **2.1 Finding Closest Permutation**
> - Given O âˆˆ SO(n), solve: P* = argmax_{PâˆˆPâ‚™} âŸ¨P, OâŸ©_F
> - This is a linear assignment problem solved by Hungarian algorithm
> - Cost: O(nÂ³)
> 
> **2.2 Geodesic Interpolation**
> The key innovation is moving O towards P along the geodesic:
> 
> 1. **Push to neighborhood of I**: Compute P^T O (this is near I when O is near P)
> 2. **Take logarithm**: log(P^T O) maps to tangent space at I
> 3. **Scale by Ï„**: Ï„Â·log(P^T O) 
> 4. **Exponential map**: exp(Ï„Â·log(P^T O)) = (P^T O)^Ï„
> 5. **Pull back to P**: Ã• = P(P^T O)^Ï„
> 
> **Why this works:**
> - When O â‰ˆ P, then P^T O â‰ˆ I
> - The power operation (Â·)^Ï„ naturally interpolates eigenvalues
> - As Ï„ â†’ 0, Ã• â†’ P
> 

> **1. Special Orthogonal vs Orthogonal Group**
> 
> The paper primarily works in SO(n) but must handle O(n) for completeness:
> - **Even permutations**: det(P) = +1, so P âˆˆ SO(n) directly
> - **Odd permutations**: det(P) = -1, so P âˆˆ O(n) \ SO(n)
> 
> For odd permutations, they use a clever trick:
> - Find agent PÌƒ = PD where D = diag(1,...,1,-1)
> - Now PÌƒ âˆˆ SO(n) and âˆ¥PÌƒ - Pâˆ¥_F is minimized
> - Work with PÌƒ in SO(n), then multiply by D^T at the end
> 
> **3. Matrix Exponential (Lie Exponential) Deep Dive**
> 
> The exponential map exp: so(n) â†’ SO(n) has profound geometric meaning:
> 
> - **Geometric interpretation**: It maps straight lines through 0 in so(n) to geodesics through I in SO(n)
> - **Local diffeomorphism**: Near 0, it's a bijection to a neighborhood of I
> - **Not globally bijective**: Multiple elements in so(n) can map to same O âˆˆ SO(n)
>   - Example: Rotations by Î¸ and Î¸ + 2Ï€ are the same
> 
> The Lie algebra so(n) is indeed the tangent space T_I SO(n) at identity. The exponential map:
> - Takes tangent vectors at I
> - Flows along geodesics for unit time
> - Lands on the manifold SO(n)
> 
> **4. Why Ï•(A) = exp(A - A^T) Instead of Just exp(A)**
> 
> This is subtle but crucial:
> - exp(A) only gives SO(n) if A is skew-symmetric
> - Starting with arbitrary A âˆˆ â„^(nÃ—n) won't guarantee orthogonality
> - The operation A - A^T:
>   - Guarantees skew-symmetry: (A - A^T)^T = A^T - A = -(A - A^T)
>   - Maintains n(n-1)/2 degrees of freedom
>   - Creates a clean parameterization
> 

> **7. Geodesic Movement - The Deep Geometry**
> 
> This is the most sophisticated part. Let me break it down:
> 
> **Why P^T O is near I when O is near P:**
> - If O = P + Îµ (small perturbation), then P^T O = P^T(P + Îµ) = I + P^T Îµ
> - Since P is orthogonal, âˆ¥P^T Îµâˆ¥ = âˆ¥Îµâˆ¥, so P^T O is I plus small perturbation
> 
> **How (Â·)^Ï„ moves along geodesics:**
> - For orthogonal matrix M near I, M^Ï„ interpolates eigenvalues
> - If M = QÎ›Q^T (eigendecomposition), then M^Ï„ = QÎ›^Ï„ Q^T
> - This traces a geodesic from I (when Ï„=0) to M (when Ï„=1)
> 
> **The shifting mechanism:**
> - Working near I is computationally convenient (logarithm converges)
> - Left multiplication by P is an isometry in SO(n) with bi-invariant metric
> - So PÂ·(geodesic from I to P^T O) = geodesic from P to O
> 
> **In manifold optimization context:**
> - This is geodesic interpolation on SO(n)
> - Temperature Ï„ controls position along geodesic
> - As Ï„ â†’ 0, we approach the endpoint (permutation matrix)
> 

> Step II: Moving Closer to Permutation Matrices (The Core of Your Question)
> Once you have an orthogonal matrix $O \in SO(n)$ (obtained from Step I), the second crucial step is to move this $O$ towards the closest permutation matrix $P$ along the geodesic, controlled by a temperature parameter $\tau$
> . Here's how it works:
> 1.
> Finding the Closest Permutation Matrix: The first sub-step is to identify the permutation matrix $P \in P_n$ that is closest to the current orthogonal matrix $O$
> . This is formulated as a maximization problem: $\rho(O) := \text{argmax}_{P \in P_n} \langle P, O \rangle_F$
> .
> â—¦
> This is a linear assignment problem, which can be efficiently solved using the Hungarian algorithm in cubic time
> . The paper explains how to handle potential negative values in $O$ for this algorithm by subtracting the minimum element
> .
> 2.
> Interpolation Towards $P$ Along the Geodesic: Once the closest permutation matrix $P$ is identified, the core idea is to interpolate the orthogonal matrix $O$ towards $P$
> . This interpolation is performed in a way that respects the manifold's geometry:
> â—¦
> The paper utilizes the logarithm map at $P$ ($\text{logm}_P$) to map orthogonal matrices near $P$ to the tangent space $T_P SO(n)$
> . Similarly, an exponential map at $P$ ($\text{expm}_P$) maps elements from $T_P SO(n)$ back onto $SO(n)$ near $P$. These maps act as local inverses and allow for a "linear-like" movement in the tangent space
> .
> â—¦
> The proposed transformation $\tilde{O}$ (the output of this step) is given by a specific formula that incorporates $O$, $P$, and the temperature $\tau$
> : $\tilde{O} = P (P^T O)^{\tau}$ [316, Eq. 10]
> â–ª
> Role of $P^T O$: Since $P$ is the closest permutation matrix to $O$, $P^T O$ is an orthogonal matrix that is "close" to the identity matrix $I$ (as $P^T P = I$).
> â–ª
> Role of $(\cdot)^{\tau}$: Raising $P^T O$ to the power of $\tau$ (where $\tau \in (0, 1]$) effectively moves it along the geodesic in $SO(n)$ towards the identity matrix $I$
> . The matrix power $A^\tau = \text{expm}(\tau \text{logm}(A))$ formally moves $A$ along a geodesic from $I$ towards $A$
> .
> â–ª
> Role of the leading $P$: Multiplying by $P$ at the front ($P(\cdot)$) then shifts this result back from the neighborhood of $I$ to the neighborhood of $P$
> .
> â–ª
> Temperature Control ($\tau$): The parameter $\tau$ is crucial for controlling how close $\tilde{O}$ gets to $P$
> .
> â€¢
> If $\tau = 1$, then $\tilde{O} = P(P^T O)^1 = PP^T O = O$. The orthogonal matrix $O$ remains unchanged
> .
> â€¢
> As $\tau \to 0^+$, the term $(P^T O)^{\tau}$ approaches $I$ (the identity matrix)
> . Therefore, $\tilde{O} = P \cdot I = P$. This means $\tilde{O}$ converges to the permutation matrix $P$
> .


<!-- - [ ] Additional considerations -->

> 3.
> Handling Odd Permutations: The formula $P(P^T O)^\tau$ works directly if $P$ is an "even" permutation (i.e., $P \in SO(n)$). For "odd" permutations, where $\text{det}(P) = -1$, the paper introduces an "agent" $PÌ‚ = PD$ where $D = \text{diag}({1, \dots, 1, -1})$ to ensure $PÌ‚ \in SO(n)$
> . The interpolation is then performed towards $PÌ‚$, and the result is mapped back to the neighborhood of $P$ by right-multiplying by $D^T$. The final comprehensive mapping is $\psi_{\tau}: SO(n) \to M_P$, where $M_P$ is the manifold that "tightly wraps around the permutation matrices"
> .

 <!-- - [ ] Birkhoff polytope vs. Orthogonal Rleaxation approach -->

> ### Comparison with Birkhoff Polytope Methods (e.g., Sinkhorn)
> 
> **Key Advantages of OT4P:**
> 1. **Lower dimensional representation**: n(n-1)/2 for OT4P vs (n-1)Â² for Birkhoff polytope
> 2. **Preserves geometric structure**: Orthogonal matrices maintain inner products, useful for problems requiring geometric invariance
> 3. **No column-sum constraints**: Simpler optimization landscape compared to doubly-stochastic constraints
> 
> **Empirical Evidence:**
> - **Mode connectivity (Table 1)**: OT4P achieves 100% precision across all architectures, while Sinkhorn fails on VGG11 (63.08% precision)
> - **Neuron identity inference (Table 2)**: OT4P consistently finds optimal solutions even in challenging scenarios (5% known neurons), where Gumbel-Sinkhorn struggles
> - **Permutation synchronization (Figure 3)**: OT4P consistently outperforms across all object classes and problem sizes
> 
> ### Why Use OT4P for Approximate Symmetries?
> 
> 1. **Robustness**: The orthogonal group relaxation appears more stable than Birkhoff polytope, avoiding poor local minima
> 2. **Efficiency**: Lower dimensional search space means faster convergence
> 3. **Geometric preservation**: Critical for symmetry-finding applications where you need to preserve distances and angles
> 4. **Temperature control**: Allows smooth transition from continuous relaxation to discrete permutations
> 

> ### Why This Framework is Elegant
> 
> The beauty of OT4P lies in how it exploits the geometric structure:
> 
> 1. **Natural parameterization**: Uses Lie group structure rather than fighting constraints
> 2. **Geodesic interpolation**: Temperature control via natural geometry
> 3. **Bi-invariant metric**: Makes left/right translations isometries
> 4. **Dimension reduction**: Works in minimal dimensional space
> 
> The method essentially says: "Don't optimize over permutations directly. Instead, optimize over orthogonal matrices (which we can parameterize nicely) and project back to permutations when needed."

> Optimization Process
> The overall OT4P algorithm takes an unconstrained vector $A$ in $R^{n(n-1)/2}$, transforms it into an orthogonal matrix $O$ via $\phi(A)$, and then moves $O$ towards its closest permutation matrix $P$ to obtain $\tilde{O}$ via $\psi_{\tau}(O)$
> . This composite mapping, $\psi_{\tau} \circ \phi$, effectively converts the challenging combinatorial optimization problem over permutation matrices into an unconstrained optimization problem in the Euclidean space $R^{n(n-1)/2}$
> .
> This means that during optimization, when you take a gradient step in the unconstrained $R^{n(n-1)/2}$ space, the OT4P transformation ensures that the resulting matrix remains on the orthogonal group and is biased towards a permutation matrix based on the temperature parameter $\tau$. Lower $\tau$ values encourage the solution to be very close to a discrete permutation matrix
> .
> In contrast to the doubly stochastic multinomial manifold $DP_n$ which has a dimension of $(n-1)^2$
> , the orthogonal group $SO(n)$ has a lower representation dimension of $n(n-1)/2$. This potentially leads to a smaller search space, one of the unique advantages highlighted by the authors.

> 3.2 Parameterization for gradient-based optimization
> This section demonstrates how to use OT4P to implement a parameterization for the relaxation
> of permutation matrices, thereby allowing gradient-based optimization for Equation (1). More
> importantly, we present three advantages of this parameterization, making it a reasonable solution.
> Recalling the manifold MP obtained from Equation (11), which converges around the permutation
> matrices controlled by the temperature parameter Ï„ . We first relax Equation (1) into an optimization
> problem on the manifold MP :
> min
> OâˆˆMP
> f (O). (12)
> Using the composite mapping ÏˆÏ„ â—¦ Ï•, we transform the constrained optimization problem on the
> manifold MP into an unconstrained optimization problem in the vector space R n(nâˆ’1)
> 2 :
> min
> AâˆˆR n(nâˆ’1)
> 2
> f (ÏˆÏ„ â—¦ Ï•(A)). (13)
> For the aforementioned optimization problem, we can employ standard optimization techniques, such
> as SGD and Adam algorithms [ 58 ], to approximate the solution. Below, we thoroughly discuss the
> three advantages brought about by the parameterization of OT4P

 <!-- - [ ] Implementation issues
    - [ ] Efficiency -->

> ### Practical Implementation Note
> The computational cost is dominated by eigendecomposition and Hungarian algorithm (both O(nÂ³)). The paper provides efficient gradient computation through clever reparameterization, making it practical for moderate-sized problems.
> 
> This overview should provide a clear foundation for your thesis exposition while maintaining technical accuracy. The method's strength lies in its elegant use of differential geometry to solve a combinatorial problem, making it particularly suitable for applications like approximate symmetry finding where geometric properties matter.


 <!-- - [ ] Note AutoDiff using pytorch is being used -->