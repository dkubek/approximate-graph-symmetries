### OT4P
[SUBTITLE: Moving into a better behaved space]

This research introduces OT4P, a novel method for relaxing permutation matrices onto the orthogonal group, offering potential benefits over existing techniques which often rely on the Birkhoff polytope. OT4P employs a temperature-controlled differentiable transformation that maps unconstrained vector space to the orthogonal group, where temperature influences the proximity of the resulting matrices to permutation matrices. This allows for gradient-based optimization of permutation-related problems and, with a re-parameterization trick, facilitates efficient stochastic optimization over latent permutations. The paper presents experimental results demonstrating OT4P's effectiveness in various tasks, including finding mode connectivity in neural networks, inferring neuron identities, and solving permutation synchronization problems.

> ## Overview of OT4P: A Gentle Exposition
> 
> ### Core Idea
> OT4P relaxes the discrete permutation matrix optimization problem by embedding permutation matrices into the continuous space of orthogonal matrices, which can then be parameterized using unconstrained vectors. This enables gradient-based optimization while maintaining key geometric properties.
> 
> ### The Two-Step Framework
> 
> **Step 1: Vector â†’ Orthogonal Matrix**
> - Start with an unconstrained vector in â„^(n(n-1)/2) 
> - Transform it to a skew-symmetric matrix A (by computing A - A^T)
> - Apply matrix exponential to get an orthogonal matrix O = exp(A - A^T)
> 
> **Step 2: Moving Towards Permutations**
> - Find the closest permutation matrix P to O using the Hungarian algorithm
> - Move O towards P along a geodesic path, controlled by temperature Ï„
> - Result: Ã• = P(P^T O)^Ï„, which approaches P as Ï„ â†’ 0
> 
> ### Addressing Your Specific Questions:
> 
> 1. **Special Orthogonal vs Orthogonal Group**: The paper primarily works in SO(n) (special orthogonal group with det=+1). The occasional mention of O(n) is to handle odd permutations (det=-1) through a simple trick using a diagonal matrix D.
> 
> 2. **Skew-Symmetric Transformation**: The mapping A â†’ A - A^T creates skew-symmetric matrices. Yes, this gets all skew-symmetric matrices because any skew-symmetric S can be written as S = (S/2) - (S/2)^T.
> 
> 3. **Matrix Exponential Motivation**: The matrix exponential (Lie exponential) is the natural map from the Lie algebra so(n) (skew-symmetric matrices) to SO(n). It's locally bijective but not globally (similar to how e^x maps â„ to â„^+).
> 
> 4. **Why A - A^T instead of just A**: The operation A - A^T ensures we get a skew-symmetric matrix, which is necessary for exp(Â·) to produce an orthogonal matrix. This is the composition of two maps: â„^(n(n-1)/2) â†’ so(n) â†’ SO(n).
> 
> 5. **Domain U**: This is the region where the mapping is injective (one-to-one). It ensures each orthogonal matrix has a unique representation, avoiding ambiguities in optimization.
> 
> 6. **SO(n) Contents**: SO(n) contains only orthogonal matrices with determinant +1. It's a subset of all orthogonal matrices O(n).
> 
> 7. **Geodesic Movement**: The key insight is that P^T O is near identity when O is near P. The power operation (Â·)^Ï„ naturally interpolates along geodesics in SO(n). Multiplying by P shifts this interpolation from around I to around P.
> 
> 8. **Gradient Computation**: Exactly! You can now compute gradients with respect to the original vector parameterization and use standard optimizers.
> 
> ### Comparison with Birkhoff Polytope Methods (e.g., Sinkhorn)
> 
> **Key Advantages of OT4P:**
> 1. **Lower dimensional representation**: n(n-1)/2 for OT4P vs (n-1)Â² for Birkhoff polytope
> 2. **Preserves geometric structure**: Orthogonal matrices maintain inner products, useful for problems requiring geometric invariance
> 3. **No column-sum constraints**: Simpler optimization landscape compared to doubly-stochastic constraints
> 
> **Empirical Evidence:**
> - **Mode connectivity (Table 1)**: OT4P achieves 100% precision across all architectures, while Sinkhorn fails on VGG11 (63.08% precision)
> - **Neuron identity inference (Table 2)**: OT4P consistently finds optimal solutions even in challenging scenarios (5% known neurons), where Gumbel-Sinkhorn struggles
> - **Permutation synchronization (Figure 3)**: OT4P consistently outperforms across all object classes and problem sizes
> 
> ### Why Use OT4P for Approximate Symmetries?
> 
> 1. **Robustness**: The orthogonal group relaxation appears more stable than Birkhoff polytope, avoiding poor local minima
> 2. **Efficiency**: Lower dimensional search space means faster convergence
> 3. **Geometric preservation**: Critical for symmetry-finding applications where you need to preserve distances and angles
> 4. **Temperature control**: Allows smooth transition from continuous relaxation to discrete permutations
> 
> ### Practical Implementation Note
> The computational cost is dominated by eigendecomposition and Hungarian algorithm (both O(nÂ³)). The paper provides efficient gradient computation through clever reparameterization, making it practical for moderate-sized problems.
> 
> This overview should provide a clear foundation for your thesis exposition while maintaining technical accuracy. The method's strength lies in its elegant use of differential geometry to solve a combinatorial problem, making it particularly suitable for applications like approximate symmetry finding where geometric properties matter.

> ## Deep Technical Analysis of OT4P
> 
> ### Step 1: Vector â†’ Orthogonal Matrix (Detailed)
> 
> The first step creates a differentiable map from unconstrained vector space to SO(n):
> 
> **1.1 Vector Representation**
> - Start with a vector **a** âˆˆ â„^(n(n-1)/2)
> - This vector fills the upper triangular part of an nÃ—n matrix A (with zeros on diagonal)
> - Example for n=3: [aâ‚, aâ‚‚, aâ‚ƒ] â†’ A = [[0, aâ‚, aâ‚‚], [0, 0, aâ‚ƒ], [0, 0, 0]]
> 
> **1.2 Skew-Symmetrization**
> - Transform A to skew-symmetric: S = A - A^T
> - This creates S âˆˆ so(n), the Lie algebra of SO(n)
> - Key property: S^T = -S
> 
> **1.3 Matrix Exponential**
> - Apply matrix exponential: O = exp(S) = I + Î£(S^k/k!)
> - This maps so(n) â†’ SO(n)
> - Result: O^T O = I and det(O) = +1
> 
> **Mathematical Properties:**
> - The mapping Ï•: â„^(n(n-1)/2) â†’ SO(n) defined by Ï•(A) = exp(A - A^T) is:
>   - Differentiable everywhere
>   - Surjective (onto)
>   - Injective on domain U = {A | Im(Î»â‚–(A - A^T)) âˆˆ (-Ï€, Ï€)}
> 
> ### Step 2: Moving Towards Permutations (Detailed)
> 
> **2.1 Finding Closest Permutation**
> - Given O âˆˆ SO(n), solve: P* = argmax_{PâˆˆPâ‚™} âŸ¨P, OâŸ©_F
> - This is a linear assignment problem solved by Hungarian algorithm
> - Cost: O(nÂ³)
> 
> **2.2 Geodesic Interpolation**
> The key innovation is moving O towards P along the geodesic:
> 
> 1. **Push to neighborhood of I**: Compute P^T O (this is near I when O is near P)
> 2. **Take logarithm**: log(P^T O) maps to tangent space at I
> 3. **Scale by Ï„**: Ï„Â·log(P^T O) 
> 4. **Exponential map**: exp(Ï„Â·log(P^T O)) = (P^T O)^Ï„
> 5. **Pull back to P**: Ã• = P(P^T O)^Ï„
> 
> **Why this works:**
> - When O â‰ˆ P, then P^T O â‰ˆ I
> - The power operation (Â·)^Ï„ naturally interpolates eigenvalues
> - As Ï„ â†’ 0, Ã• â†’ P
> 
> ### Detailed Answers to Your Questions
> 
> **1. Special Orthogonal vs Orthogonal Group**
> 
> The paper primarily works in SO(n) but must handle O(n) for completeness:
> - **Even permutations**: det(P) = +1, so P âˆˆ SO(n) directly
> - **Odd permutations**: det(P) = -1, so P âˆˆ O(n) \ SO(n)
> 
> For odd permutations, they use a clever trick:
> - Find agent PÌƒ = PD where D = diag(1,...,1,-1)
> - Now PÌƒ âˆˆ SO(n) and âˆ¥PÌƒ - Pâˆ¥_F is minimized
> - Work with PÌƒ in SO(n), then multiply by D^T at the end
> 
> **2. Skew-Symmetric Transformation Details**
> 
> The mapping A â†’ A - A^T is carefully chosen:
> - **Completeness**: Every skew-symmetric S can be written as S = B - B^T for some B
>   - Proof: Take B = S/2, then B - B^T = S/2 - S^T/2 = S/2 + S/2 = S
> - **Dimension matching**: Upper triangular has n(n-1)/2 free parameters, same as so(n)
> - **Isomorphism**: This creates a vector space isomorphism between â„^(n(n-1)/2) and so(n)
> 
> **3. Matrix Exponential (Lie Exponential) Deep Dive**
> 
> The exponential map exp: so(n) â†’ SO(n) has profound geometric meaning:
> 
> - **Geometric interpretation**: It maps straight lines through 0 in so(n) to geodesics through I in SO(n)
> - **Local diffeomorphism**: Near 0, it's a bijection to a neighborhood of I
> - **Not globally bijective**: Multiple elements in so(n) can map to same O âˆˆ SO(n)
>   - Example: Rotations by Î¸ and Î¸ + 2Ï€ are the same
> 
> The Lie algebra so(n) is indeed the tangent space T_I SO(n) at identity. The exponential map:
> - Takes tangent vectors at I
> - Flows along geodesics for unit time
> - Lands on the manifold SO(n)
> 
> **4. Why Ï•(A) = exp(A - A^T) Instead of Just exp(A)**
> 
> This is subtle but crucial:
> - exp(A) only gives SO(n) if A is skew-symmetric
> - Starting with arbitrary A âˆˆ â„^(nÃ—n) won't guarantee orthogonality
> - The operation A - A^T:
>   - Guarantees skew-symmetry: (A - A^T)^T = A^T - A = -(A - A^T)
>   - Maintains n(n-1)/2 degrees of freedom
>   - Creates a clean parameterization
> 
> **5. Domain U and Injectivity**
> 
> Domain U = {A âˆˆ â„^(n(n-1)/2) | Im(Î»â‚–(A - A^T)) âˆˆ (-Ï€, Ï€)}:
> - **Injectivity** means one-to-one (each output has at most one input)
> - Outside U, multiple inputs map to same output (like angles Î¸ and Î¸ + 2Ï€)
> - U is the "fundamental domain" where the exponential map is bijective
> - Practically: optimization stays in U most of the time
> 
> **6. Contents of SO(n)**
> 
> SO(n) contains exactly the nÃ—n real matrices O such that:
> - O^T O = I (orthogonality)
> - det(O) = +1 (special)
> 
> Nothing else! It's a smooth manifold of dimension n(n-1)/2 embedded in â„^(nÃ—n).
> 
> **7. Geodesic Movement - The Deep Geometry**
> 
> This is the most sophisticated part. Let me break it down:
> 
> **Why P^T O is near I when O is near P:**
> - If O = P + Îµ (small perturbation), then P^T O = P^T(P + Îµ) = I + P^T Îµ
> - Since P is orthogonal, âˆ¥P^T Îµâˆ¥ = âˆ¥Îµâˆ¥, so P^T O is I plus small perturbation
> 
> **How (Â·)^Ï„ moves along geodesics:**
> - For orthogonal matrix M near I, M^Ï„ interpolates eigenvalues
> - If M = QÎ›Q^T (eigendecomposition), then M^Ï„ = QÎ›^Ï„ Q^T
> - This traces a geodesic from I (when Ï„=0) to M (when Ï„=1)
> 
> **The shifting mechanism:**
> - Working near I is computationally convenient (logarithm converges)
> - Left multiplication by P is an isometry in SO(n) with bi-invariant metric
> - So PÂ·(geodesic from I to P^T O) = geodesic from P to O
> 
> **In manifold optimization context:**
> - This is geodesic interpolation on SO(n)
> - Temperature Ï„ controls position along geodesic
> - As Ï„ â†’ 0, we approach the endpoint (permutation matrix)
> 
> **8. Gradient Computation Flow**
> 
> The complete chain:
> 1. **Forward**: a â†’ A â†’ exp(A - A^T) â†’ O â†’ P(P^T O)^Ï„ â†’ Ã•
> 2. **Compute loss**: L = f(Ã•)
> 3. **Backward**: âˆ‚L/âˆ‚a via chain rule through all operations
> 
> Key insights for efficiency:
> - Eigendecomposition enables fast computation of (Â·)^Ï„ and its gradient
> - They cache W_Ï„ = Ã•O^T in forward pass for efficient backward pass
> - All operations are differentiable except Hungarian (which is piecewise constant)
> 
> ### Why This Framework is Elegant
> 
> The beauty of OT4P lies in how it exploits the geometric structure:
> 
> 1. **Natural parameterization**: Uses Lie group structure rather than fighting constraints
> 2. **Geodesic interpolation**: Temperature control via natural geometry
> 3. **Bi-invariant metric**: Makes left/right translations isometries
> 4. **Dimension reduction**: Works in minimal dimensional space
> 
> The method essentially says: "Don't optimize over permutations directly. Instead, optimize over orthogonal matrices (which we can parameterize nicely) and project back to permutations when needed."

#### Manifold background

> ## Lie Groups: Definition and Properties
> 
> **Definition:** A **Lie group** G is a set that is simultaneously:
> 1. A **smooth manifold** (has coordinates, calculus works)
> 2. A **group** (has multiplication, identity, inverses)
> 3. The group operations are **smooth** (differentiable)
> 
> **Key properties:**
> - **Tangent space at identity**: Called the **Lie algebra** ð”¤
> - **Exponential map**: exp: ð”¤ â†’ G (connects algebra to group)
> - **Logarithm map**: log: G â†’ ð”¤ (inverse of exponential, locally)
> 
> **Examples:**
> - â„â¿ under addition
> - SÂ¹ = {e^{iÎ¸} : Î¸ âˆˆ â„} under complex multiplication  
> - GL(n) = invertible nÃ—n matrices under matrix multiplication
> - **SO(n)** = special orthogonal group (our main focus)
> 
> ## Special Orthogonal Group SO(n)
> 
> **Definition:**
> ```
> SO(n) = {Q âˆˆ â„â¿Ë£â¿ : Qáµ€Q = I, det(Q) = +1}
> ```
> 
> **What this means:**
> - **Orthogonal**: Qáµ€Q = I (columns are orthonormal)
> - **Special**: det(Q) = +1 (orientation-preserving)
> - **Geometric interpretation**: Rotations in n-dimensional space
> 
> **Properties:**
> - **Compact manifold** of dimension n(n-1)/2
> - **Connected** (for n â‰¥ 2): can rotate from any orientation to any other
> - **Matrix group**: inherits matrix operations
> 
> **Examples:**
> - **SO(2)**: 2D rotations, isomorphic to SÂ¹
>   ```
>   [cos Î¸  -sin Î¸]
>   [sin Î¸   cos Î¸]
>   ```
> - **SO(3)**: 3D rotations (orientation of rigid bodies)
> 
> ## The Lie Algebra so(n)
> 
> **Definition:** The tangent space at the identity:
> ```
> so(n) = {A âˆˆ â„â¿Ë£â¿ : Aáµ€ = -A}
> ```
> 
> **Properties:**
> - **Skew-symmetric matrices**: Aáµ€ = -A
> - **Dimension**: n(n-1)/2 (same as SO(n))
> - **Lie bracket**: [A,B] = AB - BA
> 
> **Physical interpretation:** 
> - Elements represent **angular velocities**
> - Generators of infinitesimal rotations
> 
> ## The Exponential Map Connection
> 
> Here's where the naming becomes clear! For matrix Lie groups:
> 
> **Matrix exponential:**
> ```
> exp(A) = I + A + AÂ²/2! + AÂ³/3! + ... = Î£ Aáµ/k!
> ```
> 
> **Key theorem:** If A âˆˆ so(n), then exp(A) âˆˆ SO(n)
> 
> **Examples:**
> - **SO(2)**: exp([0 -Î¸; Î¸ 0]) = [cos Î¸ -sin Î¸; sin Î¸ cos Î¸]
> - **SO(3)**: Rodrigues' formula for rotation from axis-angle
> 
> **This is literally the exponential map from Riemannian geometry!**
> 
> ## Applications in Manifold Optimization
> 
> **1. Orthogonality-Constrained Problems:**
> ```
> minimize f(X)  subject to X âˆˆ SO(n)
> ```
> 
> **Examples:**
> - **Procrustes problem**: Find rotation minimizing ||AQ - B||Â²
> - **Principal component analysis** with orthogonal factors
> - **Matrix factorizations** with orthogonal constraints
> 
> **2. Concrete Algorithm Steps:**
> 
> Given objective f: SO(n) â†’ â„:
> 
> **Step 1 - Riemannian gradient:**
> ```
> grad_SO f(Q) = P_tangent(âˆ‡f(Q))
> ```
> where P_tangent projects onto so(n)
> 
> **Step 2 - Exponential map update:**
> ```
> Q_{k+1} = Q_k Â· exp(-Î± Â· Q_k^T âˆ‡f(Q_k))
> ```
> 
> **Why this works:**
> - âˆ‡f(Q) lives in ambient space â„â¿Ë£â¿
> - Q_k^T âˆ‡f(Q_k) projects to so(n) (skew-symmetric)
> - exp(-Î± Â· Q_k^T âˆ‡f(Q_k)) âˆˆ SO(n) automatically
> - Matrix multiplication keeps us in SO(n)
> 
> **3. Practical Benefits:**
> 
> **Exact geodesics:** Unlike many manifolds, we can compute exp(A) exactly for skew-symmetric A using:
> - **Rodrigues' formula** (SO(3))
> - **Trigonometric formulas** (general case)
> - **Efficient algorithms** (scaling and squaring)
> 
> **No projection needed:** The exponential map naturally stays on the manifold, unlike retractions that might need correction.
> 
> ## Example: PCA with Orthogonality
> 
> **Problem:** Minimize ||X - QR||Â² where Q âˆˆ SO(n)
> 
> **Traditional approach:** SVD or iterative projection methods
> 
> **Manifold approach:**
> 1. Compute Euclidean gradient: âˆ‡f(Q) = 2(QR - X)R^T
> 2. Project to so(n): A = Q^Tâˆ‡f(Q) - (Q^Tâˆ‡f(Q))^T
> 3. Update: Q â† QÂ·exp(-Î±A)
> 
> **Advantages:**
> - **Guaranteed orthogonality** at every step
> - **No numerical drift** from constraint violation
> - **Natural geometry** respects the manifold structure
> 
> This is why SO(n) is such a perfect showcase for manifold optimization - it's both geometrically rich and computationally tractable!

> You're off to a strong start in understanding the OT4P (Orthogonal Group-based Transformation for Permutation Relaxation) framework! Your understanding that OT4P considers permutations as being embedded in the Special Orthogonal Manifold $SO(n)$ is correct. You've also correctly identified that $SO(n)$ is a Lie group that possesses an exponential map, making it a "well-behaved" space for optimization. This underlying structure allows for the design of gradient-based optimization algorithms for problems involving permutations
> .
> Now, let's address your core question: how do you move closer to permutation matrices within this framework?
> The OT4P algorithm, proposed by Guo et al.
> , operates in two main steps to achieve this, which are summarized in Algorithm 1 of the paper
> :
> The OT4P Framework: Two Key Steps
> Step I: Mapping to an Orthogonal Matrix
> The first step is to map an unconstrained vector space $R^{n(n-1)/2}$ to the special orthogonal group $SO(n)$. This is done through a composition of two mappings:
> 1.
> From vector to skew-symmetric matrix: A vector a from the $R^{n(n-1)/2}$ space is transformed into a skew-symmetric matrix A (i.e., $A^T = -A$)
> . This skew-symmetric matrix A belongs to the Lie algebra $so(n)$
> .
> 2.
> From skew-symmetric to orthogonal matrix: The matrix exponential (Lie exponential) expm(Â·) is then applied to this skew-symmetric matrix $A - A^T$ to map it into the special orthogonal group $SO(n)$
> .
> â—¦
> This combined mapping is denoted as $\phi: R^{n(n-1)/2} \to SO(n)$, where $\phi(A) = \text{expm}(A - A^T)$
> .
> â—¦
> The mapping $\phi(Â·)$ is differentiable and surjective, meaning every orthogonal matrix in $SO(n)$ can be reached. It's also injective on a specific domain U
> .
> â—¦
> Addressing Boundary Issues: The paper notes that permutation matrices with an eigenvalue of -1 (common for even-length cycles, e.g., for $n=3$, a single swap has a -1 eigenvalue) can lie on the boundary of this domain U
> . To mitigate this, a "shifting" operation is applied by left-multiplying the result of $\phi(A)$ with a random orthogonal matrix $B \in SO(n)$, i.e., $O = BO$. This effectively shifts the boundary of U to ensure that representations of most permutation matrices fall within the interior of U
> .
> Step II: Moving Closer to Permutation Matrices (The Core of Your Question)
> Once you have an orthogonal matrix $O \in SO(n)$ (obtained from Step I), the second crucial step is to move this $O$ towards the closest permutation matrix $P$ along the geodesic, controlled by a temperature parameter $\tau$
> . Here's how it works:
> 1.
> Finding the Closest Permutation Matrix: The first sub-step is to identify the permutation matrix $P \in P_n$ that is closest to the current orthogonal matrix $O$
> . This is formulated as a maximization problem: $\rho(O) := \text{argmax}_{P \in P_n} \langle P, O \rangle_F$
> .
> â—¦
> This is a linear assignment problem, which can be efficiently solved using the Hungarian algorithm in cubic time
> . The paper explains how to handle potential negative values in $O$ for this algorithm by subtracting the minimum element
> .
> 2.
> Interpolation Towards $P$ Along the Geodesic: Once the closest permutation matrix $P$ is identified, the core idea is to interpolate the orthogonal matrix $O$ towards $P$
> . This interpolation is performed in a way that respects the manifold's geometry:
> â—¦
> The paper utilizes the logarithm map at $P$ ($\text{logm}_P$) to map orthogonal matrices near $P$ to the tangent space $T_P SO(n)$
> . Similarly, an exponential map at $P$ ($\text{expm}_P$) maps elements from $T_P SO(n)$ back onto $SO(n)$ near $P$. These maps act as local inverses and allow for a "linear-like" movement in the tangent space
> .
> â—¦
> The proposed transformation $\tilde{O}$ (the output of this step) is given by a specific formula that incorporates $O$, $P$, and the temperature $\tau$
> : $\tilde{O} = P (P^T O)^{\tau}$ [316, Eq. 10]
> â–ª
> Role of $P^T O$: Since $P$ is the closest permutation matrix to $O$, $P^T O$ is an orthogonal matrix that is "close" to the identity matrix $I$ (as $P^T P = I$).
> â–ª
> Role of $(\cdot)^{\tau}$: Raising $P^T O$ to the power of $\tau$ (where $\tau \in (0, 1]$) effectively moves it along the geodesic in $SO(n)$ towards the identity matrix $I$
> . The matrix power $A^\tau = \text{expm}(\tau \text{logm}(A))$ formally moves $A$ along a geodesic from $I$ towards $A$
> .
> â–ª
> Role of the leading $P$: Multiplying by $P$ at the front ($P(\cdot)$) then shifts this result back from the neighborhood of $I$ to the neighborhood of $P$
> .
> â–ª
> Temperature Control ($\tau$): The parameter $\tau$ is crucial for controlling how close $\tilde{O}$ gets to $P$
> .
> â€¢
> If $\tau = 1$, then $\tilde{O} = P(P^T O)^1 = PP^T O = O$. The orthogonal matrix $O$ remains unchanged
> .
> â€¢
> As $\tau \to 0^+$, the term $(P^T O)^{\tau}$ approaches $I$ (the identity matrix)
> . Therefore, $\tilde{O} = P \cdot I = P$. This means $\tilde{O}$ converges to the permutation matrix $P$
> .
> 3.
> Handling Odd Permutations: The formula $P(P^T O)^\tau$ works directly if $P$ is an "even" permutation (i.e., $P \in SO(n)$). For "odd" permutations, where $\text{det}(P) = -1$, the paper introduces an "agent" $PÌ‚ = PD$ where $D = \text{diag}({1, \dots, 1, -1})$ to ensure $PÌ‚ \in SO(n)$
> . The interpolation is then performed towards $PÌ‚$, and the result is mapped back to the neighborhood of $P$ by right-multiplying by $D^T$. The final comprehensive mapping is $\psi_{\tau}: SO(n) \to M_P$, where $M_P$ is the manifold that "tightly wraps around the permutation matrices"
> .
> Optimization Process
> The overall OT4P algorithm takes an unconstrained vector $A$ in $R^{n(n-1)/2}$, transforms it into an orthogonal matrix $O$ via $\phi(A)$, and then moves $O$ towards its closest permutation matrix $P$ to obtain $\tilde{O}$ via $\psi_{\tau}(O)$
> . This composite mapping, $\psi_{\tau} \circ \phi$, effectively converts the challenging combinatorial optimization problem over permutation matrices into an unconstrained optimization problem in the Euclidean space $R^{n(n-1)/2}$
> .
> This means that during optimization, when you take a gradient step in the unconstrained $R^{n(n-1)/2}$ space, the OT4P transformation ensures that the resulting matrix remains on the orthogonal group and is biased towards a permutation matrix based on the temperature parameter $\tau$. Lower $\tau$ values encourage the solution to be very close to a discrete permutation matrix
> .
> In contrast to the doubly stochastic multinomial manifold $DP_n$ which has a dimension of $(n-1)^2$
> , the orthogonal group $SO(n)$ has a lower representation dimension of $n(n-1)/2$. This potentially leads to a smaller search space, one of the unique advantages highlighted by the authors.
